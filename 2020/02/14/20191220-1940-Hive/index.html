<!DOCTYPE html>
<html lang="en">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/shin.png">
  <link rel="icon" type="image/png" href="/img/shin.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="description" content="">
  <meta name="author" content="Shinnosuke Guo">
  <meta name="keywords" content="">
  <title>20200204_1620_Hive ~ Shinnosuke</title>

  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"  >
<link rel="stylesheet" href="/lib/bootstrap/css/bootstrap.min.css"  >
<link rel="stylesheet" href="/lib/mdbootstrap/css/mdb.min.css"  >
<link rel="stylesheet" href="/lib/github-markdown/github-markdown.min.css"  >

<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">



  <link rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css"  >

<link rel="stylesheet" href="/css/main.css"  >


  <link rel="stylesheet" href="/lib/fancybox/jquery.fancybox.min.css"  >


<meta name="generator" content="Hexo 4.1.1"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Shinnosuke</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">Home</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">Archives</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">Categories</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">Tags</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">About</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="view intro-2" id="background"
         style="background: url('/img/archive.jpg')no-repeat center center;
           background-size: cover;
           background-attachment: fixed;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              <br>
              
                <p class="mt-3">
                  <i class="fas fa-calendar-alt" aria-hidden="true"></i>&nbsp;
                  Friday, February 14th 2020, 12:13 pm
                </p>
              

              <p>
                
                  
                  &nbsp;<i class="far fa-chart-bar"></i>
                  <span class="post-count">
                    3.7k 字
                  </span>&nbsp;
                

                
                  
                  &nbsp;<i class="far fa-clock"></i>
                  <span class="post-count">
                      18 分钟
                  </span>&nbsp;
                

                
                  <!-- 不蒜子统计文章PV -->
                  
                  &nbsp;<i class="far fa-eye" aria-hidden="true"></i>&nbsp;
                  <span id="busuanzi_container_page_pv">
                    <span id="busuanzi_value_page_pv"></span> 次
                  </span>&nbsp;
                
              </p>
            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="py-5 z-depth-3" id="board">
        <div class="post-content mx-auto" id="post">
          <div class="markdown-body">
            <h2 id="移花接木-Hive"><a href="#移花接木-Hive" class="headerlink" title="移花接木-Hive"></a>移花接木-Hive</h2><h3 id="01-Hive-1-x"><a href="#01-Hive-1-x" class="headerlink" title="01. Hive 1.x"></a>01. Hive 1.x</h3><ul>
<li>Repo Point<br><a href="http://mirror.bit.edu.cn/apache/hive/hive-1.2.2/apache-hive-1.2.2-bin.tar.gz" target="_blank" rel="noopener">apache-hive-1.2.2-bin.tar.gz</a><br><a href="https://downloads.mysql.com/archives/get/file/mysql-connector-java-5.1.44.tar.gz" target="_blank" rel="noopener">mysql-connector-java-5.1.44.tar</a><br><a href="http://archive.apache.org/dist/hive/hive-1.2.2/apache-hive-1.2.2-src.tar.gz" target="_blank" rel="noopener">apache-hive-1.2.2-src.tar.gz</a></li>
</ul>
<hr>
<h3 id="02-Beeline连接Hive（Review）"><a href="#02-Beeline连接Hive（Review）" class="headerlink" title="02. Beeline连接Hive（Review）"></a>02. Beeline连接Hive（Review）</h3><ul>
<li><p>这里使用的是<code>Hive Metastore ➡ hiveserver2 ➡ beeline</code>的连接方式（启动方式见下方）.</p>
</li>
<li><p>也可以使用<code>Hive Metastore ➡ hive（CLI）</code>的连接方式</p>
</li>
<li><p>后缀为重定向，需百度解决（已解决，详见 <a href="http://localhost:4000/2020/02/14/20191219-2254-Hive-1-x/" target="_blank" rel="noopener">20191224_1414_Hive-1.x配置</a>）</p>
</li>
</ul>
<pre><code class="bash"># hive-site.xml中配置了hive.metastore.uris后，无论是HS2或是Hive CLI开启前，都需开启Metastore
hive --service metastore  1&gt;/dev/null  2&gt;&amp;1  &amp;
# beeline方式连接Hive，默认端口为10000
hive --service hiveserver2 &amp;
#beeline
#!connect jdbc:hive2://master:10000 root 123456
beeline -u jdbc:hive2://master:10000 -n root -p 123456 --color=true</code></pre>
<p><code>Tips</code> 仅退出当前beeline: <strong>!close</strong>     彻底退出: <strong>!q</strong>     查询表: <strong>!tables</strong></p>
<hr>
<h3 id="03-Hive-元素"><a href="#03-Hive-元素" class="headerlink" title="03. Hive 元素"></a>03. Hive 元素</h3><h4 id="3-1-元数据-amp-数据"><a href="#3-1-元数据-amp-数据" class="headerlink" title="3-1 元数据 &amp; 数据"></a>3-1 元数据 &amp; 数据</h4><ul>
<li>Hive中的表是纯逻辑表，即<code>元数据</code>；</li>
<li>数据存储在<code>HDFS</code>上，元数据与数据存储分离；</li>
<li>数据计算依赖分布式计算框架 <strong>MapReduce</strong>；</li>
<li>Hive<code>读多写少</code>，从HDFS中读，MR计算，并写回HDFS；<code>不支持数据改写&amp;删除</code>；</li>
<li>用户需指定<code>三个属性</code>用来定义数据格式：<br>列分隔符：空格 ‘,’  ‘\t’<br>行分隔符：’\n’<br>读取数据的方法</li>
</ul>
<h4 id="3-2-Hive-连接方式"><a href="#3-2-Hive-连接方式" class="headerlink" title="3-2 Hive 连接方式"></a>3-2 Hive 连接方式</h4><p><code>CLI</code> <strong><em>Hive Metastore</em></strong> → hiveserver2(HS2) → <strong><em>Beeline</em></strong><br><code>JDBC</code> <strong><em>Metastore</em></strong> → Java/Python/C++<br><code>WUI</code> apache-hive-0.13.0-src.tar.gz → <strong><em>hive –service hwi</em></strong></p>
<h4 id="3-3-Hive-HDFS"><a href="#3-3-Hive-HDFS" class="headerlink" title="3-3 Hive HDFS"></a>3-3 Hive HDFS</h4><ul>
<li>HDFS部分主要是贮存离线数据，主要就是数据目录或数据的相关操作（详见 <a href="https://shinnosuke1028.github.io/2020/01/15/20191219-2347-Hadoop-2-6-1/" target="_blank" rel="noopener">20201219_2347_Hadoop-2.6.1（持续更新</a>）.</li>
</ul>
<hr>
<h3 id="04-数据入库"><a href="#04-数据入库" class="headerlink" title="04. 数据入库"></a>04. 数据入库</h3><h4 id="4-0-内置函数"><a href="#4-0-内置函数" class="headerlink" title="4-0 内置函数"></a>4-0 内置函数</h4><pre><code class="SQL">--时间戳
select current_date;    --2020-02-13
select current_timestamp;   --2020-02-13 18:09:40.666
--转内核时间格式
select to_unix_timestamp(&#39;2020-02-13 12:12:12&#39;);    --1581567132
--内核转当前可读
select from_unixtime(unix_timestamp()); --2020-02-13 18:09:12
select from_unixtime(unix_timestamp(), &#39;yyyy-MM-dd HH:mm:ss&#39;);
--字符串转日期
select to_date(&#39;2020-02-13 13:34:12&#39;);

--时间戳间隔天数
select datediff(&#39;2020-02-13&#39;,&#39;2020-02-15&#39;);
select date_sub(&#39;2020-02-13&#39;,4);
select date_add(&#39;2015-04-09&#39;,4);    --select date_sub(&#39;2020-02-13&#39;,-4);
year/month/day/hour/minute/second</code></pre>
<h4 id="4-1-DML"><a href="#4-1-DML" class="headerlink" title="4-1 DML"></a>4-1 DML</h4><ul>
<li>表的属性<ol>
<li>一张表 是内部表或者外部表， 这两个是二选一的</li>
<li>一张表可以是分区表</li>
<li>一张表可以是分桶表</li>
<li>一张表可以是分区表也可以是分桶表</li>
<li>第一条和剩余三条可以搭配（内部【外部】分区表 / 内部【外部】分桶表 / 内部【外部】分区分桶表）</li>
</ol>
</li>
</ul>
<p><code>内部表</code></p>
<pre><code class="SQL">create table hive_student_mng(id int, name string, sex string, age int, department string) row format delimited fields terminated by &quot;,&quot; lines terminated by &quot;\n&quot;;
/*
row format    格式化关键字
    delimited 
        fields terminated by &quot;,&quot;    字段与字段之间的分隔符
        lines terminated by &quot;\n&quot;;    记录与记录之间的分隔符
    serde   --这个参数忘记用途了    
        ...
*/
--从HDFS内导入
load data inpath &#39;/data/hive_hdfs/student.txt&#39; into table hive_student_mng;
--从本地导入
load data local inpath &#39;/data/hive/student.txt&#39; into table hive_student_mng;</code></pre>
<p><code>Tips</code>从HDFS导入数据相当于mv，从local导入数据相当于cp </p>
<hr>
<p><code>外部表</code></p>
<pre><code class="SQL">create external table hive_student_ext(id int, name string, sex string, age int, department string) row format delimited fields terminated by &quot;,&quot; lines terminated by &quot;\n&quot; location &#39;/usr/local/src/apache-hive-1.2.2-bin/warehouse/hive_student_ext&#39;;
--从HDFS内导入
load data inpath &#39;/data/hive_hdfs/student.txt&#39; into table hive_student_ext;
--从本地导入
--load data local inpath &#39;/data/hive/student.txt&#39; into table hive_student_mng;</code></pre>
<p><code>CTAS</code></p>
<pre><code class="SQL">--类Oracle
create table hive_student_mng_2 as select * from hive_student_mng;</code></pre>
<p><code>分区表</code></p>
<pre><code class="SQL">create table hive_student_ptn(id int, name string, sex string) partitioned by (age int, department string) row format delimited fields terminated by &quot;,&quot;;

--本地数据至分区表时，需指定分区
-- load data local inpath &quot;/home/hadoop/student.txt&quot; into table student_ptn partition(age=17,department=&quot;MA&quot;);
--多重模式插入数据至分区表，同样需要指定分区，但是只需遍历一次源表
from hive_student_mng
insert into table hive_student_ptn partition(age=17,department=&quot;MA&quot;) select id, name, sex where age = 17 and department=&quot;MA&quot;
insert into table hive_student_ptn partition(age=18,department=&quot;CS&quot;) select id, name, sex where age = 18 and department=&quot;CS&quot;
insert into table hive_student_ptn partition(age=18,department=&quot;IS&quot;) select id, name, sex where age = 18 and department=&quot;IS&quot;;</code></pre>
<p><code>静态分区表</code></p>
<pre><code class="SQL">create table hive_student_ptn_static(id int, name string, sex string) partitioned by (dt date) row format delimited fields terminated by &quot;,&quot;;
--日期直接写程&quot;-&quot;分割的string即可，Hive引擎会自动试别并转换
alter table hive_student_ptn_static add partition(dt=&#39;2020-02-14&#39;) partition(dt=&#39;2020-02-15&#39;);
load data local inpath &quot;xx&quot; into table xxx (column=&quot;xx&quot;) partition(dt=&#39;2020-02-14&#39;)</code></pre>
<p><code>动态分区表</code></p>
<ul>
<li>数据根据分区字段，自动进行分区的创建和数据导入，<code>作为分区的字段在查询中需要按顺序放在最后</code>！</li>
<li>两个重要参数：<ol>
<li>set hive.exec.dynamic.partition=true;</li>
<li>set hive.exec.dynamic.partition.mode=nonstrict;  ⬅非严格动态分区模式，严格动态分区模式需要给分区表至少配置一个静态分区</li>
<li>动态分区的一些限制：<br> set hive.exec.max.dynamic.partitions.pernode=100;   ⬅每个节点生成动态分区最大个数<br> set hive.exec.max.dynamic.partitions=1000;  ⬅生成动态分区最大个数，如果自动分区数大于这个参数，将会报错</li>
</ol>
</li>
</ul>
<pre><code class="SQL">--动态分区表插入数据
create table hive_student_ptn_2(id int, name string, sex string) partitioned by (age int, department string) row format delimited fields terminated by &quot;,&quot;;

insert into table hive_student_ptn_2 partition(department=&#39;xxx&#39;, age) select id, name, sex,  age from hive_student_mng; </code></pre>
<p><code>分桶表</code>（有疑问）</p>
<pre><code class="SQL">create table hive_student_bucket (id int, name string, sex string, age int, department string) 
clustered by (age) sort by (age desc, id asc) into 2 buckets row format delimited fields terminated by &quot;,&quot;;

--虽然创建分桶表的时候指定了分桶字段和排序字段
--但是数据到底有没有进行分桶和排序是根据插入数据的HQL决定的（这边还是不太理解）
set mapreduce.job.reduces=3;
insert into table hive_student_bucket select * from hive_student_mng distribute by age sort by age desc, id asc;

--distribute by 
--方法1：导出，将分桶表导出，不设置mapreduce.job.reduces，默认为2个reduces，导出两个文件
insert overwrite local directory &quot;/data/hive/hive_student_bucket_2/&quot; select * from hive_student_bucket;
--放法2：导出，在源表的基础上使用分桶排序关键字做查询并导出，3个reduces，三个文件
set mapreduce.job.reduces=3;
insert overwrite local directory &quot;/data/hive/hive_student_bucket/&quot; select * from hive_student_mng distribute by age sort by age desc, id asc;
--方法3：分桶表的基础上再做分桶，和方法2有点冗余，结果相同
set mapreduce.job.reduces=3;
insert overwrite local directory &quot;/data/hive/hive_student_bucket_4/&quot; select * from hive_student_bucket distribute by age sort by age desc, id asc;


--cluster by 
distribute by age sort by age == cluster by age</code></pre>
<p><code>Tips</code>虽然cluster有简化分桶操作的概念，但是日常操作时还是建议使用<em>distribute by + sort by</em>.<br><code>Tips</code>导出分桶数据有两种方式，但是导出的数据文件状态不同：其一，利用方法1导出，表本身分为3桶（0/1/2），即便设置reduce数量，但仅能导出2个文件，3桶中的0和2被放置在一个文件中；其二，按照正常分桶逻辑，3桶导出3个文件（0/1/2）；其三，对分桶表做分桶导出，和方法2一样，也是导出3文件，但显得很啰嗦.<br><code>Tips</code>导出的文件含有隐藏分隔符^A（\x01，<strong>ctrl+V+A</strong>），需要用tab替换下，便于后续入库：<br>    sed -e ‘s/^A/\t/g’  ⬅查看<br>    sed -i ‘s/^A/\t/g’  ⬅修改</p>
<hr>
<h4 id="4-2-DQL"><a href="#4-2-DQL" class="headerlink" title="4-2 DQL"></a>4-2 DQL</h4><h5 id="支持-VS-不支持"><a href="#支持-VS-不支持" class="headerlink" title="支持 VS 不支持"></a>支持 VS 不支持</h5><ul>
<li>支持union all/ join/like/where/having/各种聚合/json解析</li>
<li>支持UDF/UDAF/UDTF</li>
<li>支持in/exists，但是Hive推荐使用semi join（半连接）</li>
<li>支持case when</li>
<li>支持truncate，和Oracle类似，只清数据，不清分区</li>
<li>不支持update/delete</li>
<li>不支持or条件</li>
<li>不支持非等值连接</li>
</ul>
<pre><code class="SQL">--半连接
select a.* from a semi join b on a.id = b.id;</code></pre>
<hr>
<ul>
<li>下面是个字段切割的例子：Local数据导入，数据位于Master节点：/data/mr_wc/The_man_of_property.txt</li>
</ul>
<pre><code class="SQL">--databases list
show databases;
OK
+----------------+--+
| database_name  |
+----------------+--+
| badou          |
| default        |
+----------------+--+

--查询库路径
desc database badou;
OK
+----------+----------+----------------------------------------------------------------------------+-------------+-------------+-------------+--+
| db_name  | comment  |                                  location                                  | owner_name  | owner_type  | parameters  |
+----------+----------+----------------------------------------------------------------------------+-------------+-------------+-------------+--+
| badou    |          | hdfs://master:9000/usr/local/src/apache-hive-1.2.2-bin/warehouse/badou.db  | root        | USER        |             |
+----------+----------+----------------------------------------------------------------------------+-------------+-------------+-------------+--+
1 row selected (0.225 seconds)

use badou;
OK

--元数据建立
create table article(sentence string) row format delimited fields terminated by &#39;\n&#39;; -- location /data/xxx --每一行的format

desc article; --查询表结构
--数据导入前，表为空
select article.sentence from article limit 2; 

--本地数据load（Local Load）,注意这里是Linux服务器本地数据导入
load data local inpath &#39;/data/mr_wc/The_man_of_property.txt&#39; into table article;
--导入数据：
--load data inpath &#39;/data/The_man_of_property.txt&#39; into table article; --导入 HDFS 数据：
--load data local inpath &#39;/data/mr_wc/The_man_of_property.txt&#39; into table article; --导入本地数据：

--再次查询
select article.sentence from article limit 1;
OK
+-------------------+--+
| article.sentence  |
+-------------------+--+
| Preface           |
+-------------------+--+
1 row selected (0.185 seconds)</code></pre>
<ul>
<li>HDFS数据目录<ol>
<li>入库的article.txt数据load进了HDFS对应目录</li>
<li>一句对应sentence列中的一行，现在要做Word Count，相当于将每行sentence切分为一个个单词，并转换进同一列（一个单词一行，所有单词并入同一列）</li>
</ol>
</li>
</ul>
<pre><code class="SQL">[root@master sbin]# hadoop fs -ls /usr/local/src/apache-hive-1.2.2-bin/warehouse/badou.db/article/
Found 1 items
-rwx-wx-wx   1 root supergroup     632207 2019-12-24 14:34 /usr/local/src/apache-hive-1.2.2-bin/warehouse/badou.db/article/operty.txt

--单词按照空格切分，结果为数组格式，以下是样例演示
--Hive里很特别，语句不限于SQL模式，可以将HQL的结果按照数据下标提取出来
select &#39;the heroic and that there&#39; as origin, 
split(&#39;the heroic and that there&#39;,&#39; &#39;) as origin_list, 
split(&#39;the heroic and that there&#39;,&#39; &#39;)[0] as word1, 
split(&#39;the heroic and that there&#39;,&#39; &#39;)[1] as word2;
+----------------------------+----------------------------------------+--------+---------+--+
|           origin           |              origin_list               | word1  |  word2  |
+----------------------------+----------------------------------------+--------+---------+--+
| the heroic and that there  | [&quot;the&quot;,&quot;heroic&quot;,&quot;and&quot;,&quot;that&quot;,&quot;there&quot;]  | the    | heroic  |
+----------------------------+----------------------------------------+--------+---------+--+

--单行转多行
select explode(split(&#39;the heroic and that there&#39;,&#39; &#39;)) origin_list_row;
OK
+------------------+--+
| origin_list_row  |
+------------------+--+
| the              |
| heroic           |
| and              |
| that             |
| there            |
+------------------+--+

--wc
select origin_list_row, count(1) cn from
(
    select explode(split(&#39;the heroic and that there&#39;,&#39; &#39;)) origin_list_row
)w --记得这里要加上子查询的别名，不然会出现以下错误
group by origin_list_row
limit 100;</code></pre>
<p>iii. 这里涉及了group by，会跑一组MR<br><img src="WC_MR.png" srcset="/img/loading.gif" alt="WC_MR." title="WC_MR."> </p>
<p><code>Tips</code>HQL内子查询的外侧要起别名，遗漏时会报错<br><img src="子查询别名.png" srcset="/img/loading.gif" alt="HQL子查询别名遗漏报错." title="HQL子查询别名遗漏报错."> </p>
<h5 id="正则过滤"><a href="#正则过滤" class="headerlink" title="正则过滤"></a>正则过滤</h5><pre><code class="SQL">--HQL
select regexp_extract(&#39;(mentioned&#39;, &#39;[[\\w]]+&#39;, 0);
select regexp_extract(&#39;(mentioned&#39;, &#39;[[0-9a-zA-Z]]+&#39;, 0);

--regexp_extract
select * from
(
    select regexp_extract(origin_list_row, &#39;[[0-9a-zA-Z]]+&#39;, 0) word, count(1) cn from
    -- select origin_list_row, count(1) cn from
    (
        select explode(split(sentence,&#39; &#39;)) origin_list_row from article
    )a --记得这里要加上子查询的别名，不然会出现以下错误
    group by regexp_extract(origin_list_row, &#39;[[0-9a-zA-Z]]+&#39;, 0) --Jobs:1  Map: 1  Reduce: 1
)b
where length(word) &gt; 0 --去除None值
order by cn desc
limit 100; --加上排序后，Jobs:2  Map: 2  Reduce: 2
--部分样例对比结果如下：
⬇ RE过滤                       ⬇ 原始单词
+-----------+-----+--+         +-----------------------+-----+--+
|   word    | cn  |            |    origin_list_row    | cn  |
+-----------+-----+--+         +-----------------------+-----+--+
|           | 35  |            |                       | 35  |
| Baynes    | 1   |            | (Baynes               | 1   |
| Dartie    | 1   |            | (Dartie               | 1   |
| Dartie    | 1   |            | (Dartie’s             | 1   |
| Down      | 2   |            | (Down-by-the-starn)   | 2   |
| Down      | 1   |            | (Down-by-the-starn),  | 1   |
| He        | 1   |            | (He                   | 1   |
| I         | 1   |            | (I                    | 1   |
| James     | 1   |            | (James)               | 1   |
| L500      | 1   |            | (L500)                | 1   |
| Louisa    | 1   |            | (Louisa               | 1   |
| Mrs       | 1   |            | (Mrs.                 | 1   |
| Roger     | 1   |            | (Roger                | 1   |
| Roger     | 1   |            | (Roger’s              | 1   |
| Soames    | 1   |            | (Soames               | 1   |
| Soames    | 1   |            | (Soames)              | 1   |
| The       | 1   |            | (The                  | 1   |
| a         | 5   |            | (a                    | 5   |
| also      | 1   |            | (also                 | 1   |
| although  | 1   |            | (although             | 1   |
+-----------+-----+--+         +-----------------------+-----+--+

--where条件的添加，去除空值
+-----------+-------+--+       +-----------+-------+--+
|  b.word   | b.cn  |          |  b.word   | b.cn  |   
+-----------+-------+--+       +-----------+-------+--+
| the       | 5168  |          | the       | 5168  |   
| of        | 3425  |          | of        | 3425  |   
| to        | 2822  |          | to        | 2822  |   
| and       | 2686  |          | and       | 2686  |   
| a         | 2564  |          | a         | 2564  |   
| he        | 2251  |          | he        | 2251  |   
| his       | 1929  |          | his       | 1929  |   
| in        | 1753  |          | in        | 1753  |   
| was       | 1745  |          | was       | 1745  |   
| had       | 1534  |          | had       | 1534  |   
| that      | 1387  |          | that      | 1387  |   
|           | 1309  |          | her       | 1200  |   
| her       | 1200  |          | with      | 1037  |   
| with      | 1037  |          | it        | 976   |   
| it        | 976   |          | at        | 822   |   
| at        | 822   |          | for       | 798   |   
+-----------+-------+--+       +-----------+-------+--+</code></pre>
<hr>
<h4 id="4-3-分区-分桶（4-1中有建立过程，具体分析见0x）"><a href="#4-3-分区-分桶（4-1中有建立过程，具体分析见0x）" class="headerlink" title="4-3 分区/分桶（4.1中有建立过程，具体分析见0x）"></a>4-3 分区/分桶（4.1中有建立过程，具体分析见0x）</h4><h3 id="05-数据类型"><a href="#05-数据类型" class="headerlink" title="05. 数据类型"></a>05. 数据类型</h3><h4 id="array"><a href="#array" class="headerlink" title="array"></a>array</h4><pre><code class="sql">create table hive_array(name string, citys array&lt;string&gt;)
row format delimited fields terminated by &#39;\t&#39; collection items terminated by &#39;,&#39;;
--数据样式：huangbo beijing,shanghai,tianjin,hangzhou
load data local inpath &quot;/data/hive/array0217.txt&quot; into table hive_array;

--查询
select citys, citys[0] from hive_array;</code></pre>
<h4 id="map（dict）"><a href="#map（dict）" class="headerlink" title="map（dict）"></a>map（dict）</h4><pre><code class="sql">create table hive_map(name string, score map&lt;string, int&gt;)
row format delimited fields terminated by &#39;\t&#39; 
collection items terminated by &#39;,&#39;
map keys terminated by &#39;:&#39;;
--数据样式：huangbo yuwen:80,shuxue:89,yingyu:95
--实际工程中可以将map拆分成多个字段，便于使用

--查询
select citys, citys[&#39;yingyu&#39;] from hive_map;</code></pre>
<h4 id="struct"><a href="#struct" class="headerlink" title="struct"></a>struct</h4><pre><code class="sql">create table hive_struct(id int, score struct&lt;course:string, value:int&gt;)
row format delimited fields terminated by &#39;\t&#39;
collection items terminated by &#39;,&#39;;
--数据样式：1 english,90

--查询
select t.score.course, t.score.value from hive_struct t;</code></pre>
<h4 id="视图"><a href="#视图" class="headerlink" title="视图"></a>视图</h4><pre><code class="sql">create view hive_student_view select department, count(1) from student group by department;</code></pre>
<hr>
<h3 id="06-多字节分隔符（分组正则表达式）"><a href="#06-多字节分隔符（分组正则表达式）" class="headerlink" title="06. 多字节分隔符（分组正则表达式）"></a>06. 多字节分隔符（分组正则表达式）</h3><ul>
<li>默认的SERDE只能支持单字节的分隔符，替换默认的SERDE，可以使其支持多字节分隔符.</li>
<li>RegexSerde正则分隔符解析，利用正则的规则来进行分组，每一组就是一个字段.</li>
</ul>
<pre><code class="bash">input.regex=&#39;(.*),,,(.*),,,(.*)&#39;
output.format.string=&#39;%1$s %7$s %15$s ... %n$s&#39;</code></pre>
<pre><code class="sql">--查询
desc formatted hive_ms;
--万能替换
01,,,huangbo
02,,,xuzheng
03,,,wangbaoqiang

create table hive_ms(id string,name string)
row format serde &#39;org.apache.hadoop.hive.serde2.RegexSerDe&#39;
with serdeproperties(&#39;input.regex&#39;=&#39;(.*),,,(.*),,,(.*)&#39;,&#39;output.format.string&#39;=&#39;%1$s %2$s %3$s&#39;)
stored as textfile;</code></pre>
<h3 id="07-分析函数"><a href="#07-分析函数" class="headerlink" title="07. 分析函数"></a>07. 分析函数</h3><ul>
<li>样例数据：年月日+温度</li>
</ul>
<pre><code class="bash">2014010114
2014010216
...
2015010649
2015010722
...
2020010999
2020011023</code></pre>
<h4 id="row-number-over-partition-by-…-order-by-…"><a href="#row-number-over-partition-by-…-order-by-…" class="headerlink" title="row_number() over(partition by … order by …)"></a>row_number() over(partition by … order by …)</h4><p><code>row_number()</code>按照一定的字段，划分每组的rank（按顺序编号，不留空位），并求出每组的TopN</p>
<pre><code class="sql">--求出每一年的最高温度是那一天（日期， 最高温度）
select * from 
(
    select year, dt, temp, 
    row_number()over(partition by year order by temp) as seq 
    from hive_temp_year
) t1 --按照年份划分，比如19年一组，20年一组，每组各自排序
where t1.seq &lt;=3; --取出TopN</code></pre>
<h4 id="dense-rank-over-partition-by-…-order-by-…"><a href="#dense-rank-over-partition-by-…-order-by-…" class="headerlink" title="dense_rank() over(partition by … order by …)"></a>dense_rank() over(partition by … order by …)</h4><p><code>dense_rank()</code>按顺序编号，相同的值同编号，不留空位</p>
<pre><code class="sql">select * from 
(
    select year, dt, temp, 
    dense_rank()over(partition by year order by temp) as seq 
    from hive_temp_year
) t1 --按照年份划分，比如19年一组，20年一组，每组各自排序
where t1.seq &lt;=3; --取出TopN</code></pre>
<h4 id="rank-over-partition-by-…-order-by-…"><a href="#rank-over-partition-by-…-order-by-…" class="headerlink" title="rank() over(partition by … order by …)"></a>rank() over(partition by … order by …)</h4><p><code>rank()</code>按顺序编号，相同的值同编号，留出空位</p>
<pre><code class="sql">select * from 
(
    select year, dt, temp, 
    rank()over(partition by year order by temp) as seq 
    from hive_temp_year
) t1 --按照年份划分，比如19年一组，20年一组，每组各自排序
where t1.seq &lt;=3; --取出TopN</code></pre>
<hr>
<h3 id="07-窗口函数"><a href="#07-窗口函数" class="headerlink" title="07. 窗口函数"></a>07. 窗口函数</h3><h4 id="sum-avg-max-min-over-partition-by-…-rows-between-…"><a href="#sum-avg-max-min-over-partition-by-…-rows-between-…" class="headerlink" title="sum/avg/max/min() over(partition by … rows between …)"></a>sum/avg/max/min() over(partition by … rows between …)</h4><ul>
<li>用户/时间/点击次数</li>
</ul>
<pre><code class="bash">cookie1,2015-04-10,1
cookie1,2015-04-11,5
cookie1,2015-04-12,7
cookie1,2015-04-13,3
cookie1,2015-04-14,2
cookie1,2015-04-15,4
cookie1,2015-04-16,4</code></pre>
<ul>
<li>窗口函数上下边界<ol>
<li>默认没有给范围，那就是该partition内的第一条到当前这条：rows between unbounded preceding and current row</li>
<li>也可以给范围：rows between A and B<br> unbounded preceding  ⬅ 窗口最开始<br> 3 preceding     ⬅ 往前3行<br> 1 following     ⬅ 往后1行<br> unbounded following  ⬅ 到最后一行</li>
</ol>
</li>
</ul>
<pre><code class="sql">select cookieid,createtime,pv,
    sum(pv) over(partition by cookieid order by createtime) as pv1, -- 默认为从起点到当前行
    sum(pv) over(partition by cookieid order by createtime rows between unbounded preceding and current row) as pv2,    --从起点到当前行，结果同pv1
    sum(pv) over(partition by cookieid) as pv3, --分组内所有行
    sum(pv) over(partition by cookieid order by createtime rows between 3 preceding and current row) as pv4,    --当前行+往前3行
    sum(pv) over(partition by cookieid order by createtime rows between 3 preceding and 1 following) as pv5,    --当前行+往前3行+往后1行
    sum(pv) over(partition by cookieid order by createtime rows between current row and unbounded following) as pv6 --当前行+往后所有行
from cookie;</code></pre>
<h3 id="07-explode-amp-LATERAL-VIEW"><a href="#07-explode-amp-LATERAL-VIEW" class="headerlink" title="07. explode &amp; LATERAL VIEW"></a>07. explode &amp; LATERAL VIEW</h3><h4 id="爆炸函数"><a href="#爆炸函数" class="headerlink" title="爆炸函数"></a>爆炸函数</h4><p><code>explode</code>将数组转成多行一列：”a-b-c”；将字典转成多行多列：{name:ghr,score:85}</p>
<hr>
<h3 id="0x-分桶"><a href="#0x-分桶" class="headerlink" title="0x. 分桶"></a>0x. 分桶</h3><h4 id="有点绕的概念（还未弄懂，单独拎出来操作）"><a href="#有点绕的概念（还未弄懂，单独拎出来操作）" class="headerlink" title="有点绕的概念（还未弄懂，单独拎出来操作）"></a>有点绕的概念（还未弄懂，单独拎出来操作）</h4><p><code>语法</code> tablesample(bucket x out of y on id)</p>
<pre><code class="SQL">TABLESAMPLE (BUCKET x OUT OF y [ON colname])</code></pre>

            <hr>
          </div>
          <br>
          <div>
            <p>
            
            
              <span>
                <i class="iconfont icon-tag"></i>
                
                  <a class="hover-with-bg" href="/tags/Hive">Hive</a>
                
              </span>
            
            </p>
            
              <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" target="_blank" rel="nofollow noopener noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
            
          </div>
        </div>
      </div>
    </div>
    <div class="d-none d-lg-block col-lg-2 toc-container">
      
  <div id="toc">
    <p class="h4"><i class="far fa-list-alt"></i>&nbsp;TOC</p>
    <div id="tocbot"></div>
  </div>

    </div>
  </div>
</div>

<!-- custom -->


<!-- Comments -->
<div class="col-lg-7 mx-auto nopadding-md">
  <div class="container comments mx-auto" id="comments">
    
      <br><br>
      
      

    
  </div>
</div>

    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  <footer class="mt-5">
  <div class="text-center py-3">
    <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
    <i class="iconfont icon-love"></i>
    <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    <br>

    
  
    <!-- 不蒜子统计PV -->
    
    &nbsp;<span id="busuanzi_container_site_pv">总访问量 
          <span id="busuanzi_value_site_pv"></span> 次</span>&nbsp;
  
  
    <!-- 不蒜子统计UV -->
    
    &nbsp;<span id="busuanzi_container_site_uv">总访客数 
            <span id="busuanzi_value_site_uv"></span> 人</span>&nbsp;
  
  <br>



    


    <!-- cnzz Analytics icon -->
    

  </div>
</footer>

<!-- SCRIPTS -->
<script src="/lib/jquery/jquery.min.js" ></script>
<script src="/lib/popper/popper.min.js" ></script>
<script src="/lib/bootstrap/js/bootstrap.min.js" ></script>
<script src="/lib/mdbootstrap/js/mdb.min.js" ></script>
<script src="/js/main.js" ></script>


  <script src="/js/lazyload.js" ></script>



  
    <script src="/lib/tocbot/tocbot.min.js" ></script>
  
  <script src="/js/post.js" ></script>



  <script src="/lib/smooth-scroll/smooth-scroll.min.js" ></script>



  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>


<!-- Plugins -->


  

  

  

  

  <!-- cnzz Analytics -->
  



  <script src="/lib/prettify/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script src="/lib/typed/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "20200204_1620_Hive&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script src="/lib/anchor/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "false",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script src="/lib/fancybox/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>











</body>
</html>
