<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>20200103-1945-MURA-MySQL</title>
    <link href="/2020/01/03/20200103-1945-MURA-MySQL/"/>
    <url>/2020/01/03/20200103-1945-MURA-MySQL/</url>
    
    <content type="html"><![CDATA[<h2 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h2><h3 id="01-MySQL-5-7"><a href="#01-MySQL-5-7" class="headerlink" title="01. MySQL 5.7"></a>01. MySQL 5.7</h3><ul><li>Repo Point<br><a href="https://www.g.cn" target="_blank" rel="noopener">MySQL</a></li></ul><h3 id="02-环境配置"><a href="#02-环境配置" class="headerlink" title="02. 环境配置"></a>02. 环境配置</h3><ul><li>基本环境配置</li></ul><pre><code class="bash"># 1 查看并卸载自带的MySQLrpm -qa|grep mysql# 2 发现的话就卸载掉rpm -e --nodeps mysql...# 3 删除老版本MySQL的开头文件和库rm -rf /usr/lib/mysqlrm -rf /usr/include/mysqlrm -rf /etc/my.cnfrm -rf /var/lib/mysql# 4 mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar与视频不太一样，需要按顺序安装一些依赖：rpm -ivh mysql-community-common-5.7.28-1.el7.x86_64.rpmrpm -ivh mysql-community-libs-5.7.28-1.el7.x86_64.rpmrpm -ivh mysql-community-client-5.7.28-1.el7.x86_64.rpmrpm -ivh mysql-community-server-5.7.28-1.el7.x86_64.rpm# 5 检查安装结果rpm -qa|grep mysql# 6-1 初始密码的位置/root/.mysql_secret# 打印瞧瞧：cat /root/.mysql_secret# 这个好像5.7不太一样，需要按如下方式获取：grep &#39;temporary password&#39; /var/log/mysqld.log# 6-2 修改密码策略&amp;重启服务：vim /etc/my.cnf添加如下行：[mysqld]validate_password=off# 7 set password=password(&#39;root&#39;);mysql -uroot -p&#39;root&#39;# exit下并重新进入。exit# 8 查看下端口，可以判断MySQL是否起来了netstat -nltptcp6       0      0 :::3306                 :::*                    LISTEN      978/mysqld# 9 增加远程登录权限show grants for root;select user,host from mysql.user;# 查询指定IP对应的连接权限，即@&#39;%&#39;GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;root&#39; WITH GRANT OPTION;# JDK 配置vim ~/.bashrc# SET JAVA_PATHexport JAVA_HOME=/usr/local/src/jdk1.8.0_201export JRE_HOME=${JAVA_HOME}/jreexport CLASSPATH=.:${JAVA_HOME}/lib:${JAVA_HOME}/libexport PATH=${JAVA_HOME}/bin:$PATH</code></pre><ul><li>MySQL 5.7 ERROR 1055 BUG<br>  MySQL5.7中默认会开启 <code>only_full_group_by</code> 这个模式，可以在配置文件中把它去掉</li></ul><pre><code class="bash">vim /etc/my.ini</code></pre><img src="ERROR-1055.png" srcset="/img/loading.gif" title="ERROR 1055" alt="ERROR 1055"><h3 id="03-Common-Operation"><a href="#03-Common-Operation" class="headerlink" title="03. Common Operation"></a>03. Common Operation</h3><ul><li><p>DB Create</p><pre><code class="SQL">create database dbname;drop database dbname;use dbname;show tables;</code></pre></li><li><p>其余均为常规DDL/DML/DQL/DCL</p></li><li><p>元数据配置，与Oracle不同，新增字段可以指定插入位置<br>```SQL</p></li></ul><p>–指定位置插入新字段<br>alter table student add card char(8) after first;<br>alter table student add card char(8) after sname;</p><p>–修改数据类型<br>alter table tname change colname colname colType<br>alter table tname modify colname colType</p><p>–修改字段位置<br>alter table tname modify colname colType after colname2;</p><pre><code>* 主键约束/唯一约束（单表可配置多个）```SQLcreate table `student`(    sid int(11) primary key,    ...);create table `student`(    sid int(11) primary key,    ...,    primary key(sid, xxx));--主键alter table student add constraint PK_SID primary key(sid, xxx, xxx);alter table student drop primary key;--唯一约束alter table student add constraint UN_CARD unique(card, xxx, xxx);drop unique index UN_CARD on student;--主键自增create table `student`(    sid int(11) primary key auto_increment,    id_card char(18) unique);</code></pre><ul><li>域完整性（字段准确）：类型约束/非空约束/默认值<pre><code class="SQL">create table `person`(  sid int(11) primary key auto_increment,  pname varchar(20) not null,  sex bit(1) default 1);</code></pre></li></ul><pre><code>* 引用完整性（参照准确）```SQLcreate table `classroom`(    cid int primary key auto_increment,    cname varchar(20) not null);create table `stu`(    sid int primary key auto_increment,    sname varchar(20) not null,    cid int);alter table stu add constraint FK_CID foreign key(cid)references classroom(cid)</code></pre><ul><li>运算<pre><code class="SQL">select 3 / 2;select 3 div 2;   --1select 3 / 0;   --NULLselect 5 % 2;   --1select 3 &gt; 2;   --1select 3 &lt;&gt; 2;  --1</code></pre></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>MySQL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20191218_1300_Python3-Monitor（Up to date）</title>
    <link href="/2019/12/31/20191218-1300-Python3-Monitor/"/>
    <url>/2019/12/31/20191218-1300-Python3-Monitor/</url>
    
    <content type="html"><![CDATA[<h2 id="Python3-Oracle2CSV-Monitor"><a href="#Python3-Oracle2CSV-Monitor" class="headerlink" title="Python3 Oracle2CSV_Monitor"></a>Python3 Oracle2CSV_Monitor</h2><h3 id="01-数据库数据采集-amp-监控"><a href="#01-数据库数据采集-amp-监控" class="headerlink" title="01. 数据库数据采集&amp;监控"></a>01. 数据库数据采集&amp;监控</h3><ul><li><p>Repo Point<br>  cx_Oracle, csv, smtplib, email<br>  threading</p></li><li><p>class_email_daily3.py</p><pre><code class="python"># -*- coding:utf-8 -*-# @Author: Shin# @Date: 2019/11/20 15:14# @File: class_email_daily3.pyimport pprintimport sysimport os, re# import math# from os.path import abspath, join, dirname</code></pre></li></ul><p>import cx_Oracle<br>import csv<br>import smtplib</p><h1 id="import-numpy-as-np"><a href="#import-numpy-as-np" class="headerlink" title="import numpy as np"></a>import numpy as np</h1><p>import threading</p><h1 id="import-copy"><a href="#import-copy" class="headerlink" title="import copy"></a>import copy</h1><p>from email.mime.text import MIMEText<br>from email.header import Header<br>from email.utils import formataddr<br>from email.mime.multipart import MIMEMultipart</p><p>from time import sleep, ctime<br>from tqdm import tqdm</p><h1 id="CMD模式运行配置"><a href="#CMD模式运行配置" class="headerlink" title="CMD模式运行配置"></a>CMD模式运行配置</h1><p>from func_demo.func_f import date_f<br>from conf import bas_insert_conf<br>from conf import bas_mail_conf<br>from conf import sql_conf</p><h1 id="非CMD模式运行配置"><a href="#非CMD模式运行配置" class="headerlink" title="非CMD模式运行配置"></a>非CMD模式运行配置</h1><h1 id="from-src-conf-import-bas-mail-conf"><a href="#from-src-conf-import-bas-mail-conf" class="headerlink" title="from src.conf import bas_mail_conf"></a>from src.conf import bas_mail_conf</h1><h1 id="from-src-conf-import-sql-conf"><a href="#from-src-conf-import-sql-conf" class="headerlink" title="from src.conf import sql_conf"></a>from src.conf import sql_conf</h1><h1 id="from-src-conf-import-bas-mail-conf-1"><a href="#from-src-conf-import-bas-mail-conf-1" class="headerlink" title="from src.conf import bas_mail_conf"></a>from src.conf import bas_mail_conf</h1><h1 id="from-src-func-test-func-f-import-date-f"><a href="#from-src-func-test-func-f-import-date-f" class="headerlink" title="from src.func_test.func_f import date_f"></a>from src.func_test.func_f import date_f</h1><h1 id="sys-path-insert-0-join-abspath-dirname-file-‘-func-test-‘"><a href="#sys-path-insert-0-join-abspath-dirname-file-‘-func-test-‘" class="headerlink" title="sys.path.insert(0, join(abspath(dirname(file)), ‘../func_test/‘))"></a>sys.path.insert(0, join(abspath(dirname(<strong>file</strong>)), ‘../func_test/‘))</h1><p>sys.path.append(‘./func_test’)</p><p>class OracleExecution(object):<br>    # def <strong>init</strong>(self, sql=None, connect=None, check_style=None):<br>    def <strong>init</strong>(self):<br>        self.<strong>connect = None<br>        self.</strong>sql = None<br>        self.__check_style = None<br>        # self.message_str = ‘’<br>        self.rs = []<br>        self.message = ‘’<br>        self.message_data = []<br>        self.db = None</p><pre><code># # 装饰器用法1：# @property# def conf_f(self):#     return self.__connect, self.__sql, self.__check_style## @conf_f.setter# def conf_f(self, value):#     self.__connect = value[0]#     self.__sql = value[1]#     self.__check_style = value[2]# 装饰器用法2：&lt;1等价于2&gt;def get_conf_f(self):    return self.__connect, self.__sql, self.__check_styledef set_conf_f(self, value):    self.__connect = value[0]    self.__sql = value[1]    self.__check_style = value[2]conf_f = property(get_conf_f, set_conf_f)def connect_f(self):    try:        self.db = cx_Oracle.connect(self.__connect)    except Exception as e:        print(f&#39;Status: Failed to connect database.&#39;)        print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)        print(&#39;------------------&#39; * 2)    finally:        return self.dbdef execute_f(self):    # db = cx_oracle.connect(self.username + &quot;/&quot; + self.password + &quot;@&quot; +    try:        cur = self.db.cursor().execute(self.__sql)    except Exception as e:        print(f&#39;Status: Failed to execute SQL.\nSQL:  {self.conf_f[1]}&#39;)        print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)        print(&#39;------------------&#39; * 2)        self.db.rollback()    else:        self.rs = cur.fetchall()        # print(self.rs)        # print(type(self.rs))    # &lt;class &#39;list&#39;&gt;        if self.__check_style == &#39;JOB&#39;:            data_flag = 0            data_flag_bad = None            i = 0            for r1 in self.rs:                if i == 0:                    data_flag = r1[1]                    if data_flag != 0:                        data_flag_bad = r1[0]                    else:                        continue                elif r1[1] is not None:                    data_flag = data_flag + r1[1]                    if data_flag != 0:                        data_flag_bad = data_flag_bad + &#39;,&#39; + r1[0]                        # 这里也可以直接写成 data_flag_bad = data_flag_bad, r1[0]                    else:                        continue                else:                    data_flag = data_flag + 0                    if data_flag != 0:                        data_flag_bad = data_flag_bad + &#39;,&#39; + r1[0]                    else:                        continue                i += 1            if data_flag != 0:                self.message = &#39;数据库脚本或定时任务异常,请及时核查.报错任务号：&#39; + str(data_flag_bad)                # print(data_flag_bad)            else:                self.message = &#39;数据库脚本正常执行,详细监测日志请查看附件.&#39;            print(&#39;&lt;&#39; + date_f(0)[1] + &#39;&gt; : &#39; + self.message)        else:            self.message = &#39;定时邮件任务完成，详细信息请看附件.&#39;            print(&#39;&lt;&#39; + date_f(0)[1] + &#39;&gt; : &#39; + self.message)        cur.close()    finally:        self.db.close()        # print(type(self.message))        # print(self.rs)        return self.messagedef execute_split_f(self):    # message_data_all = []    # message_data = []    for r1 in self.rs:        # 如果写成 str(r1), list内的行对象list会变成字符串：list [(),()] -&gt; [&quot;()&quot;,&quot;()&quot;]        # 写入时会把每一组对象当成一个个完整的字符串，即一个字母一个数字进一个单元格        # 要写入.csv的文件，按行转换成最基本的list即可，一行一个list对象&lt;即元组tuple&gt;：list [(),()]；或list嵌套list：[[],[]]        # 无需转换为str再拼接为数组        try:            self.message_data.append(r1)  # 数据库拉取出来的一行为一个元组            # print(f&#39;r1:{r1}&#39;)            # r1:(datetime.datetime(2019, 12, 12, 0, 0), &#39;HW_4G_CM&lt;OMC1&gt;&#39;, &#39;00&#39;, 23, 11, 11, 11, 369.36, 357.26,            # 355.16, 0, -12.1, &#39;数量未变&#39;, &#39;大小波动小&#39;, &#39;/LTE/MOBILE/HUAWEI/OMC1/CM/&#39;)            # print(self.message_data)        except Exception as e:            print(e)    # print(self.message_data)    # print(type(self.message_data))    return self.message_data</code></pre><p>class FileWR(OracleExecution):<br>    # 初始化子类属性时，需带上父类属性，这种需全部初始化父类属性的写法，用在：调用含父类属性的父类方法时，不然实例化时，无法顺利调用父类方法<br>    # 调用不含父类属性的父类方法时，可以直接用：super(&lt;子类名&gt;, self).<strong>init</strong>()完成父类初始化<br>    # def <strong>init</strong>(self, local_file_path=None, title=None, connect=None, sql=None, check_style=None):<br>    def <strong>init</strong>(self, local_file_path=None, title=None):<br>        # super(FileWR, self).<strong>init</strong>(connect, sql, check_style)   # 初始化父类属性<br>        # super(FileWR, self).<strong>init</strong>()<br>        super().<strong>init</strong>()  # 简写<br>        # 初始化父类属性，父类属性有默认赋值时，在子类初始化时可以不再指明具体参数<br>        # 后续实例化时，若子类想调用父类属性，则可以直接调用父类属性并赋值；或写成注释行部分的形式去初始化父类属性，在初始化子类时一并加入父类属性<br>        self.local_file_path = local_file_path<br>        self.title = title<br>        # self.message_data = message_date<br>        self.file_name = ‘’  # 可以不定义在初始化内，只作为类的私有属性</p><pre><code>def file_write_f(self, message_date, job_flag, sleep_seconds=0.001):    try:        self.file_name = self.local_file_path + date_f(0)[0] + &#39;_&#39; + job_flag + &#39;.csv&#39;        # print(self.file_name)        with open(self.file_name, &#39;w&#39;, newline=&#39;&#39;, encoding=&#39;GBK&#39;) as file_1:            writer_csv = csv.writer(file_1)            writer_csv.writerow(self.title)            for row in tqdm(message_date, ncols=80):                # print([row])                writer_csv.writerow(row)  # csv提供的写入方法可以按行写入list，无需按照对象一个个写入，效率更高                sleep(sleep_seconds)                # for row in tqdm(iterable=message_date, ncols=80):                #     writer_csv.writerow(row)  # csv提供的写入方法可以按行写入list，无需按照对象一个个写入，效率更高                #     # sleep(0.05)                # file_size = len(message_date)                # for row in range(file_size):                #     writer_csv.writerow(message_date[row])                #     sys.stdout.write(&#39;\r[{0}] Percent:{1}%&#39;.format(&#39;=&#39;*int(row*50/(file_size-1)),                #     str(row*100/(file_size-1))))                #     if row == file_size:                #         sys.stdout.write(&#39;\r[{0}] Percent:{1}%&#39;.format(&#39;=&#39; * int(100), str(100)))                #         print(&#39;\n&#39;)                #     sleep(sleep_seconds)    except Exception as e:        print(e)</code></pre><p>class MailSender(object):<br>    def <strong>init</strong>(self):  # 邮件概览/正文/文件名(含路径)<br>        # self.mail_view = mail_view<br>        # self.mail_text = mail_text<br>        # self.mail_title = mail_title<br>        self.mail_attach = []<br>        # self.file_name = file_name<br>        self.msg = None<br>        # self.attach = None</p><pre><code>def mail_mime_action(self, receivers, message_body):    mail_sender = &#39;shinnosuke1028@qq.com&#39;    mail_password = &#39;ixwzutghdbtxbaie&#39;    mail_server = &#39;smtp.qq.com&#39;    # subject = &#39;Python SMTP 邮件测试...&lt;数据完整性监控(日常JOB/采集)&gt;&#39;    subject = &#39;Py-%s &lt;数据完整性监控(日常JOB/昨日采集)&gt;&#39; % date_f(0)[0]    # MIMEMultipart 形式构造邮件正文    self.msg = MIMEMultipart()  # 开辟一个带邮件的mail接口    self.msg[&#39;From&#39;] = formataddr([&#39;郭皓然测试&#39;, mail_sender])    self.msg[&#39;To&#39;] = formataddr([&#39;,&#39;.join(receivers), &#39;utf-8&#39;])  # 用&#39;,&#39;进行拼接，待拼接内容：join(x)内的x    self.msg[&#39;Subject&#39;] = Header(subject, &#39;utf-8&#39;)    # 正文loading    print(f&#39;Status: Mail body loading...&#39;)    self.msg.attach(MIMEText(message_body, &#39;plain&#39;, &#39;utf-8&#39;))    # self.msg.attach(MIMEText(message + &#39;\n&#39; + title + message_str, &#39;plain&#39;, &#39;utf-8&#39;))    # 邮件装载附件    # 方法1    # for fn in self.file_name:    #     attach_tmp = self.msg_attach(fn)    #     self.mail_attach.append(attach_tmp)    # 方法2    try:        print(f&#39;Status: Mail attachments loading...&#39;)        cur_list_re = []        for fn in os.walk(bas_mail_conf.mail_file_path_class):            print(f&#39;fn[-1]: {fn[-1]}&#39;)  # ./data_output/*            for cur in fn[-1]:                x = re.search(bas_mail_conf.file_pattern, cur)                print(f&#39;cur: {cur}, x: {x}&#39;)                # cur: 20191217_PKG.csv, x: None                # cur: 20191218_GATHER.csv, x: &lt;re.Match object; span=(0, 19), match=&#39;20191218_GATHER.csv&#39;&gt;                if x:                    cur_list_re.append(bas_mail_conf.mail_file_path_class + x.group())                else:                    continue        for rx in cur_list_re:            # print(f&#39;rx: {rx}&#39;)            tx_tmp = self.msg_attach(rx)            self.msg.attach(tx_tmp)        print(f&#39;Status: Mail loaded successfully.&#39;)    except Exception as e:        print(f&#39;Status: Failed to load mail...&#39;)        print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)        print(&#39;------------------&#39; * 2)    try:        server = smtplib.SMTP(mail_server, 25)  # 发件人邮箱中的SMTP服务器，SMTP服务端口是25        server.login(mail_sender, mail_password)  # 括号中对应的是发件人邮箱账号、邮箱密码        server.sendmail(mail_sender, receivers, self.msg.as_string())  # 括号中对应的是发件人邮箱账号、收件人邮箱账号、邮件内容发送        print(&#39;Status: Mail sended successfully.&#39;)        server.quit()  # 关闭连接    except Exception as e:        print(&#39;Status: Failed to send mail...&#39;)        print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)        print(&#39;------------------&#39; * 2)def msg_attach(self, file_name):    &quot;&quot;&quot;    :return:  type(attach): &lt;class &#39;email.mime.text.MIMEText&#39;&gt; 附件封装结果    Ex:        att1 = MIMEText(open(file_name, &#39;rb&#39;).read(), &#39;base64&#39;, &#39;utf-8&#39;)        att1[&quot;Content-Type&quot;] = &#39;application/octet-stream&#39;        att1[&quot;Content-Disposition&quot;] = &#39;attachment; filename=&#39; + file_name    # 这里的filename可任意，写什么名字，邮件中显示什么名字        msg.attach(att1)        att2 = MIMEText(open(file_name2, &#39;rb&#39;).read(), &#39;base64&#39;, &#39;utf-8&#39;)        att2[&quot;Content-Type&quot;] = &#39;application/octet-stream&#39;        att2[&quot;Content-Disposition&quot;] = &#39;attachment; filename=&#39; + file_name2    # 这里的filename可任意，写什么名字，邮件中显示什么名字        msg.attach(att2)    &quot;&quot;&quot;    attach = MIMEText(open(file_name, &#39;rb&#39;).read(), &#39;base64&#39;, &#39;utf-8&#39;)    attach[&quot;Content-Type&quot;] = &#39;application/octet-stream&#39;    attach[&quot;Content-Disposition&quot;] = &#39;attachment; filename=&#39; + file_name  # 这里的filename可任意，写什么名字，邮件中显示什么名字    # print(f&#39;type(attach): {type(attach)}&#39;)    return attach</code></pre><p>class MyThread(threading.Thread):<br>    def <strong>init</strong>(self, func=None, args=()):<br>        super().<strong>init</strong>()<br>        self.func = func<br>        self.args = args<br>        self.result = []</p><pre><code>def run(self):    self.result = self.func(*self.args)def get_result(self):    # noinspection PyBroadException    try:        # print(f&#39;Results return:&#39;)        return self.result    except Exception as e:        print(f&#39;Status: 线程返回结果.&#39;)        print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)        print(&#39;------------------&#39; * 2)        return 1</code></pre><p>######################################################<br>######################################################</p><h1 id="以下是装饰器修饰函数的用法，可省略代码的反复加工"><a href="#以下是装饰器修饰函数的用法，可省略代码的反复加工" class="headerlink" title="以下是装饰器修饰函数的用法，可省略代码的反复加工"></a>以下是装饰器修饰函数的用法，可省略代码的反复加工</h1><p>balance = []</p><p>def lock_f(lock_flag=’N’):<br>    def threading_f(f):<br>        def inner_f(<em>value):<br>            print(‘Status: 1号装饰器测试开始！’)<br>            global balance<br>            if lock_flag == ‘Y’:<br>                # i += 1<br>                lock = threading.RLock()<br>                with lock:<br>                    # r_lock.acquire()<br>                    print(f’Thread {threading.current_thread().getName()} is running. Time: {ctime()}’)<br>                    result = f(</em>value)<br>                    # pprint.pprint(results)<br>                    # print(type(results))    # &lt;class ‘tuple’&gt;<br>                    balance.append(result)<br>                    # r_lock.release()<br>                    print(f’Thread {threading.current_thread().getName()} end. Time: {ctime()}’)<br>                    print(‘1号装饰器测试结束！’)<br>            else:<br>                print(‘Status: 1号装饰器不再调用！’)<br>                print(f’Thread {threading.current_thread().getName()} is running. Time: {ctime()}’)<br>                result = f(*value)<br>                balance.append(result)<br>                print(f’Thread {threading.current_thread().getName()} end. Time: {ctime()}’)<br>            # print(f’balance: {balance}’)    # 线程结果集合 &lt;class: list&gt;<br>            # pprint.pprint(result)   # 单线程结果<br>            return balance</p><pre><code>    return inner_freturn threading_f</code></pre><p>def email_f(email_flag=’N’):<br>    def mail_post_f(f):<br>        def inner_f(i=0, <em>value):<br>            print(‘Status: 2号装饰器测试开始！’)<br>            if email_flag == ‘Y’:<br>                print(f’Thread {threading.current_thread().getName()} is running. Time: {ctime()}’)<br>                results = f(</em>value)</p><pre><code>            # 获取邮件正文body，这里定位到JOB返回的内容            body = f&#39;{results[&quot;JOB&quot;][1]}\n{&quot;,&quot;.join(bas_mail_conf.titleDict[&quot;CONF_JOB&quot;])}&#39;            # 中间对象初始化            body_tmp = None            i_tmp = None            # 拆分数据结果            for rs in results[&quot;JOB&quot;][2]:                # print(f&#39;i: {i}, rs: {rs}&#39;)    # tuple转正文中的字符串                # Ex:                # rs: (21, 0, datetime.datetime(2020, 1, 4, 8, 0), &#39;TRUNC(sysdate+1) + 8/(24)&#39;,..., &#39;PKG...;&#39;)                # rs: (41, 0, datetime.datetime(2020, 1, 3, 17, 0), &#39;TRUNC(sysdate+1) + 17/(24)&#39;,..., &#39;PKG...;&#39;)                # ...                # 转换每一组tuple为字符串并拼接，主要是为了时间的字符显示                for rn in rs:                    if body_tmp is None or i_tmp != i:                        body_tmp = str(rn)                        i_tmp = i                    else:                        body_tmp = str(body_tmp) + &#39;, &#39; + str(rn)                # print(f&#39;body_tmp:\n {body_tmp}&#39;)                # 按行拼接每一组转换后的tuple                body = body + &#39;\n&#39; + body_tmp                i += 1            # 打印body            # print(f&#39;body:\n{body}&#39;)            # 装载/发送            mail = MailSender()            mail.mail_mime_action(bas_mail_conf.receivers, body)            print(f&#39;Thread {threading.current_thread().getName()} end. Time: {ctime()}&#39;)            print(&#39;2号装饰器测试结束！&#39;)        else:            print(&#39;Status: 2号装饰器不再调用！&#39;)            print(f&#39;Thread {threading.current_thread().getName()} is running. Time: {ctime()}&#39;)            results = f(*value)            print(f&#39;Thread {threading.current_thread().getName()} end. Time: {ctime()}&#39;)        # pprint.pprint(results)        return results    return inner_freturn mail_post_f</code></pre><h1 id="程序编译时优先编译内层装饰器-再编译外层装饰器-编译顺序-email-f-gt-lock-f"><a href="#程序编译时优先编译内层装饰器-再编译外层装饰器-编译顺序-email-f-gt-lock-f" class="headerlink" title="程序编译时优先编译内层装饰器/再编译外层装饰器,编译顺序:email_f -&gt; lock_f"></a>程序编译时优先编译内层装饰器/再编译外层装饰器,编译顺序:email_f -&gt; lock_f</h1><h1 id="执行时-类似于Queue-先进后出-执行顺序-lock-f-执行-f-value-之前的内容-gt-email-f-gt-lock-f-f-value"><a href="#执行时-类似于Queue-先进后出-执行顺序-lock-f-执行-f-value-之前的内容-gt-email-f-gt-lock-f-f-value" class="headerlink" title="执行时,类似于Queue(先进后出),执行顺序:lock_f(执行 f(value) 之前的内容) -&gt; email_f -&gt; lock_f(f(value))"></a>执行时,类似于Queue(先进后出),执行顺序:lock_f(执行 f(<em>value) 之前的内容) -&gt; email_f -&gt; lock_f(f(</em>value))</h1><h1 id="email-f-‘Y’-1号装饰器-邮件的发送需要跳出多线程，不然会出现多次发送的现象"><a href="#email-f-‘Y’-1号装饰器-邮件的发送需要跳出多线程，不然会出现多次发送的现象" class="headerlink" title="@email_f(‘Y’)  # 1号装饰器  # 邮件的发送需要跳出多线程，不然会出现多次发送的现象"></a>@email_f(‘Y’)  # 1号装饰器  # 邮件的发送需要跳出多线程，不然会出现多次发送的现象</h1><p>@lock_f(‘Y’)  # 2号装饰器<br>def ora_job(conf_job, file_path, file_title):<br>    “””</p><pre><code>:param conf_job::param file_path::param file_title::return: &lt;class: list&gt;: [(job_flag, file_mail_view_tmp, file_mail_text_tmp),...,()]&quot;&quot;&quot;# with r_lock:# 实例化ora = FileWR(local_file_path=file_path, title=file_title)ora.conf_f = conf_job  # 列表按顺序，进行数据库检索配置job_flag = ora.conf_f[2]ora.connect_f()file_mail_view_tmp = str(ora.execute_f())del ora.message# print(&#39;1:&#39;, file_mail_view_tmp)file_mail_text_tmp = ora.execute_split_f()del ora.message_data# print(type(file_mail_text_tmp))ora.file_write_f(file_mail_text_tmp, job_flag, )return job_flag, file_mail_view_tmp, file_mail_text_tmp</code></pre><h1 id="最后总结send"><a href="#最后总结send" class="headerlink" title="最后总结send"></a>最后总结send</h1><p>@email_f(‘Y’)  # 1号装饰器  # 邮件的发送需要跳出多线程，不然会出现多次发送的现象<br>def main_job():<br>    # print(sys.path)</p><pre><code>####################################################### 准备工作threads = []# 实例化信息&amp;检索语句初始化sqlconf = sql_conf.sqlDict# 文件生成路径/各报表标题初始化filepath, filetitlejob = bas_mail_conf.mail_file_path_class, bas_mail_conf.titleDict############################################################################################################# 采集多线程初始化# r_lock = threading.RLock()for rs in sqlconf.keys():    t = MyThread(ora_job, (sqlconf[rs], filepath, filetitlejob[rs]))    threads.append(t)rt = None# 线程批量启动for rt in threads:    rt.start()dict_final = {}for rt in threads:    rt.join()for rn in range(len(threads)):    # print(f&#39;rt.get_result()[rn]: {rt.get_result()[rn]}&#39;)    # 为什么rt.get_result()一口气返回了所有线程的结果，线程返回的是一个生成器？？？    dict_final[rt.get_result()[rn][0]] = rt.get_result()[rn]  # {&#39;JOB&#39;: (&#39;JOB&#39;,&#39;&#39;,[(),(),()]),... }# pprint.pprint(dict_final)return dict_final</code></pre><p>def demo():<br>    print(f’This is class demo.’)</p><h1 id="以下是Demo"><a href="#以下是Demo" class="headerlink" title="以下是Demo"></a>以下是Demo</h1><p>if <strong>name</strong> == ‘<strong>main</strong>‘:<br>    print(‘Thread’, threading.current_thread().getName(), ‘is Running. Time: %s’ % date_f()[2])</p><pre><code>mail_dict_combine = main_job()# print(f&#39;mail_dict_combine_view:{mail_dict_combine[&quot;JOB&quot;][0]}&#39;)# print(f&#39;mail_dict_combine_view:{mail_dict_combine[&quot;JOB&quot;][1]}&#39;)# print(f&#39;mail_dict_combine_mail_text:{mail_dict_combine[&quot;JOB&quot;][2]}&#39;)# print(f&#39;mail_dict_combine:{mail_dict_combine[&quot;PKG&quot;][1]}&#39;)print(&#39;Thread&#39;, threading.current_thread().getName(), &#39;End. Time: %s&#39; % date_f()[2])# # 下次遍历前初始化字典的写法# mail_dict_combine.append(mail_dict)# mail_dict = {}# print(id(mail_dict_combine))    # 这里打印的内存值虽相同,但??????# # 注意:下面的写法有问题# 若直接mail_dict_combine.append(mail_dict),会出现覆盖情况,数据始终指向mail_dict初始内存,故只能取到最后一组数据# mail_dict_combine.append(mail_dict)# print(id(mail_dict_combine)) # 即多次打印这里的内存值相同# print(mail_dict_combine)############################################################################################################# # 单线程# for rs in sqlConf.keys():#     ora_job(sqlConf[rs], filePath, fileTitleJob[rs])######################################################</code></pre><pre><code>---* main.py```python# -*- coding:utf-8 -*-# @Author: Shin# @Date: 2020/1/3 14:48# @File: main.py# @Usage: Main Drive# CMD Modefrom class_email_daily_3 import *import threadingif __name__ == &#39;__main__&#39;:    print(&#39;Thread&#39;, threading.current_thread().getName(), &#39;is Running. Time: %s&#39; % date_f()[2])    # mail_dict_combine = main_job()    demo()    print(&#39;Thread&#39;, threading.current_thread().getName(), &#39;End. Time: %s&#39; % date_f()[2])    key = input(&#39;Put any key to quit...&#39;)</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>Python3</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20191223_2356-Oracle12cR2</title>
    <link href="/2019/12/28/20191221-1006-Oracle12cR2/"/>
    <url>/2019/12/28/20191221-1006-Oracle12cR2/</url>
    
    <content type="html"><![CDATA[<h2 id="OCP12c"><a href="#OCP12c" class="headerlink" title="OCP12c"></a>OCP12c</h2><h3 id="01-Oracle12cR2"><a href="#01-Oracle12cR2" class="headerlink" title="01. Oracle12cR2"></a>01. Oracle12cR2</h3><ul><li>Repo Point<br><a href="https://www.oracle.com/downloads/software-license-agreement.html#license-lightbox" target="_blank" rel="noopener">&lt;Oracle-12.2&gt;linuxx64_12201_database.zip</a></li></ul><h3 id="02-runInstaller"><a href="#02-runInstaller" class="headerlink" title="02. ./runInstaller"></a>02. ./runInstaller</h3><p><code>To be continued...</code></p><h3 id="03-Oracle12c-架构"><a href="#03-Oracle12c-架构" class="headerlink" title="03. Oracle12c 架构"></a>03. Oracle12c 架构</h3><h4 id="实例和内存结构"><a href="#实例和内存结构" class="headerlink" title="实例和内存结构"></a>实例和内存结构</h4><p><code>Instance</code> 实例其实就是物理内存段的一部分，相当于软件进程.<br><code>kernel.shmall</code> 为共享内存大小，按照192.168.62.103测试库来看，可以配置为 <strong>4294967296</strong> 个page（4KB/page）.<br><code>kernel.shmmax</code> 用于定义单个共享内存段的最大值，<code>kernel.shmmax</code> 设置应该足够大，能在一个共享内存段下容纳下整个的 <code>SGA</code> ，设置的过低可能会导致需要创建多个共享内存段，这样可能导致系统性能的下降，最大值为16GB（在大多数情况下，该值应该比SGA大）.按照192.168.62.103测试库（内存128G）来看，可以配置为 <strong>88719476736</strong>（82GB）.<br>53477376 pages / 219043332096 bytes</p><p><code>Ex</code> 内核参数，系统内存4G，下面为对应内核配置大小（出自官方白皮书）<br><img src="Linux内核参数.png" srcset="/img/loading.gif" title="Linux内核参数" alt="Linux内核参数"></p><p>实例内存由下面<strong>两组内存参数</strong>动态调整：<br><code>SGA</code>（total_mem × 80%） × 80%，一般不超过物理内存的1/2.<br><code>PGA</code>（total_mem × 80%） × 20%.</p><table><thead><tr><th>×</th><th>×</th><th>MEM参数</th></tr></thead><tbody><tr><td>×</td><td>×</td><td><code>MEMORY_MAX_TARGET</code>（静态）⬇</td></tr><tr><td><code>SGA_MAX_SIZE</code></td><td>×</td><td><code>MEMORY_TARGET</code>（动态）⬇</td></tr><tr><td>×</td><td><code>SGA_TARGET</code>⬇</td><td><code>PGA_AGGREGATE_TARGET</code>（PGA_AGGREGATE_LIMIT）⬇</td></tr><tr><td>×</td><td><code>DB_CACHE</code> <code>SHARED_POOL</code> <code>LARGE_POOL</code> …</td><td></td></tr></tbody></table><img src="RAM.png" srcset="/img/loading.gif" title="内存架构" alt="内存架构"><hr><h4 id="关于SGA和PGA的一点配置总结"><a href="#关于SGA和PGA的一点配置总结" class="headerlink" title="关于SGA和PGA的一点配置总结"></a>关于SGA和PGA的一点配置总结</h4><p><code>Tips</code> 相关总结不一定正确，仍需实验验证.<br>[相关博客：关于oracle11G的自动内存管理MEMORY_TARGET和MEMORY_MAX_TARGET]（<a href="https://blog.csdn.net/fjseryi/article/details/50818843）" target="_blank" rel="noopener">https://blog.csdn.net/fjseryi/article/details/50818843）</a><br><code>MEMORY_MAX_TARGET</code> 参数定义了 <code>MEMORY_TARGET</code> 可以达到的最大值，若未设置，则默认等于 <code>MEMORY_TARGET</code> 的值；该值为数据库初始化参数，不可动态调节，通过调整Spfile中的<code>MEMORY_MAX_TARGET</code>并重启实例，可以达到调整的目的.<br><code>MEMORY_TARGET</code> SGA + PGA. Oracle总共可以使用的共享内存大小，不可超过 <code>MEMORY_MAX_TARGET</code> 的大小，默认为0；该值可以动态调节，无需重启实例.<br><code>动态内存管理</code> 使用动态内存管理时，<code>MEMORY_TARGET</code>下的 <code>SGA_TARGET</code> 和 <code>PGA_AGGREGATE_TARGET</code> 代表它们各自内存区域的<strong>最小设置</strong>，要让Oracle完全控制内存管理，上述两个参数应该设置为0.</p><ul><li><p><code>MEMORY_TARGET</code>设置为非0值：<br>  → 设置了<code>SGA</code>/<code>PGA_AGGREGATE_TARGET</code>，则两个参数将各自作为最小值，作为各自的初始化目标值.<br>  → 未设置<code>SGA</code>/<code>PGA_AGGREGATE_TARGET</code>，则根据DB状态按照一个固定比例分配：<br>  SGA = MEMORY_TARGET * 60%.<br>  PGA_AGGREGATE_TARGET = MEMORY_TARGET * 40%.<br>  → 仅设置了两个中的一个，则 SGA = MEMORY_TARGET - PGA_AGGREGATE_TARGET；反之类似.</p></li><li><p><code>MEMORY_TARGET</code>设置为0或未设置：<br>  → 设置了<code>SGA</code>/<code>PGA_AGGREGATE_TARGET</code>，则自动调节 <code>SGA</code> 中的 Shared pool、Buffer Cache、Redo Log Buffer、Java Pool、Larger Pool等内存空间的大小；PGA 则依赖 <code>PGA_AGGREGATE_TARGET</code> 的大小。 <code>SGA</code>/<code>PGA_AGGREGATE_TARGET</code> 不能自动增长和自动缩小.<br>  → 未设置<code>SGA</code>/<code>PGA_AGGREGATE_TARGET</code>，第1点<code>SGA</code> 中的各二级内存配置需要被明确设定，<code>SGA</code>/<code>PGA_AGGREGATE_TARGET</code> 不能自动增长和自动缩小.<br>  → <code>MEMORY_MAX_TARGET</code> 设置而 <code>MEMORY_TARGET</code> = 0，这种情况不太懂？？？</p></li></ul><hr><h4 id="SGA下的各种缓冲区（二级内存配置）"><a href="#SGA下的各种缓冲区（二级内存配置）" class="headerlink" title="SGA下的各种缓冲区（二级内存配置）"></a>SGA下的各种缓冲区（二级内存配置）</h4><ul><li><code>Data Buffer Cache</code> 数据高速缓冲区，又分级为：Dirty Buffer/Free Buffer/Pinned Buffer<br>  → <code>Dirty Buffer</code> 脏缓冲区，当数据库发生 DML（Insert、Update、Delete）操作时，会对缓冲区内容进行修改，这样缓冲区的内容就会和相对应的数据文件不一致，这时，缓冲区标识为“脏缓冲区”.<br>  → <code>Free Buffer</code> 自由缓冲区，当“脏缓冲区”的内容被写入数据文件后，因为该缓冲区与相应数据文件部分内容一致，所以将这些缓冲区称为“自由缓冲区”；当执行 SELECT 语句时，会将对应数据文件部分数据读取到数据高速缓存的相应缓冲区，因为缓冲区与数据块内容完全一致，所以这些缓冲区也被称为“自由缓冲区”.<br>  → <code>Pinned Buffer</code> 忙缓冲区，指服务器进程正在访问的缓冲区.<br>  → 为了防止数据库高速缓冲区空间不够用，Oracle 会将脏缓冲区中的数据写入对应的数据文件中（<code>Redo.log</code>），以腾出空间给新的数据.</li></ul><p><code>Ex</code> 高速缓冲区的大小管理</p><pre><code class="bash"># 显示高速缓冲区的大小# &quot;0&quot; 表示数据库自动管理，这里表示设置的是最小值。show parameter db_cache_size# 修改数据库高速缓冲区大小alter system set db_cache_size=500m; # flush缓冲区，生产库慎用alter system flush buffer_cache</code></pre><hr><ul><li><code>Redo Log Buffer</code> 重做日志缓冲区（循环文件，redo01.log/redo02.log/redo03.log），由一条条重做项构成，大小初始化参数为LOG_BUFFER.</li></ul><hr><ul><li><code>Shared Pool</code> SGA的共享池，内含库缓存、数字字典缓冲区（执行计划的依赖来源）等.<br>  <code>数据字典缓冲区</code> 数据库参考信息（数据库结构/用户等）<br>  <code>库高速缓存</code> 共享SQL区和共享PL/SQL区.</li></ul><p><code>Ex</code> Shared Pool 展示</p><pre><code class="SQL">--当用户执行语句时SELECT * FROM emp WHERE empno=7788;--Oracle 需要查询数据字典 dba_tables 确定表 emp 是否存在--如果该表已经存在，还需要查询数据字典 dba_tab_columns 确定列 empno 在表 emp 中是否存在SELECT * FROM dba_tab_columns WHERE column_name = &#39;EMPNO&#39;;--然后才能生成执行语句的过程（执行计划），这些定义在首次查询时存入数据字典高速缓冲区.</code></pre><hr><ul><li><code>大型池 Java池 流池</code><br>主要是大型池的大小影响数据备份效率.</li></ul><hr><h4 id="PGA（了解即可）"><a href="#PGA（了解即可）" class="headerlink" title="PGA（了解即可）"></a>PGA（了解即可）</h4><p><code>PGA</code> 私有SQL、会话内存、SQL工作区.</p><h4 id="In-Memory-Column-Store"><a href="#In-Memory-Column-Store" class="headerlink" title="In-Memory Column Store"></a>In-Memory Column Store</h4><ul><li><code>In-Memory area</code> 适用于：<br>  → 资源表中的行非常多，但查询结果行不多.<br>  → 资源表中的列很多，但查询结果的列很少.<br>  → 查询聚集数据</li></ul><h4 id="数据更新时相关进程的走势"><a href="#数据更新时相关进程的走势" class="headerlink" title="数据更新时相关进程的走势"></a>数据更新时相关进程的走势</h4><ul><li><p>用户进程：用户机器上的进程，在服务端体现为进程状态中的LOCAL和非LOCAL.</p><img src="用户进程.png" srcset="/img/loading.gif" title="用户进程" alt="用户进程"></li><li><p>后台进程：<code>LGWR</code> <code>DBWn</code> <code>CKPT</code>/ <code>SMON</code> <code>PMON</code> </p><img src="进程1.png" srcset="/img/loading.gif" title="Origin ➡ Buffer_cache" alt="Buffer_cache"></li></ul><hr><img src="进程2.png" srcset="/img/loading.gif" title="New ➡ Redo_buffer" alt="Redo_buffer">New ➡ Redo_buffer       <p>Redo_buffer ➡ Buffer_cache</p><hr><img src="进程3.png" srcset="/img/loading.gif" title="Redo_buffer ➡ Buffer_cache" alt="Buffer_cache">Redo_buffer ➡ redo01~03.log<p>Buffer_cache ➡ Origin &amp;&amp; Update SCN &amp;&amp; 数据库同步</p><pre><code class="bash">[oracle@ocp12c prod1]$ ll /u01/app/oracle/oradata/prod1total 3607664-rw-r----- 1 oracle oinstall   10600448 Dec 28 12:53 control01.ctl-rw-r----- 1 oracle oinstall  209715712 Dec 28 12:52 redo01.log-rw-r----- 1 oracle oinstall  209715712 Dec 28 11:46 redo02.log-rw-r----- 1 oracle oinstall  209715712 Dec 28 11:47 redo03.log-rw-r----- 1 oracle oinstall  817897472 Dec 28 12:51 sysaux01.dbf-rw-r----- 1 oracle oinstall  933240832 Dec 28 12:50 system01.dbf-rw-r----- 1 oracle oinstall   67117056 Dec 28 09:36 temp01.dbf-rw-r----- 1 oracle oinstall 1289756672 Dec 28 12:50 undotbs01.dbf-rw-r----- 1 oracle oinstall    5251072 Dec 28 11:51 users01.dbf</code></pre><hr><h4 id="数据更新时相关日志的走势"><a href="#数据更新时相关日志的走势" class="headerlink" title="数据更新时相关日志的走势"></a>数据更新时相关日志的走势</h4><p><code>Tips</code> <code>redo.log/undo.data</code>之间的一点关联和解析，需结合上述后台进程进行理解<br><img src="redo_log&undo_data.png" srcset="/img/loading.gif" title="数据更新流程" alt="数据更新流程"></p><hr><h3 id="04-实例管理"><a href="#04-实例管理" class="headerlink" title="04. 实例管理"></a>04. 实例管理</h3><h4 id="实例加载"><a href="#实例加载" class="headerlink" title="实例加载"></a>实例加载</h4><ul><li><p><code>NOMOUNT</code> 进程启动<br>  $ORACLE_HOME/dbs/ 下的 spfile<SID>.ora/init<SID>.ora<br>  打开alter_<SID>.log/trace文件<br>  分配SGA，启动进程</p></li><li><p><code>MOUNT</code> 挂载状态<br>  定位所有控制文件<br>  通过加载控制文件，定位实例数据文件/Redo日志在哪里，但不介入（不判断是否真的存在），加载后台实例/动态性能视图，</p></li><li><p><code>OPEN</code> 介入数据文件，打开数据库数据文件/Redo日志</p></li></ul><pre><code class="SQL">--NOMOUNT&amp;OPENSYS@prod1&gt; startup nomountSYS@prod1&gt; alter database mountSYS@prod1&gt; alter database open--MOUNT&amp;OPENSYS@prod1&gt; startup mountSYS@prod1&gt; alter database open--STARTUPSYS@prod1&gt; startup</code></pre><h4 id="实例关闭"><a href="#实例关闭" class="headerlink" title="实例关闭"></a>实例关闭</h4><pre><code class="bash">SYS@prod1&gt; shutdown immediateSYS@prod1&gt; shutdown abort   ⬅断电，生产环境应避免使用，数据库不会进行同步</code></pre><h4 id="动态性能视图"><a href="#动态性能视图" class="headerlink" title="动态性能视图"></a>动态性能视图</h4><ul><li>数据库在MOUNT阶段就激活的功能，因为是动态视图，所以其数值的变化会贯穿整个数据库的启动状态.</li></ul><h4 id="ORACLE-HOME-dbs"><a href="#ORACLE-HOME-dbs" class="headerlink" title="$ORACLE_HOME/dbs"></a>$ORACLE_HOME/dbs</h4><p><code>spfile&lt;sid&gt;.ora/init&lt;sid&gt;.ora</code> 初始化文件，后者为文本可读格式的静态文件，后续有详细介绍.</p><hr><h3 id="05-数据文件详解"><a href="#05-数据文件详解" class="headerlink" title="05. 数据文件详解"></a>05. 数据文件详解</h3><h4 id="控制文件-Control-Files"><a href="#控制文件-Control-Files" class="headerlink" title="控制文件 Control Files"></a>控制文件 Control Files</h4><pre><code class="SQL">-- SYS@prod1&gt; DESC V$DATAFILE;-- SYS@prod1&gt; DESC DBA_DATA_FILES;--查看控制文件路径及数量SYS@prod1&gt; show parameter controlNAME                     TYPE     VALUE------------------------------------ ----------- ------------------------------control_file_record_keep_time        integer     7control_files                        string     /u01/app/oracle/oradata/prod1/control01.ctl, /u01/app/oracle/fast_recovery_area/prod1/control02.ctlcontrol_management_pack_access        string     DIAGNOSTIC+TUNING--备份--二进制备份SYS@prod1&gt; ALTER DATABASE BACKUP CONTROLFILE TO &#39;/home/oracle/control.ctl&#39;;--文本备份SYS@prod1&gt; ALTER DATABASE BACKUP CONTROLFILE TO trace AS &#39;/home/oracle/control.ctl&#39;;&lt;img src=&quot;control_ctl.png&quot; title=&quot;数据更新流程&quot; alt=&quot;数据更新流程&quot;&gt;--新增控制文件，数据库mountSYS@prod1&gt; startup mountORACLE instance started.Total System Global Area 1560281088 bytesFixed Size            8621088 bytesVariable Size          989856736 bytesDatabase Buffers      553648128 bytesRedo Buffers            8155136 bytesDatabase mounted.SYS@prod1&gt; show parameter controlNAME                     TYPE     VALUE------------------------------------ ----------- ------------------------------control_file_record_keep_time         integer     7control_files                 string     /u01/app/oracle/oradata/prod1/control01.ctl, /u01/app/oracle/fast_recovery_area/prod1/control02.ctlSYS@prod1&gt; alter system set control_files=&#39;/u01/app/oracle/oradata/prod1/control01.ctl&#39;,&#39;/u01/app/oracle/fast_recovery_area/prod1/control02.ctl&#39;,&#39;/u01/app/oracle/oradata/prod1/control03.ctl&#39; scope=spfile;--关闭实例SYS@prod1&gt; shutdown immediate--赋值control01文件--记住，赋值的文件需是实例关闭后，同步过SNC号的.ctl文件cp /u01/app/oracle/oradata/prod1/control01.ctl /u01/app/oracle/oradata/prod1/control03.ctl--重启实例SYS@prod1&gt; startupORACLE instance started.Total System Global Area 1560281088 bytesFixed Size            8621088 bytesVariable Size          989856736 bytesDatabase Buffers      553648128 bytesRedo Buffers            8155136 bytesDatabase mounted.Database opened.--查看新增control文件后的control参数状态SYS@prod1&gt; show parameter controlNAME                     TYPE     VALUE------------------------------------ ----------- ------------------------------control_file_record_keep_time         integer     7control_files                         string         /u01/app/oracle/oradata/prod1/control01.ctl, /u01/app/oracle/fast_recovery_area/prod1/control02.ctl, /u01/app/oracle/oradata/prod1/control03.ctl</code></pre><hr><ul><li>控制文件部分丢失或完全丢失的情况下重建控制文件<br>  ➡ 关闭实例/删除/启动/查看告警/关闭实例<br>  ➡ 复现：先删除某control文件</li></ul><img src="control1_miss.png" srcset="/img/loading.gif" title="某control文件丢失" alt="某control文件丢失"><ul><li>启动到mount即可（mount阶段已开始介入control文件）</li></ul><pre><code class="SQL">[oracle@ocp12c ~]$ sqlplus / as sysdbaSQL*Plus: Release 12.2.0.1.0 Production on Sat Dec 28 18:43:07 2019Copyright (c) 1982, 2016, Oracle.  All rights reserved.Connected to an idle instance.  ➡ 再次强调，此时数据库是关闭的！！！后续才可以进行正确的copy！！！SYS@prod1&gt; startup mountORACLE instance started.Total System Global Area 1560281088 bytesFixed Size            8621088 bytesVariable Size          989856736 bytesDatabase Buffers      553648128 bytesRedo Buffers            8155136 bytesORA-00205: error in identifying control file, check alert log for more info</code></pre><p>可以去按照提示，去告警日志里看下原因<br><img src="control1_miss2.png" srcset="/img/loading.gif" title="某control文件丢失" alt="某control文件丢失"></p><ul><li>恢复丢失的控制文件（在数据库实例完全关闭的情况下！！！）<pre><code class="BASH">SYS@prod1&gt; shutdown immediateORA-01507: database not mountedORACLE instance shut down.</code></pre></li></ul><p>–进入到控制文件所在目录，该目录下也包括一些初始化参数文件/密码文件等<br>cd $ORACLE_BASE/oradata/prod1/<br>–复制未丢失的control文件并重命名至control01.ctl<br>cp control03.ctl control01.ctl<br>–再次重启<br>SYS@prod1&gt; startup mount</p><pre><code>&lt;img src=&quot;control1_miss3.png&quot; title=&quot;某control文件丢失后恢复&quot; alt=&quot;某control文件丢失后恢复&quot;&gt;---#### 数据文件 Data Files```SQL--查看相关数据文件及路径select T.bytes/1024/1024/1024 &quot;TB/(GB)&quot;, T.* FROM DBA_DATA_FILES t /*where t.tablespace_name like &#39;UNDOTBS1%&#39;*/ ORDER BY 2 desc;--创建特定属性的表空间，后续也有类似的罗列CREATE TABLESPACE SHIN DATAFILE &#39;/u01/app/oracle/oradata/prod1/shin01.dbf&#39; SIZE 10M AUTOEXTEND ON NEXT 10M EXTENT MANAGEMENT LOCALUNIFORM SIZE 1M SEGMENT SPACE MANAGEMENT MANUAL;    ⬅ 默认是AUTOALTER TABLESPACE SHIN ADD DATAFILE &#39;/u01/app/oracle/oradata/prod1/shin02.dbf&#39; SIZE 10M;</code></pre><hr><h4 id="联机重做日志文件（归档日志）-redo-log"><a href="#联机重做日志文件（归档日志）-redo-log" class="headerlink" title="联机重做日志文件（归档日志） redo.log"></a>联机重做日志文件（归档日志） redo.log</h4><ul><li>修改归档日志路径</li></ul><pre><code class="SQL">mkdir /u01/app/oracle/archivelogSYS@prod1&gt; alter system set log_archive_dest_1=&#39;location=/u01/app/oracle/archivelog&#39; socpe=spfile;SYS@prod1&gt; alter system switch logfile; ⬅手动切换下日志试试，看看新路径下会不会有指定格式的归档日志SYS@prod1&gt; show parameter log_archive_dest_1;NAME                     TYPE     VALUE------------------------------------ ----------- ------------------------------log_archive_dest_1             string     location=/u01/app/oracle/archiveloglog_archive_dest_10             stringlog_archive_dest_11             stringlog_archive_dest_12             stringlog_archive_dest_13             stringlog_archive_dest_14             stringlog_archive_dest_15             stringlog_archive_dest_16             stringlog_archive_dest_17             stringlog_archive_dest_18             stringlog_archive_dest_19             string--手动切换日志SYS@prod1&gt; alter system switch logfile;--进入相关日志路径，查看是否产生了新的日志ls $ORACLE_BASE/archivelog--redo log日志组的增加alter database add logfile group 4 (&#39;/u01/app/oracle/oradata/prod1/redo04_01.log&#39;,&#39;/u01/app/oracle/oradata/prod1/redo04_02.log&#39;) size 50M;--redo log组新增成员alter database add logfile member&#39;/u01/app/oracle/oradata/prod1/redo04_03.log&#39;to group 4;--redo log组删除成员，删除时当前redo log成员的状态为不为CURRENT即可--查看当前日志成员状态SYS@prod1&gt; select group#, thread#, sequence#, bytes, members, archived, status from v$log;--if status(the member that wait to be drop) = CURRENT SYS@prod1&gt; alter system switch logfile;--drop logfile memberALTER DATABASE DROP LOGFILE MEMBER &#39;/u01/app/oracle/oradata/prod1/redo04_03.log&#39;;--删除redo log日志组，删除组的条件比删除成员的条件严苛，需要状态为 INACTIVE--如何将状态切换至INACTIVE? 手动切换当前的 redo.log 日志组，并触发checkpoint同步实例SYS@prod1&gt; alter system switch logfile;SYS@prod1&gt; select group#, thread#, sequence#, bytes, members, archived, status from v$log;--触发checkpoint，同步redo.log内的数据至数据文件内SYS@prod1&gt; alter system checkpoint;--此时再执行日志组的删除SYS@prod1&gt; alter database drop logfile group 4;</code></pre><hr><h3 id="06-操作系统文件详解"><a href="#06-操作系统文件详解" class="headerlink" title="06. 操作系统文件详解"></a>06. 操作系统文件详解</h3><h4 id="初始化文件"><a href="#初始化文件" class="headerlink" title="初始化文件"></a>初始化文件</h4><ul><li><p><code>spfile&lt;sid&gt;.ora</code> </p><img src="spfile.png" srcset="/img/loading.gif" title="spfile"></li><li><p><code>init&lt;sid&gt;.ora</code></p><img src="pfile.png" srcset="/img/loading.gif" title="pfile"></li></ul><h4 id="口令文件"><a href="#口令文件" class="headerlink" title="口令文件"></a>口令文件</h4><h4 id="归档文件"><a href="#归档文件" class="headerlink" title="归档文件"></a>归档文件</h4><pre><code class="SQL">--归档路径查询，这里的log_archive_dest_n为自己配置的路径，一般配置log_archive_dest_1即可SYS@prod1&gt; show parameter archiveNAME                            TYPE        VALUE-----------------------------   ----------- ------------------------------archive_lag_target                integer        0log_archive_config                stringlog_archive_dest                stringlog_archive_dest_1                string        location=/u01/app/oracle/archivelog...                             ...         ...log_archive_dest_7                stringlog_archive_dest_8                stringlog_archive_dest_9                stringlog_archive_dest_state_1        string        enablelog_archive_dest_state_10        string        enable...                             ...         ...log_archive_dest_state_17        string        enablelog_archive_dest_state_18        string        enablelog_archive_dest_state_19        string        enablelog_archive_dest_state_2        string        enable...                             ...         ...log_archive_dest_state_3        string        enablelog_archive_dest_state_9        string        enablelog_archive_duplex_dest         stringlog_archive_format                string        %t_%s_%r.dbflog_archive_max_processes        integer        4log_archive_min_succeed_dest    integer        1log_archive_start                boolean        FALSElog_archive_trace                integer        0standby_archive_dest            string        ?#/dbs/arch</code></pre><h4 id="Trace-File-amp-Alter-Log-File"><a href="#Trace-File-amp-Alter-Log-File" class="headerlink" title="Trace File &amp; Alter Log File"></a>Trace File &amp; Alter Log File</h4><pre><code class="SQL">--alert文件和.trc文件如下所示--第一部分.trc文件在如下命令显示的路径下SYS@prod1&gt; show parameter dumpNAME                                    TYPE            VALUE------------------------------------    -----------     ------------------------------background_core_dump                    string            partialbackground_dump_dest                    string            /u01/app/oracle/product/12.2.0/db_1/rdbms/logcore_dump_dest                            string            /u01/app/oracle/diag/rdbms/prod1/prod1/cdumpmax_dump_file_size                        string            unlimitedshadow_core_dump                        string            partialuser_dump_dest                            string            /u01/app/oracle/product/12.2.0/db_1/rdbms/log--user_dump_dest路径下有两组实例的.trc文件cd /u01/app/oracle/product/12.2.0/db_1/rdbms/log--alert文件和第二部分.trc文件在下面的路径下--下面这个路径下也有相关的.trc和alert文件，但是和上述的.trc文件不太类似，生成的时间周期不太一样cd /u01/app/oracle/diag/rdbms/prod1/prod1/trace</code></pre><ul><li>两组路径下有不同的.trc文件，有何区别.</li></ul><h3 id="07-数据库逻辑结构"><a href="#07-数据库逻辑结构" class="headerlink" title="07. 数据库逻辑结构"></a>07. 数据库逻辑结构</h3><h4 id="行片段-块-区-段-表空间"><a href="#行片段-块-区-段-表空间" class="headerlink" title="行片段/块/区/段/表空间"></a>行片段/块/区/段/表空间</h4><pre><code class="sql">CREATE TABLESPACE &lt;TBSPACE_NAME&gt; DATAFILE &#39;/u01/app/oracle/oradata/prod1/TBSPACE_NAME01.dbf&#39; size 10MAUTOEXTEND ON NEXT 10M  ⬅ 是指表空间的数据文件大小自动扩展，最大扩充至32GEXTENT MANAGEMENT LOCAL     ⬅ 区的管理是LOCALUNIFORM SIZE 1M             ⬅ 区以后是每次1MB来扩展，即表的实际大小按1MB大小递增SEGMENT SPACE MANAGEMENT MANUAL     ⬅ 默认是AUTO--TABLESPACE GROUP GROUP_TMP          ⬅ 一半临时表空间可以配置为临时表空间组，回避单临时表空间不足的问题</code></pre><hr><h4 id="表空间和数据文件"><a href="#表空间和数据文件" class="headerlink" title="表空间和数据文件"></a>表空间和数据文件</h4><p><code>SYSTEM</code> 不能脱机 offline / 不能置为只读 read only / 不能重命名 / 不能删除</p><pre><code class="SQL">create tablespace shin datafile &#39;/u01/&#39;</code></pre><p><code>TABLESPACE</code> 常规表空间，建议按需求功能配置不同表空间</p><pre><code class="SQL">CREATE TABLESPACE SHIN DATAFILE &#39;/u01/app/oracle/oradata/prod1/shin01.dbf&#39; SIZE 10M AUTOEXTEND ON NEXT 10M EXTENT MANAGEMENT LOCALUNIFORM SIZE 1M SEGMENT SPACE MANAGEMENT MANUAL;</code></pre><hr><p><code>TEMP TABLESPACE</code> 临时表空间</p><pre><code class="SQL">SYS@prod1&gt; create temporary tablespace tempts1 tempfile &#39;/home/oracle/temp1_02.dbf&#39; size 2Mtablespace group group1;SYS@prod1&gt; create temporary tablespace tempts2 tempfile &#39;/home/oracle/temp2_02.dbf&#39; size 2Mtablespace group group2;SYS@prod1&gt; select * from dba_tablespace_groups;GROUP_NAME TABLESPACE_NAME------------------------------ ------------------------------GROUP1 TEMPTS1GROUP2 TEMPTS2--将表空间从一个临时表空间组移动到另外一个临时表空间组：SYS@prod1&gt; alter tablespace tempts1 tablespace group GROUP2 ;SYS@prod1&gt; select * from dba_tablespace_groups;GROUP_NAME TABLESPACE_NAME------------------------------ ------------------------------GROUP2 TEMPTS1GROUP2 TEMPTS2</code></pre><hr><h4 id="段（表-索引-簇等）"><a href="#段（表-索引-簇等）" class="headerlink" title="段（表/索引/簇等）"></a>段（表/索引/簇等）</h4><h4 id="区"><a href="#区" class="headerlink" title="区"></a>区</h4><h4 id="块"><a href="#块" class="headerlink" title="块"></a>块</h4><hr><h4 id="高水位线（-High-Water-Mark-HWM）"><a href="#高水位线（-High-Water-Mark-HWM）" class="headerlink" title="高水位线（(High Water Mark, HWM）"></a>高水位线（(High Water Mark, HWM）</h4><p><code>HWM</code> 原则上HWM只会增大，不会缩小.</p><ul><li><p>相关影响<br>  → 全表扫描通常要读出直到 HWM 标记的所有的属于该表数据库块，即使该表中没有任何数据.<br>  → 即使 HWM 以下有空闲的数据库块，键入在插入数据时使用了 append 关键字，则在插入时使用 HWM 以上的数据块，此时 HWM 会自动增大（插入速度快）.<br>  → HWM会直接影响到相关表空间的大小，即resize表空间时会失败.<br>  → 通常需要我们去优化这些高水位线但实际数据很少的表.</p></li><li><p><code>HWM</code> 的修正</p></li></ul><pre><code class="SQL">&lt;img src=&quot;HWM.png&quot; title=&quot;HWM修正&quot; alt=&quot;HWM修正&quot;&gt;--实际数据低于高水位线30%的表的查询SELECT TABLE_NAME,(BLOCKS*8192/1024/1024)&quot;理论大小 M&quot;,(NUM_ROWS*AVG_ROW_LEN/1024/1024/0.9)&quot;实际大小 M&quot;,round((NUM_ROWS*AVG_ROW_LEN/1024/1024/0.9)/(BLOCKS*8192/1024/1024),3)*100||&#39;%&#39; &quot; 实际使用率%&quot;FROM DBA_TABLES where blocks&gt;100 and (NUM_ROWS*AVG_ROW_LEN/1024/1024/0.9)/(BLOCKS*8192/1024/1024)&lt;0.3 order by (NUM_ROWS*AVG_ROW_LEN/1024/1024/0.9)/(BLOCKS*8192/1024/1024) desc--重建并收缩alter table te123 enable ROW MOVEMENT; --表重建，行迁移alter table te123 shrink space cascade;--alter database datafile &#39;/u01/app/oracle/oradata/prod1/te123&#39; resize 15M;</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>Oracle12c</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20191226_1309_Python3-FTP-list</title>
    <link href="/2019/12/26/20191225-1309-Python3-FTP-list/"/>
    <url>/2019/12/26/20191225-1309-Python3-FTP-list/</url>
    
    <content type="html"><![CDATA[<h2 id="Python3-FTP-List-Gather"><a href="#Python3-FTP-List-Gather" class="headerlink" title="Python3 FTP_List_Gather"></a>Python3 FTP_List_Gather</h2><h3 id="01-服务器数据清单采集（暂时不含可视化部分）"><a href="#01-服务器数据清单采集（暂时不含可视化部分）" class="headerlink" title="01. 服务器数据清单采集（暂时不含可视化部分）"></a>01. 服务器数据清单采集（暂时不含可视化部分）</h3><ul><li>Repo Point<br>  ftplib, os, socket, re<br>  Oracle2File.FileWR（class_email_daily3.py）<br>  func_demo.func_f.date_f</li></ul><pre><code class="bash"># tree ftp_monitor/src -L 2$ tree src -L 3src|-- __init__.py|-- __pycache__|   `-- __init__.cpython-37.pyc|-- conf|   |-- __init__.py|   `-- ftp_conf_bak.py|-- data|-- data_output|   `-- __init__.py|-- ftp_wc.py|-- func_demo|   |-- Oracle2File.py|   |-- __init__.py|   |-- __pycache__|   |-- func_f.py|   `-- os_f.py`-- main.py</code></pre><hr><pre><code class="python"># -*- coding:utf-8 -*-# @Author: Shin# @Date: 2019/12/5 11:48# @File: main.py# @Usage: Main# Self Repofrom src.ftp_wc import *from src.func_demo.os_f import file_createfrom concurrent.futures import ThreadPoolExecutor, as_completeddef ftp_job(flag, local_path, file_flag, file_title ):    &quot;&quot;&quot;    :param flag:        采集标签，标签用来区分清单采集的厂家和内容    :param local_path:  清单输出路径    :param file_flag:   hint to distinguish between different flag_lists    :param file_title:  file title    Ex:        (main_job(&#39;HW_CM&#39;), local_path=file_nlst_path, file_flag=fileDict[&#39;HW_CM&#39;][&#39;flag&#39;], file_title=fileDict[&#39;HW_CM&#39;][&#39;title&#39;])        # 清单标签,和采集清单一一对应        fileDict = {            &#39;LOCAL&#39;:{                &#39;title&#39;: (&#39;flag&#39;, &#39;LOCAL&#39;),                &#39;flag&#39;: &#39;FileList&#39;            },            &#39;HW_CM&#39;:{                &#39;title&#39;: (&#39;flag&#39;, &#39;85_HW_CM&#39;),                &#39;flag&#39;: &#39;85_HW_CM&#39;            },            &#39;HW_CM_NSA&#39;: {                &#39;title&#39;: (&#39;flag&#39;, &#39;85_HW_CM_NSA&#39;),                &#39;flag&#39;: &#39;85_HW_CM&#39;            },            &#39;NSN_CM&#39;: {                &#39;title&#39;: (&#39;flag&#39;, &#39;86_NSN_CM&#39;),# &#39;86_NSN_CM&#39;,                &#39;flag&#39;: &#39;86_NSN_CM&#39;            },        }    :return:    counter: 0 or 1    &quot;&quot;&quot;    res_list = main_job(flag)    # pprint.pprint(res_list)    if len(res_list[&quot;error_list&quot;]):        pass    else:        ftp_nlst_write(res_list[&quot;ftp_findall&quot;], local_path=local_path, file_flag=file_flag, file_title=file_title)    return res_list[&quot;error_list&quot;]if __name__ == &#39;__main__&#39;:    # 测试用文件&amp;文件夹生成，生产环境无需部署以下两步    file_create(d_path, 0, d_name)    file_create(f_path, 1, *file_name_list)    print(&#39;------------------&#39;*2)    all_task = []    # 方法3 ThreadPoolExecutor    # 需要打印时替换方法2    executor = ThreadPoolExecutor(max_workers=5)    for rs in fileDict.keys():        if rs in [&#39;LOCAL&#39;, &#39;HW_CM_NSA&#39;]:            future = executor.submit(ftp_job, rs, file_nlst_path, fileDict[rs][&#39;flag&#39;], fileDict[rs][&#39;title&#39;])            all_task.append(future)            print(f&#39;Future: {future}\n&#39;)    # # 方法2 ThreadPoolExecutor    # executor = ThreadPoolExecutor(max_workers=5)    # all_task = [executor.submit(ftp_job, rs, file_nlst_path, fileDict[rs][&#39;flag&#39;], fileDict[rs][&#39;title&#39;])    #             for rs in fileDict.keys() if rs in [&#39;LOCAL&#39;, &#39;HW_CM_NSA&#39;] ]    A = None    for future in as_completed(all_task):        if not future.result():            print(f&#39;Future: {future}, OK&#39;)        else:            print(f&#39;Future: {future}, Error: {future.result()}&#39;)</code></pre><hr><pre><code class="Python"># -*- coding:utf-8 -*-# @Author: Shin# @Date: 2019/12/5 11:13# @File: ftp_wc.py# @Usage: Generation for Remote FTP Listimport ftplibimport osimport socketimport re# import sys# import csv# Self Repo# from src.func_demo.func_f import date_ffrom src.conf.ftp_conf_bak import *from src.func_demo.Oracle2File import *# from src.ftp_wc import *from src.func_demo.os_f import file_createdef ftp_connect(host, usr, passwd, port=21, timeout=5):    &quot;&quot;&quot;    :param host: remote FTP address    :param usr: username for FTP    :param passwd: password    :param port: port &lt;int&gt;    :param timeout: the timeout to set against the ftp socket(s)    :return: &lt;class &#39;ftplib.FTP&#39;&gt; or 1&lt;num&gt;    Ex: ftp = ftp_connect(host=rs[&#39;host&#39;], port=rs[&#39;port&#39;], usr=rs[&#39;usr&#39;], passwd=rs[&#39;passwd&#39;])    &quot;&quot;&quot;    try:        print(f&#39;Current Connection Info: {host}:{port}/{usr}&#39;)        ftp = ftplib.FTP()        # ftp.encoding = &#39;utf-8&#39;        # print(type(ftp))        ftp.connect(host, port, timeout)    except socket.timeout as e:        print(f&#39;Status: Host&lt;{host}&gt; timed out during connection.&#39;)        print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)        print(&#39;------------------&#39; * 2)        return 1        # raise OSError(&#39;FTP connect timed out!&#39;)    except ConnectionRefusedError as e:        print(&#39;Status: Login failed. Please check whether the remote address is normal.&#39;)        print(&#39;------------------&#39;*2, f&#39;\nError Details:\n{e}&#39;)        print(&#39;------------------&#39;*2)        return 1    else:        ftp.set_debuglevel(0)  # 打开调试级别2，显示详细信息        try:            ftp.login(usr, passwd)            # print(f&#39;***Welcome Infomation: {ftp.welcome}***&#39;)  # 打印出欢迎信息            print(f&#39;Status: FTP User &lt;{usr}&gt; has connected to &lt;{host} {port}&gt;.&#39;)            return ftp        except ftplib.error_perm as e:            print(&#39;Status: Login failed. Please check whether the login information is correct.&#39;)            print(&#39;------------------&#39;*2, f&#39;\nError Details:\n{e}&#39;)            print(&#39;------------------&#39;*2)            return 1        except socket.timeout as e:            print(&#39;Status: Time out during login.&#39;)            print(&#39;------------------&#39;*2, f&#39;\nError Details:\n{str(e).title()}&#39;)            print(&#39;------------------&#39;*2)            return 1def ftp_nlst(ftp, remote_path, re_pattern):    &quot;&quot;&quot;    :param ftp:         &lt;class &#39;ftplib.FTP&#39;&gt;    :param remote_path: FTP file path    :param re_pattern:     RE: Regular expression    Ex: ftp_reply = ftp_nlst(ftp, remote_path=remote_path, re_pattern=f&#39;.*.{y}{m}{d}.*.csv$&#39;)    :return: &lt;class &#39;list&#39;&gt; or Error 1    &quot;&quot;&quot;    # ftp = ftplib.FTP()    print(f&#39;***NLST***&#39;)    # Local Path    print(f&#39;--Local script path: {os.getcwd()}&#39;)    # Remote Path    print(f&#39;--Remote path: {remote_path}&#39;)    try:        n_lst_decode = []        ftp.cwd(remote_path)        # ftp.dir() # &lt;class &#39;NoneType&#39;&gt;        # print(f&#39;ftp_dir: {type(ftp_dir)}&#39;)        cur = ftp.pwd()        print(f&#39;Status: Successfully change dirName into {cur}.&#39;)        try:            n_lst = ftp.nlst()  # &lt;class &#39;list&#39;&gt;            for rs in n_lst:                n_lst_decode.append(rs.encode(&#39;iso-8859-1&#39;).decode(&#39;gbk&#39;))  # 解决Python3中文乱码  latin-1 ---&gt; gbk/gb2312            # print(f&#39;n_lst_decode: {n_lst_decode}&#39;)            # ftp.retrlines(cmd=&#39;LIST&#39;, &lt;function&gt;)            match_result = re_match(list_input=n_lst_decode, re_pattern=re_pattern)            return match_result        except Exception as e:            print(&#39;Status: Failed to obtain the file lists!&#39;)            print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)            print(&#39;------------------&#39; * 2)            return 1    except ftplib.all_errors as e:        print(&#39;Status: Path switching failed!&#39;)        print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)        print(&#39;------------------&#39; * 2)        return 1    finally:        ftp.close()def re_match(list_input, re_pattern):    &quot;&quot;&quot;    :param list_input:  &lt;class: list&gt;    :param re_pattern:  RE    :return:    返回正则匹配结果集 re.compile(re_pattern).findall() &lt;class: list&gt;    &quot;&quot;&quot;    print(f&#39;***RE***&#39;)    try:        # # 1        #     match_result = [rs for rs in list_input if re.match(re_pattern, rs)]        # # 2        #     match_result = []        #     for rs in list_input:        #         if re.match(re_pattern, rs):        #             match_result.append(rs)        #             print(f&#39;Status: File match successfully. Filename: {rs}&#39;)        #         else:        #             # print(f&#39;Match failed. Filename: {rs}&#39;)        #             print(f&#39;Match failed.&#39;)        #     print(match_result)        #     return match_result        # 3        #     match_result = []        #     p = re.compile(re_pattern)        #     for rs in list_input:        #         if p.findall(rs):        #             match_result.append(rs)        #             print(rs)        p = re.compile(re_pattern)        match_result = [rs for rs in list_input if p.findall(rs)]  # 别忘了，findall返回的是 &lt;class &#39;list&#39;&gt;        # print(match_result)        return match_result    except Exception as e:        print(&#39;Status: RE failed!&#39;)        print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)        print(&#39;------------------&#39; * 2)        return 1def main_job(key):    &quot;&quot;&quot;    :param key: 采集标签，标签用来区分清单采集的厂家和内容    :return: &lt;class: dict&gt;  [ftp_findall, error_counter, error_list]    Ex:    &quot;&quot;&quot;    ftp_result = []    ftp_findall = []    error_list = []    # error_counter = 0    # try:    if key in ftp_ip_dict.keys():        print(f&#39;***MAIN***&#39;)        for rn_dict in ftp_ip_dict.keys():            # 测试Limit            if rn_dict == key:  # &#39;NSN_CM&#39;:                # pprint.pprint(ftp_ip_dict[rs])                # print(f&#39;{rn_dict}: {ftp_ip_dict[rn_dict]}&#39;)                for rs in ftp_ip_dict[rn_dict]:                    # print(rs)  # &lt;class &#39;dict&#39;&gt;                    # 这里后续可以引入多线程，同时统计当前采集服务器上的对应的所有IP                    ftp = ftp_connect(host=rs[&#39;host&#39;], port=rs[&#39;port&#39;], usr=rs[&#39;usr&#39;], passwd=rs[&#39;passwd&#39;])                    if ftp == 1:                        # print(f&#39;FinishStatus: User {rs[&quot;usr&quot;]} Exception!&#39;, &#39;\n&#39;*2)                        print(f&#39;FinishStatus: Connect Exception!&#39;, &#39;\n&#39; * 2)                        # error_counter += 1                        error_list.append(rs[&#39;host&#39;])                        continue                    else:                        remote_path = rs[&#39;remotePath&#39;]                        # print(f&#39;已配置的远程路径: {remote_path}&#39;)                        ftp_reply = ftp_nlst(ftp, remote_path=remote_path, re_pattern=rs[&#39;re_pattern&#39;]) # &lt;class &#39;list&#39;&gt;                        if ftp_reply == 1:                            print(f&#39;FinishStatus: NLST Exception!&#39;, &#39;\n&#39; * 2)                            # error_counter += 1                            error_list.append(rs[&#39;host&#39;])                            continue                        else:                            print(f&#39;ftp_current: {ftp_reply}&#39;)                            ftp_result.extend(ftp_reply)                            del ftp_reply                        print(f&#39;ftp_result: {ftp_result}&#39;)                        print(f&#39;FinishStatus: Succeed!\n&#39;)                # Final Result Gather                print(&#39;***FINAL***&#39;)                for rf in ftp_result:                    ftp_findall.append([key, rf])                print(f&#39;ftp_findall: {ftp_findall}&#39;)        # Results filled in dict        data_dict = dict(ftp_findall=ftp_findall, error_list=error_list)        return data_dict    # elif not ftp_findall:    #     return error_counter    else:        print(f&#39;Error FTP Key in ftp_conf...&#39;)def ftp_nlst_write(message_date, local_path, file_flag, file_title):    &quot;&quot;&quot;    :param message_date: list input    :param local_path:  local file path for out-put    :param file_flag:   hint to distinguish between different flag_lists    :param file_title:  file title    :return 0(file output) or 1    &quot;&quot;&quot;    print(f&#39;***NLST_WRITE***&#39;)    # 清单    # local_path = file_nlst_path    # file_title = &#39;LOCAL&#39;    # fileDict[&#39;LOCAL&#39;][&#39;title&#39;]    # file_flag = fileDict[&#39;HW_CM&#39;][&#39;flag&#39;]    try:        file = FileWR(local_file_path=local_path, title=file_title)        flag = file.file_write_f(message_date=message_date, job_flag=file_flag)        return flag    # 方法2:    # try:    #     with open(output_file, &#39;w&#39;, newline=&#39;&#39;, encoding=&#39;UTF-8&#39;) as file_1:    #         writer_csv = csv.writer(file_1)    #         writer_csv.writerow([file_title])    #         for row in n_lst:    #             writer_csv.writerow([row])  # csv提供的写入方法可以按行写入list，无需按照对象一个个写入，效率更高    #         # writer_csv.writerows([n_lst])    #     return 0    # except Exception as e:    #     print(&#39;Status: 文件写入失败!&#39;)    #     print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)    #     print(&#39;------------------&#39; * 2)    #     return 1    except (IOError, OSError, Exception) as e:        print(&#39;Status: File write error!&#39;)        print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)        print(&#39;------------------&#39; * 2)        return 1# if __name__ == &#39;__main__&#39;:#     pass</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>Python3</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20191224-1414_Hive-1.x</title>
    <link href="/2019/12/24/20191219-2254-Hive-1-x/"/>
    <url>/2019/12/24/20191219-2254-Hive-1-x/</url>
    
    <content type="html"><![CDATA[<h2 id="Hive-配置"><a href="#Hive-配置" class="headerlink" title="Hive 配置"></a>Hive 配置</h2><h3 id="01-Hive-1-x"><a href="#01-Hive-1-x" class="headerlink" title="01. Hive 1.x"></a>01. Hive 1.x</h3><ul><li>Repo Point<br><a href="http://mirror.bit.edu.cn/apache/hive/hive-1.2.2/apache-hive-1.2.2-bin.tar.gz" target="_blank" rel="noopener">apache-hive-1.2.2-bin.tar.gz</a><br><a href="https://downloads.mysql.com/archives/get/file/mysql-connector-java-5.1.44.tar.gz" target="_blank" rel="noopener">mysql-connector-java-5.1.44.tar</a></li></ul><h3 id="02-环境配置"><a href="#02-环境配置" class="headerlink" title="02. 环境配置"></a>02. 环境配置</h3><h4 id="1-环境包"><a href="#1-环境包" class="headerlink" title="1. 环境包"></a>1. 环境包</h4><h5 id="组件"><a href="#组件" class="headerlink" title="组件"></a>组件</h5><p><code>Tips</code> 记得安装JDBC连接器mysql-connector-java-5.1.44-bin.jar，并将Hive和Hadoop侧的jline版本同步！！！</p><pre><code class="bash"># 解压并拷贝MySQL-jdbc的jar包至 $Hive/libs下：cd /usr/local/srctar -zxvf mysql-connector-java-5.1.44.tarcp /usr/local/src/mysql-connector-java-5.1.44/mysql-connector-java-5.1.44-bin.jar $HIVE_HOME/lib/# 进入yarn下的lib查看Hadoop-2.6.1自带的jline版本：ll $HADOOP_HOME/share/hadoop/yarn/lib/*jline*#发现Jjline版本过低：jline-0.9.94.jar# 进入Hive下的lib查看Hive-1.2.2自带的jline版本：ll $HIVE_HOME/lib/*jline*# jline-2.12.jar# 备份Hadoop侧的jline，用Hive侧的jline替代：cd $HADOOP_HOME/share/hadoop/yarn/lib/mv jline-0.9.94.jar jline-0.9.94.jar.oldcp $HIVE_HOME/lib/jline-2.12.jar $HADOOP_HOME/share/hadoop/yarn/lib/# 替换后检查ll $HADOOP_HOME/share/hadoop/yarn/lib/jline*</code></pre><h5 id="配置文件修改-amp-相关文件夹创建"><a href="#配置文件修改-amp-相关文件夹创建" class="headerlink" title="配置文件修改&amp;相关文件夹创建"></a>配置文件修改&amp;相关文件夹创建</h5><pre><code class="bash"># 对应创建Hive配置文件中配置的文件夹：mkdir /usr/local/src/apache-hive-1.2.2-bin/warehousemkdir /usr/local/src/apache-hive-1.2.2-bin/tmpmkdir /usr/local/src/apache-hive-1.2.2-bin/log# 用户环境变量，添加Hive路径：vim ~/.bashrc# ~/.bashrc# SET JAVA PATHexport JAVA_HOME=/usr/local/src/jdk1.8.0_201export JRE_HOME=${JAVA_HOME}/jreexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/libexport PATH=${JAVA_HOME}/bin:$PATH# SET HADOOP PATHexport HADOOP_HOME=/usr/local/src/hadoop-2.6.1export PATH=$PATH:$HADOOP_HOME/bin# SET HIVE PATHexport HIVE_HOME=/usr/local/src/apache-hive-1.2.2-binexport PATH=$PATH:$HIVE_HOME/bin# SET SCALA PATHexport SCALA_HOME=/usr/local/src/scala-2.11.8export PATH=$PATH:$SCALA_HOME/bin# SET INI PATHexport INI_PATH=/usr/local/src# SET SPARK PATHexport SPARK_HOME=/usr/local/src/spark-2.0.2-bin-hadoop2.6export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin####### 生效source ~/.bashrc# Hive log位置配置mv hive-log4j.properties.template hive-log4j.propertie# Hive exec-log位置配置(貌似没有任务执行日志生成？？？)mv hive-exec-log4j.properties.template hive-exec-log4j.properties# 添加JDK/Hadoop/Hive家目录/Hive配置文件路径 cp hive-env.sh.template hive-env.sh# hive-env.sh# Folder containing extra ibraries required for hive compilation/execution can be controlled by:export JAVA_HOME=/usr/local/src/jdk1.8.0_201export HADOOP_HOME=/usr/local/src/hadoop-2.6.1export HIVE_HOME=/usr/local/src/apache-hive-1.2.2-binexport HIVE_CONF_DIR=/usr/local/src/apache-hive-1.2.2-bin/conf######</code></pre><h5 id="Hive配置的文件需要手动新建，修改MySQL连接信息-amp-更新多组文件夹路径"><a href="#Hive配置的文件需要手动新建，修改MySQL连接信息-amp-更新多组文件夹路径" class="headerlink" title="Hive配置的文件需要手动新建，修改MySQL连接信息&amp;更新多组文件夹路径"></a>Hive配置的文件需要手动新建，修改MySQL连接信息&amp;更新多组文件夹路径</h5><pre><code class="bash">touch hive-site.xml # hive-site.xml# 目前仅配置这么多，还有其他配置尚未添加# javax.jdo.option.ConnectionURL: Hive使用MySQL作为 Metadata 存储时(MySql为5.7.12版本)，需要在连接串中指定是否采用SSL连接Tips：mysql -V    java -version&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;                &lt;value&gt;jdbc:mysql://master:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt;                &lt;description&gt;Warning disabled.&lt;/description&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;                &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;                &lt;value&gt;root&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;                &lt;value&gt;123456&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;                &lt;value&gt;/usr/local/src/apache-hive-1.2.2-bin/warehouse&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;hive.exec.scratchdir&lt;/name&gt;                &lt;value&gt;/usr/local/src/apache-hive-1.2.2-bin/tmp&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;hive.querylog.location&lt;/name&gt;                &lt;value&gt;/usr/local/src/apache-hive-1.2.2-bin/log&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;hive.cli.print.header&lt;/name&gt;                &lt;value&gt;true&lt;/value&gt;                &lt;description&gt;Whether to print the names of the columns in query output.&lt;/description&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;hive.cli.print.current.db&lt;/name&gt;                &lt;value&gt;true&lt;/value&gt;                &lt;description&gt;Whether to include the current database in the Hive prompt.&lt;/description&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;hive.metastore.uris&lt;/name&gt;                &lt;value&gt;thrift://master:9083&lt;/value&gt;                &lt;description&gt;Path for Hive metadata.&lt;/description&gt;        &lt;/property&gt;&lt;/configuration&gt;</code></pre><h5 id="Beeline连接Hive"><a href="#Beeline连接Hive" class="headerlink" title="Beeline连接Hive"></a>Beeline连接Hive</h5><pre><code class="bash"># 配置了hive.metastore.uris后需开启 Metastorecd $HIVE_HOME/bin/# 后缀为重定向，需百度解决hive --service metastore  1&gt;/dev/null  2&gt;&amp;1  &amp;# beeline方式连接Hive，默认端口为10000hive --service hiveserver2 &amp;#beeline#!connect jdbc:hive2://master:10000 root 123456beeline -u jdbc:hive2://master:10000 -n root -p 123456 --color=true</code></pre><p><code>Tips</code> <strong>退出当前jdbc:</strong> !close    <strong>退出Beeline:</strong> !q  <strong>查询表:</strong>!tables</p>]]></content>
    
    
    
    <tags>
      
      <tag>Hive</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20191224-1220_Hive</title>
    <link href="/2019/12/24/20191220-1940-Hive/"/>
    <url>/2019/12/24/20191220-1940-Hive/</url>
    
    <content type="html"><![CDATA[<h2 id="移花接木-Hive"><a href="#移花接木-Hive" class="headerlink" title="移花接木-Hive"></a>移花接木-Hive</h2><h3 id="01-Hive-1-x"><a href="#01-Hive-1-x" class="headerlink" title="01. Hive 1.x"></a>01. Hive 1.x</h3><ul><li>Repo Point<br><a href="http://mirror.bit.edu.cn/apache/hive/hive-1.2.2/apache-hive-1.2.2-bin.tar.gz" target="_blank" rel="noopener">apache-hive-1.2.2-bin.tar.gz</a><br><a href="https://downloads.mysql.com/archives/get/file/mysql-connector-java-5.1.44.tar.gz" target="_blank" rel="noopener">mysql-connector-java-5.1.44.tar</a><br><a href="http://archive.apache.org/dist/hive/hive-1.2.2/apache-hive-1.2.2-src.tar.gz" target="_blank" rel="noopener">apache-hive-1.2.2-src.tar.gz</a></li></ul><h3 id="02-Hive-元素"><a href="#02-Hive-元素" class="headerlink" title="02. Hive 元素"></a>02. Hive 元素</h3><h4 id="元数据-amp-数据"><a href="#元数据-amp-数据" class="headerlink" title="元数据 &amp; 数据"></a>元数据 &amp; 数据</h4><ul><li>Hive中的表是纯逻辑表，即<code>元数据</code>；</li><li>数据存储在<code>HDFS</code>上，元数据与数据存储分离；</li><li>数据计算依赖分布式计算框架 <strong>MapReduce</strong>；</li><li>Hive<code>读多写少</code>，从HDFS中读，MR计算，并写回HDFS；<code>不支持数据改写&amp;删除</code>；</li><li>用户需指定<code>三个属性</code>用来定义数据格式：<br>列分隔符：空格 ‘,’  ‘\t’<br>行分隔符：’\n’<br>读取数据的方法</li></ul><h4 id="Hive-架构"><a href="#Hive-架构" class="headerlink" title="Hive 架构"></a>Hive 架构</h4><p><code>CLI</code> <strong><em>Hive</em></strong> / Metastore → hiveserver2(HS2) → <strong><em>Beeline</em></strong><br><code>JDBC</code> <strong><em>Metastore</em></strong> → Java/Python/C++<br><code>WUI</code> apache-hive-0.13.0-src.tar.gz → <strong><em>hive –service hwi</em></strong></p><h3 id="03-数据入库"><a href="#03-数据入库" class="headerlink" title="03. 数据入库"></a>03. 数据入库</h3><h4 id="3-1-建表"><a href="#3-1-建表" class="headerlink" title="3-1 建表"></a>3-1 建表</h4><pre><code class="SQL">--HQLcreate table article(sentence string)   --一句话就是一行row format delimited fields terminated by &#39;\n&#39;;--每一行的formatdesc article; --查询表结构--数据导入前，表为空select article.sentence from article limit 2; --本地数据load（Local Load）load data local inpath &#39;/data/mr_wc/The_man_of_property.txt&#39;into table article;--再次查询0: jdbc:hive2://master:10000&gt; select article.sentence from article limit 1;OK+-------------------+--+| article.sentence  |+-------------------+--+| Preface           |+-------------------+--+1 row selected (0.185 seconds)</code></pre><h4 id="3-2-数据清洗"><a href="#3-2-数据清洗" class="headerlink" title="3-2 数据清洗"></a>3-2 数据清洗</h4><pre><code class="SQL">--HQL</code></pre><h4 id="3-3-分区-分桶（详见04）"><a href="#3-3-分区-分桶（详见04）" class="headerlink" title="3-3 分区/分桶（详见04）"></a>3-3 分区/分桶（详见04）</h4><hr><h3 id="04-分桶"><a href="#04-分桶" class="headerlink" title="04. 分桶"></a>04. 分桶</h3><h4 id="有点绕的概念"><a href="#有点绕的概念" class="headerlink" title="有点绕的概念"></a>有点绕的概念</h4><p><code>语法</code> tablesample(bucket x out of y on id)</p><pre><code class="SQL">TABLESAMPLE (BUCKET x OUT OF y [ON colname])</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>Hive</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20191220_1347_ubuntu16.04.2-server-amd64</title>
    <link href="/2019/12/20/20191220-1347-ubuntu16-04-2-server-amd64/"/>
    <url>/2019/12/20/20191220-1347-ubuntu16-04-2-server-amd64/</url>
    
    <content type="html"><![CDATA[<h2 id="Ubuntu-Server-配置"><a href="#Ubuntu-Server-配置" class="headerlink" title="Ubuntu Server 配置"></a>Ubuntu Server 配置</h2><h3 id="01-Ubuntu16-04-2-Server-amd64"><a href="#01-Ubuntu16-04-2-Server-amd64" class="headerlink" title="01. Ubuntu16.04.2-Server-amd64"></a>01. Ubuntu16.04.2-Server-amd64</h3><ul><li>Repo Point<br><a href="http://old-releases.ubuntu.com/releases/16.04.2/ubuntu-16.04.2-server-amd64.iso" target="_blank" rel="noopener">ubuntu-16.04.2-server-amd64.iso</a><br><code>Tips</code> <strong>最新的版本已到19.01</strong></li></ul><h3 id="02-环境配置"><a href="#02-环境配置" class="headerlink" title="02. 环境配置"></a>02. 环境配置</h3><h4 id="1-Ubuntu16-04-2-Server-amd64"><a href="#1-Ubuntu16-04-2-Server-amd64" class="headerlink" title="1. Ubuntu16.04.2-Server-amd64"></a>1. Ubuntu16.04.2-Server-amd64</h4><h5 id="服务器BOIS启动"><a href="#服务器BOIS启动" class="headerlink" title="服务器BOIS启动"></a>服务器BOIS启动</h5><pre><code class="bash"># F2、F12或 Del 进BOIS# 部分前置的选项包括语言、键盘语言&amp;布局、用户描述、非root用户账密、时区等</code></pre><h4 id="2-LVM"><a href="#2-LVM" class="headerlink" title="2. LVM"></a>2. LVM</h4><p><code>Tips</code> <a href="https://help.ubuntu.com/lts/serverguide/advanced-installation.html" target="_blank" rel="noopener">https://help.ubuntu.com/lts/serverguide/advanced-installation.html</a></p><h5 id="下面是官方文档步骤，建议使用LVM配置除了-boot外的所有挂载，便于后续LVM扩容"><a href="#下面是官方文档步骤，建议使用LVM配置除了-boot外的所有挂载，便于后续LVM扩容" class="headerlink" title="下面是官方文档步骤，建议使用LVM配置除了/boot外的所有挂载，便于后续LVM扩容."></a>下面是官方文档步骤，建议使用LVM配置除了/boot外的所有挂载，便于后续LVM扩容.</h5><ol><li>Follow the installation steps until you get to the Partition disks step, then: At the “Partition Disks screen choose “Manual”.</li></ol><ul><li>选择分区手动配置.</li></ul><hr><ol start="2"><li>Select the hard disk and on the next screen choose “yes” to “Create a new empty partition table on this device”.</li></ol><ul><li>选择在磁盘上创建新的分区.</li></ul><hr><ol start="3"><li>Next, create standard /boot, swap, and / partitions with whichever filesystem you prefer.</li></ol><ul><li>先手动建立 <code>/boot</code> 分区，剩余所有大小全部配置为 <code>LVM</code> 逻辑卷.</li></ul><hr><ol start="4"><li>For the LVM /srv, create a new Logical partition. Then change “Use as” to “physical volume for LVM” then “Done setting up the partition”.</li></ol><ul><li>LVM内按照自己的需求进行生成和挂载，选择 “physical volume for LVM”.</li></ul><hr><ol start="5"><li>Now select “Configure the Logical Volume Manager” at the top, and choose “Yes” to write the changes to disk.</li></ol><ul><li>磁盘分切完后(/boot &amp; LVM)，进入LVM管理配置，开始配置划分后的LVM磁盘.</li></ul><hr><ol start="6"><li>For the “LVM configuration action” on the next screen, choose “Create volume group”. Enter a name for the VG such as vg01, or something more descriptive. After entering a name, select the partition configured for LVM, and choose “Continue”.</li></ol><ul><li>顺序：<code>PV(Free physical volumes) → VG → LV</code> 此处应该可以从配置项里看到可用的PV，选择创建命名VG(例如：vg01).</li></ul><hr><ol start="7"><li>Back at the “LVM configuration action” screen, select “Create logical volume”. Select the newly created volume group, and enter a name for the new LV, for example srv since that is the intended mount point. Then choose a size, which may be the full partition because it can always be extended later. Choose “Finish” and you should be back at the main “Partition Disks” screen.</li></ol><ul><li>根据自己的需求，将创建好的VG分切至各个LV，并取名(例如：swap、root).</li></ul><hr><ol start="8"><li>Now add a filesystem to the new LVM. Select the partition under “LVM VG vg01, LV srv”, or whatever name you have chosen, the choose Use as. Setup a file system as normal selecting /srv as the mount point. Once done, select “Done setting up the partition”.</li></ol><ul><li>到这里，LVM已按照需求划分为不同分区，现在要做的就是挂载到对应的目录下，按照先前的命名，一一对应挂载.</li></ul><hr><ol start="9"><li>Finally, select “Finish partitioning and write changes to disk”. Then confirm the changes and continue with the rest of the installation.</li></ol><ul><li>Finish.</li></ul><hr><h5 id="具体磁盘配置步骤"><a href="#具体磁盘配置步骤" class="headerlink" title="具体磁盘配置步骤"></a><strong>具体磁盘配置步骤</strong></h5><img src="1.png" srcset="/img/loading.gif" title="01" alt=""><hr><img src="2.png" srcset="/img/loading.gif" title="02" alt=""><hr><img src="3.png" srcset="/img/loading.gif" title="03" alt=""><hr><img src="4.png" srcset="/img/loading.gif" title="单独建立/boot" alt=""><hr><img src="5.png" srcset="/img/loading.gif" title="单独建立/boot" alt=""><hr><img src="6.png" srcset="/img/loading.gif" title="单独建立/boot" alt=""><hr><img src="7.png" srcset="/img/loading.gif" title="单独建立/boot" alt=""><hr><img src="8.png" srcset="/img/loading.gif" title="08" alt=""><hr><img src="9.png" srcset="/img/loading.gif" title="09" alt=""><hr><img src="10.png" srcset="/img/loading.gif" title="10" alt=""><hr><img src="11.png" srcset="/img/loading.gif" title="11" alt=""><hr><img src="12.png" srcset="/img/loading.gif" title="12" alt=""><hr><img src="13.png" srcset="/img/loading.gif" title="13" alt=""><hr><img src="14.png" srcset="/img/loading.gif" title="14" alt=""><hr><img src="15.png" srcset="/img/loading.gif" title="15" alt=""><hr><img src="16.png" srcset="/img/loading.gif" title="16" alt=""><hr><img src="17.png" srcset="/img/loading.gif" title="17" alt=""><hr><img src="18.png" srcset="/img/loading.gif" title="18" alt=""><hr><img src="19.png" srcset="/img/loading.gif" title="19" alt=""><hr><img src="20.png" srcset="/img/loading.gif" title="20" alt=""><hr><img src="21.png" srcset="/img/loading.gif" title="21" alt=""><hr><img src="22.png" srcset="/img/loading.gif" title="配置完成后的挂载" alt=""><hr><img src="23.png" srcset="/img/loading.gif" title="23" alt=""><hr><img src="24.png" srcset="/img/loading.gif" title="24" alt=""><hr><img src="25.png" srcset="/img/loading.gif" title="25" alt=""><hr><img src="26.png" srcset="/img/loading.gif" title="26" alt=""><hr><img src="27.png" srcset="/img/loading.gif" title="27" alt=""><hr><img src="28.png" srcset="/img/loading.gif" title="28" alt=""><hr><img src="29.png" srcset="/img/loading.gif" title="29" alt=""><hr><img src="30.png" srcset="/img/loading.gif" title="不自动更新" alt=""><hr><img src="31.png" srcset="/img/loading.gif" title="31" alt=""><hr><img src="32.png" srcset="/img/loading.gif" title="Install GRUB boot loader" alt=""><hr><img src="33.png" srcset="/img/loading.gif" title="Install GRUB boot loader" alt=""><hr><img src="34.png" srcset="/img/loading.gif" title="Finish!" alt=""><hr><h5 id="安装过程完成后会自动reboot"><a href="#安装过程完成后会自动reboot" class="headerlink" title="安装过程完成后会自动reboot."></a>安装过程完成后会自动reboot.</h5><img src="35.png" srcset="/img/loading.gif" title="Finish!" alt=""><h4 id="3-静态IP"><a href="#3-静态IP" class="headerlink" title="3. 静态IP"></a>3. 静态IP</h4><h5 id="配置root用户"><a href="#配置root用户" class="headerlink" title="配置root用户"></a>配置root用户</h5><pre><code class="bash"># 配置静态IPubuntu@ubuntu:~$ sudo vim /etc/network/interfaces[sudo] password for ubuntu: # interfaces# This file describes the network interfaces available on your system# and how to activate them. For more information, see interfaces(5).source /etc/network/interfaces.d/*# The loopback network interfaceauto loiface lo inet loopback# The primary network interfaceauto ens33                 ⬅ 网关自启# iface ens33 inet dhcpiface ens33 inet staticaddress 192.168.73.21      ⬅ 内网静态IPgateway 192.168.73.2       ⬅ 内网网关netmask 255.255.255.0network 192.168.73.0       ⬅ 子网IP####### 配置DNSubuntu@ubuntu:~$ sudo vim /etc/resolv.conf # resolv.conf# Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8)#     DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN# nameserver 192.168.73.2# search localdomainnameserver 202.96.128.86nameserver 114.114.114.114####### 重启网络ubuntu@ubuntu:~$ sudo service networking restart</code></pre><h5 id="4-上述配置正确，此时就可以用客户端进行连接"><a href="#4-上述配置正确，此时就可以用客户端进行连接" class="headerlink" title="4.上述配置正确，此时就可以用客户端进行连接"></a>4.上述配置正确，此时就可以用客户端进行连接</h5><h5 id="5-root用户激活"><a href="#5-root用户激活" class="headerlink" title="5. root用户激活"></a>5. root用户激活</h5><p><code>Tips</code> 这一步可以放在第3步执行</p><pre><code class="bash">ubuntu@ubuntu:~$ sudo passwd rootEnter new UNIX password:        ⬅ root Retype new UNIX password:       ⬅ rootpasswd: password updated successfullyubuntu@ubuntu:~$ su - rootPassword:                       ⬅ rootroot@ubuntu:~# root@ubuntu:~# root@ubuntu:~# </code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>OS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20191219-2347_Hadoop-2.6.1</title>
    <link href="/2019/12/19/20191219-2347-Hadoop-2-6-1/"/>
    <url>/2019/12/19/20191219-2347-Hadoop-2-6-1/</url>
    
    <content type="html"><![CDATA[<h2 id="Hadoop-2-X-配置"><a href="#Hadoop-2-X-配置" class="headerlink" title="Hadoop 2.X 配置"></a>Hadoop 2.X 配置</h2><h3 id="01-Hadoop-2-6-1"><a href="#01-Hadoop-2-6-1" class="headerlink" title="01. Hadoop 2.6.1"></a>01. Hadoop 2.6.1</h3><ul><li>Repo Point<br><a href="http://archive.apache.org/dist/hadoop/common/hadoop-2.6.1/hadoop-2.6.1.tar.gz" target="_blank" rel="noopener">hadoop-2.6.1.tar.gz</a></li></ul><h4 id="01-SSH互信（通信基本）"><a href="#01-SSH互信（通信基本）" class="headerlink" title="01. SSH互信（通信基本）"></a>01. SSH互信（通信基本）</h4><pre><code class="bash"># 生成密钥对# 当前node为pp-web01！！！[root@sh02-oscar-hapomc-pp-web01 ~] ssh-keygen -t rsa# 或# [root@sh02-oscar-hapomc-pp-web01 ~] ssh-keygen -t rsa -P &quot;&quot; -f ~/.ssh/id_rsaGenerating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa):    ⬅ 回车Enter passphrase (empty for no passphrase):                 ⬅ 回车Enter same passphrase again:                                ⬅ 回车Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:8stOB0k90dcNBcJomZAgFtDCdWTJ3JTGLZK3METx45I root@sh02-oscar-hapomc-pp-web01.novalocal# The key&#39;s randomart image is:+---[RSA 2048]----+| ..+=BOOo=.*. o=o||  o..+O.O.*.o.. o||   .   *++o  .   ||       +.o .     ||      E S   ..   ||       + .       ||        o .      ||       o o       ||       .+        |+----[SHA256]-----+# 公钥加入授权码内，用于后续分发[root@sh02-oscar-hapomc-pp-web01 ~] cat /root/.ssh/id_rsa.pub &gt; /root/.ssh/authorized_keys# 编辑IP映射（无vim，vi替代）[root@sh02-oscar-hapomc-pp-web01 ~] vi /etc/hosts# hostshosts        hosts.allow  hosts.deny   127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain610.73.8.34    webpp3410.72.8.25    web2510.72.8.56    web56####### 分开公钥至子节点[root@sh02-oscar-hapomc-pp-web01 ~] scp ~/.ssh/authorized_keys root@web25:~/.ssh/The authenticity of host &#39;web25 (10.72.8.25)&#39; can&#39;t be established.ECDSA key fingerprint is SHA256:Rz6Sn/xaHpGzQpZ6nFyPc+0bmppcNDAkdAj+x3VQBPE.ECDSA key fingerprint is MD5:8a:51:3a:11:f3:2e:de:96:2d:83:ba:18:37:31:e2:a8.Are you sure you want to continue connecting (yes/no)? yes  ⬅ 首次互信连接需要确认known_host并输入对应子节点用户密码Warning: Permanently added &#39;web25,10.72.8.25&#39; (ECDSA) to the list of known hosts.root@web25&#39;s password: ⬅ 输入密码Last login: Mon Dec 30 03:05:25 2019 from 10.13.88.38authorized_keys                                                     100%  423   149.8KB/s   00:00    # 同样步骤，分发主节点的hosts文件至子节点[root@sh02-oscar-hapomc-pp-web01 ~] scp /etc/hosts root@web25:/etc/hostshosts                                                               100%  211    64.2KB/s   00:00 # 切换至子节点处看下发送结果[root@sh02-oscar-hapomc-pp-web01 ~] ssh web25Last login: Mon Dec 30 03:14:32 2019 from webpp34# 这里看主机名确认已切换至子节点，查看下分发结果[root@sh02-oscar-hapomc-prod-web01 ~] nl /etc/hosts[root@sh02-oscar-hapomc-prod-web01 ~] nl ~/.ssh/authorized_keys      1    ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC9skly4Dl68BeKuAZdevQEgxylu1HQINn8QrnEx3DJGL43Jo3nIYbC0GAflUHzRRFBb4JLrICm63FuTzwZMwiBK1fr339NZqMyBk2hUqwAY8u1eG/JagGGsXsNSpaCwVEsGBw5OnC2SGxLIwfINZD7zH8dauONvoCqIe0SH44q0eKWr5nHdybOwdq3Q6X3lTuNI91QxP4LkrtyZG07j/b4DOCHLT4KxDYur69zB64vOTboDiTBfqP/syGkRXsGnJmZt/s3Uk0DEGoU6ZReYBvbK+cr43UPnQiSKxieceKm884DyVZRPQZcmOydKPWEFcCsaDafiR+2lwfwDUxDEQu7 root@sh02-oscar-hapomc-pp-web01.novalocal# 至此，完成pp-web01至prod-web01的公钥分发（即单向互信，pp-web01可以免密连接prod-web01）# 同理，完成prod-web01至pp-web01的公钥分发（完成双向互信）[root@sh02-oscar-hapomc-prod-web01 ~] ssh-keygen -t rsaGenerating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:kKlNCwZJ24AKh9DN0tnvn7XdvawYSr9Lf10a1xIG2PA root@sh02-oscar-hapomc-prod-web01.novalocal# The key&#39;s randomart image is:+---[RSA 2048]----+|o=+= o    .+     ||+ === .o  ..o    ||o...+ =.    E.   ||.  . = o.     o  ||    . o.S    . ..||        .   . o +||         o = o *+||        . * = +.+||         . =oooo.|+----[SHA256]-----+# 这里用追加&gt;&gt;，不要用覆盖！！！[root@sh02-oscar-hapomc-prod-web01 ~] cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys # 查看赋权码文件内的结果，有两组公钥[root@sh02-oscar-hapomc-prod-web01 ~] nl ~/.ssh/authorized_keys      1    ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC9skly4Dl68BeKuAZdevQEgxylu1HQINn8QrnEx3DJGL43Jo3nIYbC0GAflUHzRRFBb4JLrICm63FuTzwZMwiBK1fr339NZqMyBk2hUqwAY8u1eG/JagGGsXsNSpaCwVEsGBw5OnC2SGxLIwfINZD7zH8dauONvoCqIe0SH44q0eKWr5nHdybOwdq3Q6X3lTuNI91QxP4LkrtyZG07j/b4DOCHLT4KxDYur69zB64vOTboDiTBfqP/syGkRXsGnJmZt/s3Uk0DEGoU6ZReYBvbK+cr43UPnQiSKxieceKm884DyVZRPQZcmOydKPWEFcCsaDafiR+2lwfwDUxDEQu7 root@sh02-oscar-hapomc-pp-web01.novalocal     2    ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDwlTic1GjMw9YiMnwsJ4f/94S30fLD+0BhX0691CwIvEBFbWTd4Hn60f7/wcuChck1Q9ICcLS6fd9jM5GoOxmyIwQ6P3+mCweexcaKyS18I6o2PjCTdV0LWooLlTIzg9N00frtw8e46dEtbY08WgS+xzASv3Dcw/WDjhqokCxaSbFPPMzWT8pd8r4aWWNsWvtINe84FW9k0YlGYUxmEw2kwuT6aJJgYSrEkwMRyw8Iw+HzoBexr6KQc85eCjEYFieiqLg3HGvenf+VP0yht20wbRxxYU1R/0NYNiClSnTazl6G9cFVEHQ39FgNP+LR+yLnCCrWATC2d6UI2ynXDqH9 root@sh02-oscar-hapomc-prod-web01.novalocal# 重复上述操作，完成子节点prod-web01至pp-web01的公钥分发[root@sh02-oscar-hapomc-prod-web01 ~] scp ~/.ssh/authorized_keys root@webpp34:~/.ssh/# Status: OK</code></pre><h4 id="02-挂载yum源"><a href="#02-挂载yum源" class="headerlink" title="02. 挂载yum源"></a>02. 挂载yum源</h4><pre><code class="bash"># 分发镜像[root@sh02-oscar-hapomc-pp-web01 mnt] scp /mnt/CentOS-7-x86_64-DVD-1908.iso root@web25:/mnt[root@sh02-oscar-hapomc-pp-web01 mnt] cp /etc/yum.repo.d/CentOS-Base.repo /etc/yum.repo.d/CentOS-Base.repo.bak[root@sh02-oscar-hapomc-pp-web01 mnt] vi /etc/yum.repo.d/CentOS-Base.repo# CentOS-Base.reposcp /etc/yum.repos.d/CentOS-Base.repo root@web25:/etc/yum.repos.d/# scp /etc/yum.repos.d/CentOS-Base.repo root@web56:/etc/yum.repos.d/# 确认分发结果nl /etc/yum.repos.d/CentOS-Base.repo</code></pre><h3 id="02-环境配置（部分Linux基础环境配置暂未罗列，如host、jdk等）"><a href="#02-环境配置（部分Linux基础环境配置暂未罗列，如host、jdk等）" class="headerlink" title="02. 环境配置（部分Linux基础环境配置暂未罗列，如host、jdk等）"></a>02. 环境配置（部分Linux基础环境配置暂未罗列，如host、jdk等）</h3><h4 id="1-环境包-rz-至相关路径，分流至其它节点："><a href="#1-环境包-rz-至相关路径，分流至其它节点：" class="headerlink" title="1. 环境包 rz 至相关路径，分流至其它节点："></a>1. 环境包 rz 至相关路径，分流至其它节点：</h4><h5 id="组件-amp-源码分发"><a href="#组件-amp-源码分发" class="headerlink" title="组件&amp;源码分发"></a>组件&amp;源码分发</h5><pre><code class="bash"># 文件解压：hadoop-2.6.1/etc/hadoopcd /usr/local/src/tar -xvf hadoop-2.6.1.tar.gz</code></pre><h5 id="当前用户环境变量"><a href="#当前用户环境变量" class="headerlink" title="当前用户环境变量"></a>当前用户环境变量</h5><pre><code class="bash">vim ~/.bashrc# ~/.bashrc# SET JAVA PATHexport JAVA_HOME=/usr/local/src/jdk1.8.0_201export JRE_HOME=${JAVA_HOME}/jreexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/libexport PATH=${JAVA_HOME}/bin:$PATH# SET HADOOP PATHexport HADOOP_HOME=/usr/local/src/hadoop-2.6.1export PATH=$PATH:$HADOOP_HOME/bin# SET HIVE PATHexport HIVE_HOME=/usr/local/src/apache-hive-1.2.2-binexport PATH=$PATH:$HIVE_HOME/bin# SET SCALA PATHexport SCALA_HOME=/usr/local/src/scala-2.11.8export PATH=$PATH:$SCALA_HOME/bin# SET INI PATHexport INI_PATH=/usr/local/src# SET SPARK PATHexport SPARK_HOME=/usr/local/src/spark-2.0.2-bin-hadoop2.6export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin####### 本地生效source ~/.bashrcscp ~/.bashrc root@slave1 ~/.bashrcscp ~/.bashrc root@slave2 ~/.bashrc# 节点分发&amp;生效ssh slave1source ~/.bashrcexitssh slave2source ~/.bashrc</code></pre><h5 id="配置文件修改-amp-相关文件夹创建"><a href="#配置文件修改-amp-相关文件夹创建" class="headerlink" title="配置文件修改&amp;相关文件夹创建"></a>配置文件修改&amp;相关文件夹创建</h5><pre><code class="bash">cd /usr/local/src/hadoop-2.6.1/etc/hadoopvim hadoop-env.sh# hadoop-env.sh######vim yarn-env.sh# yarn-env.sh######vim slaves# 配置节点# slaves######vim core-site.xml# 配置master节点IP及Hadoop临时文件根目录tmp# core-site.xml######vim hdfs-site.xml# 配置namenode/datanode/replication（备份）文件目录# hdfs-site.xml######vim mapred-site.xml# mapred-site.xml######vim yarn-site.xml# yarn-site.xml####### 创建Hadoop临时目录和文件目录mkdir /usr/local/src/hadoop-2.6.1/tmpmkdir -p /usr/local/src/hadoop-2.6.1/dfs/namemkdir -p /usr/local/src/hadoop-2.6.1/dfs/data</code></pre><h5 id="主节点配置好的-Hadoop-源码分发至集群其余节点"><a href="#主节点配置好的-Hadoop-源码分发至集群其余节点" class="headerlink" title="主节点配置好的 Hadoop 源码分发至集群其余节点"></a>主节点配置好的 Hadoop 源码分发至集群其余节点</h5><pre><code class="bash">scp /usr/local/src/hadoop-2.6.1 root@slave1:/usr/local/src/hadoop-2.6.1scp /usr/local/src/hadoop-2.6.1 root@slave2:/usr/local/src/hadoop-2.6.1</code></pre><h3 id="03-Hadoop集群初始化-启动-关闭"><a href="#03-Hadoop集群初始化-启动-关闭" class="headerlink" title="03. Hadoop集群初始化/启动/关闭"></a>03. Hadoop集群初始化/启动/关闭</h3><h4 id="集群格式化-集群启动-JAVA进程查看-集群关闭"><a href="#集群格式化-集群启动-JAVA进程查看-集群关闭" class="headerlink" title="集群格式化/集群启动/JAVA进程查看/集群关闭"></a>集群格式化/集群启动/JAVA进程查看/集群关闭</h4><p><code>Tips</code> 初始化只需要做一次！！！</p><h5 id="格式化Hadoop-NN"><a href="#格式化Hadoop-NN" class="headerlink" title="格式化Hadoop(NN)"></a>格式化Hadoop(NN)</h5><pre><code class="bash">cd $HADOOP_HOME/binhadoop namenode -formatcd $HADOOP_HOME/sbin./start-all.sh# 关闭集群cd $HADOOP_HOME/sbin./stop-all.ssh</code></pre><h5 id="jps查看集群状态"><a href="#jps查看集群状态" class="headerlink" title="jps查看集群状态"></a>jps查看集群状态</h5><pre><code class="bash">[root@master bin]# jps2784 NameNode       ⬅ NN3122 ResourceManager    ⬅ RM13827 Jps6853 Master     ⬅ (这个为Spark集群启动标识，Hadoop启动时不包括)2943 SecondaryNameNode  ⬅ Secondary Bak NN[root@master bin]# [root@master bin]# </code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>Hadoop 2.X</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20191219_1545_SPARK</title>
    <link href="/2019/12/19/20191219-1545-Spark/"/>
    <url>/2019/12/19/20191219-1545-Spark/</url>
    
    <content type="html"><![CDATA[<h2 id="Spark-配置"><a href="#Spark-配置" class="headerlink" title="Spark 配置"></a>Spark 配置</h2><h3 id="01-Spark"><a href="#01-Spark" class="headerlink" title="01. Spark"></a>01. Spark</h3><ul><li>Repo Point<br><a href="http://archive.apache.org/dist/spark/spark-2.0.2/spark-2.0.2-bin-hadoop2.6.tgz" target="_blank" rel="noopener">spark-2.0.2-bin-hadoop2.6.tgz</a></li></ul><h3 id="02-环境配置"><a href="#02-环境配置" class="headerlink" title="02. 环境配置"></a>02. 环境配置</h3><h4 id="1-环境包-rz-至相关路径，分流至其它节点"><a href="#1-环境包-rz-至相关路径，分流至其它节点" class="headerlink" title="1. 环境包 rz 至相关路径，分流至其它节点"></a>1. 环境包 rz 至相关路径，分流至其它节点</h4><h5 id="源码分发"><a href="#源码分发" class="headerlink" title="源码分发"></a>源码分发</h5><pre><code class="bash">scp -r /usr/local/src/spark-2.0.2-bin-hadoop2.6 root@slave1:/usr/local/src/scp -r /usr/local/src/spark-2.0.2-bin-hadoop2.6 root@slave2:/usr/local/src/</code></pre><h4 id="2-用户环境变量配置"><a href="#2-用户环境变量配置" class="headerlink" title="2. 用户环境变量配置"></a>2. 用户环境变量配置</h4><pre><code class="bash"># SET JAVA PATHexport JAVA_HOME=/usr/local/src/jdk1.8.0_201export JRE_HOME=${JAVA_HOME}/jreexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/libexport PATH=${JAVA_HOME}/bin:$PATH# SET HADOOP PATHexport HADOOP_HOME=/usr/local/src/hadoop-2.6.1export PATH=$PATH:$HADOOP_HOME/bin# SET HIVE PATHexport HIVE_HOME=/usr/local/src/apache-hive-1.2.2-binexport PATH=$PATH:$HIVE_HOME/bin# SET SCALA PATHexport SCALA_HOME=/usr/local/src/scala-2.11.8export PATH=$PATH:$SCALA_HOME/bin# SET SPARK PATHexport SPARK_HOME=/usr/local/src/spark-2.0.2-bin-hadoop2.6export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin# SET INI PATHexport INI_PATH=/usr/local/src</code></pre><h5 id="环境变量分发"><a href="#环境变量分发" class="headerlink" title="环境变量分发"></a>环境变量分发</h5><pre><code class="bash">scp ~/.bashrc root@slave1:~/scp ~/.bashrc root@slave2:~/</code></pre><h4 id="3-修改-SPARK-源码环境配置文件、添加集群节点："><a href="#3-修改-SPARK-源码环境配置文件、添加集群节点：" class="headerlink" title="3. 修改 SPARK 源码环境配置文件、添加集群节点："></a>3. 修改 SPARK 源码环境配置文件、添加集群节点：</h4><pre><code class="bash">cd /usr/local/src/spark-2.0.2-bin-hadoop2.6/conf/mv spark-env.sh.template spark-env.shvim spark-env.sh# spark-env.shexport SCALA_HOME=/usr/local/src/scala-2.11.8export JAVA_HOME=/usr/local/src/jdk1.8.0_201export HADOOP_HOME=/usr/local/src/hadoop-2.6.1export HADOOP_CONF_DIR=/usr/local/src/hadoop-2.6.1/etc/hadoopSPARK_MASTER_IP=masterSPARK_LOCAL_DIRS=/usr/local/src/spark-2.0.2-bin-hadoop2.6SPARK_DRIVER_MEMORY=1G######mv slaves.template slavesvim slaves# slaves 添加集群节点slave1slave2######</code></pre><h4 id="4-适配Spark-SQL"><a href="#4-适配Spark-SQL" class="headerlink" title="4. 适配Spark-SQL"></a>4. 适配Spark-SQL</h4><h5 id="MySQL相关配置和赋权见相关Blog，此处暂时不进行罗列：MySQL配置"><a href="#MySQL相关配置和赋权见相关Blog，此处暂时不进行罗列：MySQL配置" class="headerlink" title="MySQL相关配置和赋权见相关Blog，此处暂时不进行罗列：MySQL配置"></a>MySQL相关配置和赋权见相关Blog，此处暂时不进行罗列：<a href="">MySQL配置</a></h5><p>修改 $HIVE_HOME/bin下的Hive启动脚本 <strong><code>hive</code></strong>：<br><code>Tips</code> <strong>SPARK2.0</strong> 中没有 <code>${SPARK_HOME}/lib/spark-assembly-*.jar</code>，<strong>lib</strong>文件中的jar包已经被分成多个小的jar包在放置在 <code>${SPARK_HOME}/jars/</code> 下.</p><h5 id="修改前："><a href="#修改前：" class="headerlink" title="修改前："></a>修改前：</h5><pre><code class="bash"># add SPARK assembly jar to the classpathif [[ -n &quot;$SPARK_HOME&quot; ]]then  sparkAssemblyPath=`ls ${SPARK_HOME}/lib/spark-assembly-*.jar`  CLASSPATH=&quot;${CLASSPATH}:${sparkAssemblyPath}&quot;fi</code></pre><h5 id="YARN-Hadoop-2-0-内的："><a href="#YARN-Hadoop-2-0-内的：" class="headerlink" title="YARN (Hadoop 2.0) 内的："></a>YARN (Hadoop 2.0) 内的：</h5><pre><code class="bash"># add SPARK assembly jar to the classpathif [[ -n &quot;$SPARK_HOME&quot; ]]then  sparkAssemblyPath=`ls ${SPARK_HOME}/jars/*.jar`  CLASSPATH=&quot;${CLASSPATH}:${sparkAssemblyPath}&quot;fi</code></pre><h4 id="5-更新Hadoop-Yarn下的老版-jline-0-9-94-jar"><a href="#5-更新Hadoop-Yarn下的老版-jline-0-9-94-jar" class="headerlink" title="5. 更新Hadoop Yarn下的老版 jline-0.9.94.jar"></a>5. 更新Hadoop Yarn下的老版 <strong>jline-0.9.94.jar</strong></h4><pre><code class="bash"># Hive内较新版本的jline替换Hadoop内的OLD版本cd $HADOOP_HOME/share/hadoop/yarn/lib/mv jline-0.9.94.jar jline-0.9.94.jar.oldcp $HIVE_HOME/lib/jline-2.12.jar $HADOOP_HOME/share/hadoop/yarn/lib/</code></pre><h4 id="6-Hive集成Spark-SQL"><a href="#6-Hive集成Spark-SQL" class="headerlink" title="6. Hive集成Spark-SQL"></a>6. Hive集成Spark-SQL</h4><p><code>Tips</code> 这部分集成后的功能暂未进行操作</p><h5 id="hive-site-xml-内新增-Metastore-配置"><a href="#hive-site-xml-内新增-Metastore-配置" class="headerlink" title="hive-site.xml 内新增 Metastore 配置"></a>hive-site.xml 内新增 Metastore 配置</h5><pre><code class="bash">vim $HIVE_HOME/conf/hive-site.xml# hive-site.xml 新增如下内容：&lt;property&gt;    &lt;name&gt;hive.metastore.uris&lt;/name&gt;    &lt;value&gt;thrift://hostname:9083&lt;/value&gt;&lt;/property&gt;####### 将Hive的配置文件拷贝给SPARKcp $HIVE_HOME/conf/hive-site.xml $SPARK_HOME/conf/# 将MySQL下的jdbc驱动包拷贝给SPARKcp $HIVE_HOME/lib/mysql-connector-java-5.1.44.jar  $SPARK_HOME/jars/</code></pre><h4 id="7-SPARK集群启动-关闭"><a href="#7-SPARK集群启动-关闭" class="headerlink" title="7. SPARK集群启动/关闭"></a>7. SPARK集群启动/关闭</h4><pre><code class="bash">cd /usr/local/src/spark-2.0.2-bin-hadoop2.6/sbin/./start-all.sh# 关闭集群：cd /usr/local/src/spark-2.0.2-bin-hadoop2.6/sbin/./stop-all.sh</code></pre><h5 id="启动-Hive-metastore"><a href="#启动-Hive-metastore" class="headerlink" title="启动 Hive metastore"></a>启动 Hive metastore</h5><p><code>Metedata</code>元数据.<br><code>Metastore</code>Hive客户端连接Metastore元存储服务，Metastore再去连接MySQL来存取 <strong>元数据</strong>。多个客户端连接Hive时，只需要连接Metastore服务即可；即启动后可以用Java、Python等调用该jdbc</p><pre><code class="bash"># hive --service metastorecd $HIVE_HOME/bin/hive --service metastore  1&gt;/dev/null  2&gt;&amp;1  &amp;# 启动Spark-SQL的shell交互界面cd $SPARK_HOME/bin/spark-shell --master spark://master:7077</code></pre><h5 id="疑问"><a href="#疑问" class="headerlink" title="疑问"></a>疑问</h5><p><code>hiveserver2</code>HS2是一个服务端接口，使远程客户端可以执行对Hive的查询并返回结果，HS2提供了新的CLI：Beeline(基于SQLLine)，可以作为Hive jdbc Client端访问HS2.<br><code>hiveserver2</code> 和 <code>Metastore</code>一样同为Thrift Service，区别是否在于前者访问数据，后者访问元数据？？？</p><ul><li>网上有一部分解释：</li></ul><ol><li><p>若有在 <code>hive-site.xml</code> 中配置 <code>hive.metastore.uris</code>，则 <code>hiveserver2</code> 启动时会去连接配置好的 Metastore 服务(故需开启Metastore服务，其余jdbc访问也需开启该服务)，这种方式最为常用；</p></li><li><p>配置情况下未启动 <code>Metastore</code>，本地Beeline Client连接会失败；<img src="Spark-error_metastore_beeline.png" srcset="/img/loading.gif" alt="配置MS但未开启时，Beeline报错." title="配置MS但未开启时，Beeline报错."> </p></li><li><p>若未配置，则启动HS2时，会先启动一个 <code>Metastore</code> 服务，然后再启动HS2，便于后续Beeline CLI的连接.</p></li><li></li></ol><h5 id="回顾下Beeline连接Hive"><a href="#回顾下Beeline连接Hive" class="headerlink" title="回顾下Beeline连接Hive"></a>回顾下Beeline连接Hive</h5><pre><code class="bash"># 配置了hive.metastore.uris后需开启 Metastore# 后缀为重定向，需百度解决hive --service metastore  1&gt;/dev/null  2&gt;&amp;1  &amp;# beeline方式连接Hive，默认端口为10000hive --service hiveserver2#beeline#!connect jdbc:hive2://master:10000 root 123456beeline -u jdbc:hive2://master:10000 -n root -p 123456 --color=true</code></pre><p><code>Tips</code> 可以单独启动 SPARK 集群，但此时无ResourceManager/NodeManager等进程！！！</p>]]></content>
    
    
    
    <tags>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20191219_1545_ZooKeeper-Kafka</title>
    <link href="/2019/12/19/20191219-1545-ZooKeeper-Kafka/"/>
    <url>/2019/12/19/20191219-1545-ZooKeeper-Kafka/</url>
    
    <content type="html"><![CDATA[<h2 id="ZooKeeper-amp-Kafka-配置"><a href="#ZooKeeper-amp-Kafka-配置" class="headerlink" title="ZooKeeper &amp; Kafka 配置"></a>ZooKeeper &amp; Kafka 配置</h2><h3 id="01-ZooKeeper"><a href="#01-ZooKeeper" class="headerlink" title="01. ZooKeeper"></a>01. ZooKeeper</h3><ul><li>Repo Point<br><a href="http://mirrors.tuna.tsinghua.edu.cn/apache/kafka/0.10.2.1/kafka_2.11-0.10.2.1.tgz" target="_blank" rel="noopener">kafka_2.11-0.10.2.1.tgz</a><br><a href="http://archive.apache.org/dist/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz" target="_blank" rel="noopener">ZooKeeper</a></li></ul><h3 id="02-环境配置"><a href="#02-环境配置" class="headerlink" title="02. 环境配置"></a>02. 环境配置</h3><h4 id="1-环境包-rz-至相关路径，分流至其它节点："><a href="#1-环境包-rz-至相关路径，分流至其它节点：" class="headerlink" title="1. 环境包 rz 至相关路径，分流至其它节点："></a>1. 环境包 rz 至相关路径，分流至其它节点：</h4><h5 id="源码分发"><a href="#源码分发" class="headerlink" title="源码分发"></a>源码分发</h5><pre><code class="bash">scp -r /usr/local/src/spark-2.0.2-bin-hadoop2.6 root@slave1:/usr/local/src/scp -r /usr/local/src/spark-2.0.2-bin-hadoop2.6 root@slave2:/usr/local/src/</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>ZooKeeper Kafka</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20191218-2224-MURA-Excel</title>
    <link href="/2019/12/18/20191218-2224-MURA-Excel/"/>
    <url>/2019/12/18/20191218-2224-MURA-Excel/</url>
    
    <content type="html"><![CDATA[<h2 id="Execl-数据分析"><a href="#Execl-数据分析" class="headerlink" title="Execl 数据分析"></a>Execl 数据分析</h2><h3 id="01-数据验证"><a href="#01-数据验证" class="headerlink" title="01. 数据验证"></a>01. 数据验证</h3><p>通过 <strong>数据验证</strong> 填充数据</p><p><code>Excel</code> 的 <strong>数据验证</strong> 选项是在 <strong>数据</strong> 目录下：<br><img src="Excel-数据验证.png" srcset="/img/loading.gif" width="100" title="数据验证." alt="数据验证."></p><!-- ![001-数据验证-w001](/image/Excel-数据验证.png) --><h3 id="02-快速选中-amp-快速填充"><a href="#02-快速选中-amp-快速填充" class="headerlink" title="02. 快速选中&amp;快速填充"></a>02. 快速选中&amp;快速填充</h3><p>ctrl + shift + 方向键</p><h3 id="03-分类统计"><a href="#03-分类统计" class="headerlink" title="03. 分类统计"></a>03. 分类统计</h3><p>SUMIF(&lt;条件区域&gt;,&lt;条件值&gt;,&lt;待求和区域&gt;)<br>SUMIFS(&lt;待求和区域&gt;,&lt;条件区域1&gt;,&lt;条件值1&gt;,…,&lt;条件区域n&gt;,&lt;条件值n&gt;)</p><h3 id="04-函数"><a href="#04-函数" class="headerlink" title="04. 函数"></a>04. 函数</h3><pre><code class="Excel">SUMPRODUCT(A3:A3,B2:B3,C2:C3)SUMPRODUCT((A2:A3=&quot;2018&quot;)*(D2:D3=&quot;上海&quot;)*(B2:B3)*(C2:C3))</code></pre><h3 id="0x-样例分析"><a href="#0x-样例分析" class="headerlink" title="0x. 样例分析"></a>0x. 样例分析</h3><ul><li>某顶尖外卖平台数据分析师面试题。现有交易数据表user_goods_table如下：<br>user_name    用户名<br>goods_kind   用户订购的的外卖品类</li></ul><ul><li><p>现在老板想知道每个用户购买的外卖品类偏好分布，并取出每个用户购买最多的外卖品类是哪个。</p></li><li><p>输出要求如下：<br>user_name    用户名<br>goods_kind   该用户购买的最多外卖品类</p></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Excel</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2019/12/02/20191202_2329_Hello%20World/"/>
    <url>/2019/12/02/20191202_2329_Hello%20World/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Demo</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
