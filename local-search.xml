<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>20200227_1831_Hive-数据倾斜（小记）</title>
    <link href="/2020/09/10/20200227-1831-Hive-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%EF%BC%88%E5%B0%8F%E8%AE%B0%EF%BC%89/"/>
    <url>/2020/09/10/20200227-1831-Hive-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%EF%BC%88%E5%B0%8F%E8%AE%B0%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h2 id="Hive关联优化"><a href="#Hive关联优化" class="headerlink" title="Hive关联优化"></a>Hive关联优化</h2><h3 id="01-大小连接"><a href="#01-大小连接" class="headerlink" title="01. 大小连接"></a>01. 大小连接</h3><ul><li>使用HINT在关联时完成大小表的指定</li></ul><p><code>/*+ STREAMTABLE(bigtable) */</code>join顺序中最后一张表应是大表，小表在前可以读入buffer，在关联时可直接从内存中获取小表的key值与大表的key值关联；下面有两种关联写法，若a是小表，那后续join关联字段始终用a的关联key，原则上是谁小用谁.</p><pre><code class="sql">--select /*+ STREAMTABLE(a) */ a.x, b.x from a join b on (a.id = b.id)join c on (a.id = c.id);    ⬅ 这边始终用a中的key去关联其余表--这种写法，a表为大表，不入内存，且生成多个MRselect /*+ STREAMTABLE(a) */ a.x, b.x from a join b on (a.id = b.id)join c on (b.id = c.id);    ⬅ 这边依次用每轮join的左表中的key去关联其余表</code></pre><p><code>/*+ MAPJOIN(smalltable) */</code>MAPJOIN会将小表全部读入内存，并在MAP阶段将另一张表的数据和内存中的数据直接匹配，由于Map-side join过程中会在每次遍历大表数据时，查内存小表内是否有关联的key匹配，并进行join连接，且不存在shuffle洗牌和reduce部分，效率很高. 我在这里将Map这步key关联匹配，理解为Map.Spill溢出写入磁盘前，在buffer内的<strong>Map.Sort</strong>和<strong>Map.Combiner</strong>，即将相同的key进行reduce操作（Combiner其实本质上是reducer函数），从而减少Map输出量，减少后续的shuffle和reduce操作.</p><pre><code class="sql">--这种写法，b表为小表，优先读入内存，select /*+ MAPJOIN(o) */ o.order_id, p.product_id from orders o joinorder_products_prior pon o.order_id = p.order_idlimit 500;</code></pre><p><code>Tips</code>到底是key的相同与否影响MR数量，还是同key情况下表连接方式影响MR数量，这一点有待商榷.</p><h3 id="02-大大连接（Sorted-Bucket-MapSide）"><a href="#02-大大连接（Sorted-Bucket-MapSide）" class="headerlink" title="02. 大大连接（Sorted Bucket MapSide）"></a>02. 大大连接（Sorted Bucket MapSide）</h3><ul><li>举个完整的小栗子： </li></ul><pre><code class="sql">--建student &amp; student1 表： create table student(id INT, age INT, name STRING)partitioned by(stat_date STRING) clustered by(id) sorted by(age) into 2 bucketsrow format delimited fields terminated by &#39;,&#39;;create table student1(id INT, age INT, name STRING)partitioned by(stat_date STRING) clustered by(id) sorted by(age) into 2 bucketsrow format delimited fields terminated by &#39;,&#39;;--设置环境变量： set hive.enforce.bucketing = true;  --join时需要打开的参数：--set hive.optimize.bucketmapjoin = true;--set hive.optimize.bucketmapjoin.sortedmerge = true;--set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;     --这个参数不懂--插入数据： cat bucket.txt1,20,zxm2,21,ljz3,19,cds4,18,mac5,22,android6,23,symbian7,25,wpLOAD DATA local INPATH &#39;/home/lijun/bucket.txt&#39; OVERWRITE INTO TABLE student partition(stat_date=&quot;20120802&quot;);from student insert overwrite table student1 partition(stat_date=&quot;20120802&quot;) select id,age,name where stat_date=&quot;20120802&quot; sort by age;--查看文件目录： hadoop fs -ls /hive/warehouse/test.db/student1/stat_date=20120802 Found 2 items -rw-r--r--   2 lijun supergroup         31 2013-11-24 19:16 /hive/warehouse/test.db/student1/stat_date=20120802/000000_0 -rw-r--r--   2 lijun supergroup         39 2013-11-24 19:16 /hive/warehouse/test.db/student1/stat_date=20120802/000001_0 --查看sampling数据： hive&gt; select * from student1 tablesample(bucket 1 out of 2 on id); Total MapReduce jobs = 1Launching Job 1 out of 1.......OK4       18      mac     201208022       21      ljz     201208026       23      symbian 20120802Time taken: 20.608 seconds</code></pre><p><code>Tips</code> tablesample是抽样语句，语法：TABLESAMPLE(BUCKET x OUT OF y)，y必须是table总bucket数的倍数或者因子。hive根据y的大小，决定抽样的比例。例如，table总共分了64份，当y=32时，抽取(64/32=)2个bucket的数据，当y=128时，抽取(64/128=)1/2个bucket的数据。x表示从哪个bucket开始抽取。例如，table总bucket数为32，tablesample(bucket 3 out of 16)，表示总共抽取（32/16=）2个bucket的数据，分别为第3个bucket和第（3+16=）19个bucket的数据.</p><p><code>Tips</code>大表分桶贮存，记得一定是为了使用<strong>SBM Join</strong>，即大表拆分，各通排序，join过程中可以回避掉不需要的数据抽取.</p><h3 id="03-各种参数（应当按map-gt-join-gt-reduce分层理解）"><a href="#03-各种参数（应当按map-gt-join-gt-reduce分层理解）" class="headerlink" title="03. 各种参数（应当按map -&gt; join -&gt; reduce分层理解）"></a>03. 各种参数（应当按map -&gt; join -&gt; reduce分层理解）</h3><p><code>数据倾斜-万能参数</code>hive.groupby.skewindata=true;<br><code>并行执行</code>hive.exec.parallel=true;<br><code>JVM重用</code>mapred.job.reuse.jvm.num.tasks=10;</p><h3 id="04-数据倾斜"><a href="#04-数据倾斜" class="headerlink" title="04. 数据倾斜"></a>04. 数据倾斜</h3><h4 id="key值不均导致"><a href="#key值不均导致" class="headerlink" title="key值不均导致"></a>key值不均导致</h4><h4 id="万能方法"><a href="#万能方法" class="headerlink" title="万能方法"></a>万能方法</h4><h4 id="大小表关联"><a href="#大小表关联" class="headerlink" title="大小表关联"></a>大小表关联</h4><h4 id="聚合操作时存在大量特殊值（NULL）"><a href="#聚合操作时存在大量特殊值（NULL）" class="headerlink" title="聚合操作时存在大量特殊值（NULL）"></a>聚合操作时存在大量特殊值（NULL）</h4><h4 id="空间换时间"><a href="#空间换时间" class="headerlink" title="空间换时间"></a>空间换时间</h4>]]></content>
    
    
    
    <tags>
      
      <tag>Hive</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20201219_2347_Hadoop-2.6.1（持续更新）</title>
    <link href="/2020/09/08/20191219-2347-Hadoop-2-6-1/"/>
    <url>/2020/09/08/20191219-2347-Hadoop-2-6-1/</url>
    
    <content type="html"><![CDATA[<h2 id="Hadoop-2-X-配置"><a href="#Hadoop-2-X-配置" class="headerlink" title="Hadoop 2.X 配置"></a>Hadoop 2.X 配置</h2><h3 id="01-Hadoop-2-6-1"><a href="#01-Hadoop-2-6-1" class="headerlink" title="01. Hadoop 2.6.1"></a>01. Hadoop 2.6.1</h3><ul><li>Repo Point<br><a href="http://archive.apache.org/dist/hadoop/common/hadoop-2.6.1/hadoop-2.6.1.tar.gz" target="_blank" rel="noopener">hadoop-2.6.1.tar.gz</a></li></ul><h4 id="Linux环境配置"><a href="#Linux环境配置" class="headerlink" title="Linux环境配置"></a>Linux环境配置</h4><h5 id="1-1-防火墙-amp-主机名"><a href="#1-1-防火墙-amp-主机名" class="headerlink" title="1-1 防火墙 &amp; 主机名"></a>1-1 防火墙 &amp; 主机名</h5><pre><code class="bash">service  iptables statuschkconfig iptables offsystemctl status firewalldsystemctl stop firewalld# 永久关闭systemctl disable firewalldchkconfig iptables off# CentOS 6# service  iptables status# service  iptables stop# chkconfig iptables off# 临时关闭内核防火墙setenforce 0# 关闭selinux内核防火墙vim /etc/selinux/config# configSELINUX=disabled####### 修改各主机名vim /etc/sysconfig/network# networkNETWORKING=yesHOSTNAME=master######</code></pre><h5 id="1-2-SSH互信（通信基本）"><a href="#1-2-SSH互信（通信基本）" class="headerlink" title="1-2 SSH互信（通信基本）"></a>1-2 SSH互信（通信基本）</h5><pre><code class="bash"># 生成密钥对# 当前node为pp-web01！！！[root@sh02-oscar-hapomc-pp-web01 ~] ssh-keygen -t rsa# 或# [root@sh02-oscar-hapomc-pp-web01 ~] ssh-keygen -t rsa -P &quot;&quot; -f ~/.ssh/id_rsaGenerating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa):    ⬅ 回车Enter passphrase (empty for no passphrase):                 ⬅ 回车Enter same passphrase again:                                ⬅ 回车Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:8stOB0k90dcNBcJomZAgFtDCdWTJ3JTGLZK3METx45I root@sh02-oscar-hapomc-pp-web01.novalocal# The key&#39;s randomart image is:+---[RSA 2048]----+| ..+=BOOo=.*. o=o||  o..+O.O.*.o.. o||   .   *++o  .   ||       +.o .     ||      E S   ..   ||       + .       ||        o .      ||       o o       ||       .+        |+----[SHA256]-----+# 公钥加入授权码内，用于后续分发[root@sh02-oscar-hapomc-pp-web01 ~] cat /root/.ssh/id_rsa.pub &gt; /root/.ssh/authorized_keys# 编辑IP映射（无vim，vi替代）[root@sh02-oscar-hapomc-pp-web01 ~] vim /etc/hosts# hostshosts        hosts.allow  hosts.deny   127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain610.73.8.34    webpp3410.72.8.25    web2510.72.8.56    web56####### 分开公钥至子节点[root@sh02-oscar-hapomc-pp-web01 ~] scp ~/.ssh/authorized_keys root@web25:~/.ssh/The authenticity of host &#39;web25 (10.72.8.25)&#39; can&#39;t be established.ECDSA key fingerprint is SHA256:Rz6Sn/xaHpGzQpZ6nFyPc+0bmppcNDAkdAj+x3VQBPE.ECDSA key fingerprint is MD5:8a:51:3a:11:f3:2e:de:96:2d:83:ba:18:37:31:e2:a8.Are you sure you want to continue connecting (yes/no)? yes  ⬅ 首次互信连接需要确认known_host并输入对应子节点用户密码Warning: Permanently added &#39;web25,10.72.8.25&#39; (ECDSA) to the list of known hosts.root@web25&#39;s password: ⬅ 输入密码Last login: Mon Dec 30 03:05:25 2019 from 10.13.88.38authorized_keys                                                     100%  423   149.8KB/s   00:00    # 同样步骤，分发主节点的hosts文件至子节点[root@sh02-oscar-hapomc-pp-web01 ~] scp /etc/hosts root@web25:/etc/hostshosts                                                               100%  211    64.2KB/s   00:00 # 切换至子节点处看下发送结果[root@sh02-oscar-hapomc-pp-web01 ~] ssh web25Last login: Mon Dec 30 03:14:32 2019 from webpp34# 这里看主机名确认已切换至子节点，查看下分发结果[root@sh02-oscar-hapomc-prod-web01 ~] nl /etc/hosts[root@sh02-oscar-hapomc-prod-web01 ~] nl ~/.ssh/authorized_keys      1    ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC9skly4Dl68BeKuAZdevQEgxylu1HQINn8QrnEx3DJGL43Jo3nIYbC0GAflUHzRRFBb4JLrICm63FuTzwZMwiBK1fr339NZqMyBk2hUqwAY8u1eG/JagGGsXsNSpaCwVEsGBw5OnC2SGxLIwfINZD7zH8dauONvoCqIe0SH44q0eKWr5nHdybOwdq3Q6X3lTuNI91QxP4LkrtyZG07j/b4DOCHLT4KxDYur69zB64vOTboDiTBfqP/syGkRXsGnJmZt/s3Uk0DEGoU6ZReYBvbK+cr43UPnQiSKxieceKm884DyVZRPQZcmOydKPWEFcCsaDafiR+2lwfwDUxDEQu7 root@sh02-oscar-hapomc-pp-web01.novalocal# 至此，完成pp-web01至prod-web01的公钥分发（即单向互信，pp-web01可以免密连接prod-web01）# 同理，完成prod-web01至pp-web01的公钥分发（完成双向互信）[root@sh02-oscar-hapomc-prod-web01 ~] ssh-keygen -t rsaGenerating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:kKlNCwZJ24AKh9DN0tnvn7XdvawYSr9Lf10a1xIG2PA root@sh02-oscar-hapomc-prod-web01.novalocal# The key&#39;s randomart image is:+---[RSA 2048]----+|o=+= o    .+     ||+ === .o  ..o    ||o...+ =.    E.   ||.  . = o.     o  ||    . o.S    . ..||        .   . o +||         o = o *+||        . * = +.+||         . =oooo.|+----[SHA256]-----+# 这里用追加&gt;&gt;，不要用覆盖！！！[root@sh02-oscar-hapomc-prod-web01 ~] cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys # 查看赋权码文件内的结果，有两组公钥[root@sh02-oscar-hapomc-prod-web01 ~] nl ~/.ssh/authorized_keys      1    ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC9skly4Dl68BeKuAZdevQEgxylu1HQINn8QrnEx3DJGL43Jo3nIYbC0GAflUHzRRFBb4JLrICm63FuTzwZMwiBK1fr339NZqMyBk2hUqwAY8u1eG/JagGGsXsNSpaCwVEsGBw5OnC2SGxLIwfINZD7zH8dauONvoCqIe0SH44q0eKWr5nHdybOwdq3Q6X3lTuNI91QxP4LkrtyZG07j/b4DOCHLT4KxDYur69zB64vOTboDiTBfqP/syGkRXsGnJmZt/s3Uk0DEGoU6ZReYBvbK+cr43UPnQiSKxieceKm884DyVZRPQZcmOydKPWEFcCsaDafiR+2lwfwDUxDEQu7 root@sh02-oscar-hapomc-pp-web01.novalocal     2    ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDwlTic1GjMw9YiMnwsJ4f/94S30fLD+0BhX0691CwIvEBFbWTd4Hn60f7/wcuChck1Q9ICcLS6fd9jM5GoOxmyIwQ6P3+mCweexcaKyS18I6o2PjCTdV0LWooLlTIzg9N00frtw8e46dEtbY08WgS+xzASv3Dcw/WDjhqokCxaSbFPPMzWT8pd8r4aWWNsWvtINe84FW9k0YlGYUxmEw2kwuT6aJJgYSrEkwMRyw8Iw+HzoBexr6KQc85eCjEYFieiqLg3HGvenf+VP0yht20wbRxxYU1R/0NYNiClSnTazl6G9cFVEHQ39FgNP+LR+yLnCCrWATC2d6UI2ynXDqH9 root@sh02-oscar-hapomc-prod-web01.novalocal# 重复上述操作，完成子节点prod-web01至pp-web01的公钥分发[root@sh02-oscar-hapomc-prod-web01 ~] scp ~/.ssh/authorized_keys root@webpp34:~/.ssh/# Status: OK</code></pre><h5 id="1-3-挂载yum源-LVM（补充，非相关）"><a href="#1-3-挂载yum源-LVM（补充，非相关）" class="headerlink" title="1-3 挂载yum源/LVM（补充，非相关）"></a>1-3 挂载yum源/LVM（补充，非相关）</h5><ul><li>yum源</li></ul><pre><code class="bash"># 分发镜像[root@sh02-oscar-hapomc-pp-web01 mnt] scp /mnt/CentOS-7-x86_64-DVD-1908.iso root@web25:/mnt[root@sh02-oscar-hapomc-pp-web01 mnt] mkdir -p /mnt/yum[root@sh02-oscar-hapomc-pp-web01 mnt] cp /etc/yum.repo.d/CentOS-Base.repo /etc/yum.repo.d/CentOS-Base.repo.bak[root@sh02-oscar-hapomc-pp-web01 mnt] vim /etc/yum.repo.d/CentOS-Base.repo# CentOS-Base.repo[CentOS-7]name=CentOSbaseurl=file:///mnt/yumenabled=1gpgcheck=0# gpgkey=file:///mnt/yum/RPM-GPG-KEY-CentOS-7######mount -o loop -t iso9660 /mnt/CentOS-7-x86_64-DVD-1908.iso /mnt/yum/scp /etc/yum.repos.d/CentOS-Base.repo root@web25:/etc/yum.repos.d/# 并在其它节点执行光盘挂载# scp /etc/yum.repos.d/CentOS-Base.repo root@web56:/etc/yum.repos.d/# 确认分发结果# nl /etc/yum.repos.d/CentOS-Base.repo</code></pre><ul><li>LVM</li></ul><p><code>Tips</code>若没有lvm相关命令，可以挂载对应系统磁盘，yum -y install lvm2</p><pre><code class="bash"># 查看磁盘情况fdisk -l</code></pre><p><a href="https://blog.51cto.com/qicheng0211/1620171" target="_blank" rel="noopener">转</a></p><img src="lvm.jpg" srcset="/img/loading.gif" title="LVM"><pre><code class="bash"># 查看物理卷/逻辑卷组/逻辑卷情况pvdisplayvgdisplaylvdisplay# 创建逻辑卷组PV，假设这里的磁盘叫/dev/vdb1pvcreate /dev/vdb1pvdisplay# 基于PV新建VG，有多个的话就直接加在后面vgcreate vg1 /dev/vdb1 /dev/vdb2# 扩展现有VGvgextend vg1 /dev/vdb1# 基于VG创建逻辑卷LVlvcreate -n lv1 -L 10G vg1# 格式化并挂载 mkfs.xfs /dev/vg1/lv1mkdir /lvm     # 确认lvm为新的空路径，不可用非空路径mount /dev/vg1/lv1 /u1# 设置开机挂载vim /etc/fstab/dev/vg1/lv1 /lvm xfs    defaults  0 0</code></pre><p><code>Tips</code>CentOS 7：mkfs.xfs      CentOS6：mkfs.ext4（resize2fs）</p><h3 id="02-环境配置（部分Linux基础环境配置暂未罗列，如host、jdk等）"><a href="#02-环境配置（部分Linux基础环境配置暂未罗列，如host、jdk等）" class="headerlink" title="02. 环境配置（部分Linux基础环境配置暂未罗列，如host、jdk等）"></a>02. 环境配置（部分Linux基础环境配置暂未罗列，如host、jdk等）</h3><h4 id="环境包-rz-至相关路径，分流至其它节点："><a href="#环境包-rz-至相关路径，分流至其它节点：" class="headerlink" title="环境包 rz 至相关路径，分流至其它节点："></a>环境包 rz 至相关路径，分流至其它节点：</h4><h5 id="2-1-组件-amp-源码分发"><a href="#2-1-组件-amp-源码分发" class="headerlink" title="2-1 组件&amp;源码分发"></a>2-1 组件&amp;源码分发</h5><pre><code class="bash"># 文件解压：hadoop-2.6.1/etc/hadoopcd /usr/local/src/tar -xvf hadoop-2.6.1.tar.gz</code></pre><h5 id="2-2-当前用户环境变量"><a href="#2-2-当前用户环境变量" class="headerlink" title="2-2 当前用户环境变量"></a>2-2 当前用户环境变量</h5><pre><code class="bash">vim /etc/hosts# hosts192.168.73.10 master192.168.73.11 slave1192.168.73.12 slave2192.168.73.13 slave3######vim ~/.bashrc# ~/.bashrc# SET JAVA PATHexport JAVA_HOME=/usr/local/src/jdk1.8.0_201export JRE_HOME=${JAVA_HOME}/jreexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/libexport PATH=${JAVA_HOME}/bin:$PATH# SET HADOOP PATHexport HADOOP_HOME=/usr/local/src/hadoop-2.6.1export PATH=$PATH:$HADOOP_HOME/bin# SET HIVE PATHexport HIVE_HOME=/usr/local/src/apache-hive-1.2.2-binexport PATH=$PATH:$HIVE_HOME/bin# SET SCALA PATHexport SCALA_HOME=/usr/local/src/scala-2.11.8export PATH=$PATH:$SCALA_HOME/bin# SET INI PATHexport INI_PATH=/usr/local/src# SET SPARK PATHexport SPARK_HOME=/usr/local/src/spark-2.0.2-bin-hadoop2.6export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin####### 本地生效source ~/.bashrcscp ~/.bashrc root@slave1 ~/.bashrcscp ~/.bashrc root@slave2 ~/.bashrc# 节点分发&amp;生效ssh slave1source ~/.bashrcexitssh slave2source ~/.bashrc</code></pre><h5 id="2-3-配置文件修改-amp-相关文件夹创建"><a href="#2-3-配置文件修改-amp-相关文件夹创建" class="headerlink" title="2-3 配置文件修改&amp;相关文件夹创建"></a>2-3 配置文件修改&amp;相关文件夹创建</h5><pre><code class="bash">cd /usr/local/src/hadoop-2.6.1/etc/hadoopvim hadoop-env.sh     ⬅ 配置JAVA环境# hadoop-env.shexport JAVA_HOME=/usr/local/src/jdk1.8.0_201####### yarn-env.sh貌似我自己没改配置vim yarn-env.sh# yarn-env.shexport JAVA_HOME=/usr/local/src/jdk1.8.0_152######vim slaves     ⬅ DataNode节点# 配置DN节点# slavesslave1slave2######</code></pre><pre><code class="xml">vim core-site.xml     ⬅ 核心配置文件# 配置master节点IP及Hadoop临时文件根目录tmp# core-site.xml&lt;configuration&gt;     &lt;property&gt;          &lt;name&gt;fs.defaultFS&lt;/name&gt;          &lt;value&gt;HDFS://master:9000&lt;/value&gt;          &lt;description&gt;默认文件系统为HDFS文件系统&amp;NN节点位置&amp;对应网络端口&lt;/description&gt;     &lt;/property&gt;     &lt;property&gt;          &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;          &lt;value&gt;file:/usr/local/src/hadoop-2.6.1/tmp/&lt;/value&gt;     &lt;/property&gt;&lt;/configuration&gt;</code></pre><pre><code class="xml">vim hdfs-site.xml     ⬅ 配置HDFS相关的参数# 配置NameNode/DataNode/Replication（备份）文件目录# hdfs-site.xml&lt;configuration&gt;&lt;property&gt;     &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;     &lt;value&gt;master:9001&lt;/value&gt;     &lt;description&gt;NN助理节点位置&amp;对应网络端口&lt;/description&gt;&lt;/property&gt;&lt;property&gt;     &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;     &lt;value&gt;file:/usr/local/src/hadoop-2.6.1/dfs/name&lt;/value&gt;&lt;/property&gt;&lt;property&gt;     &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;     &lt;value&gt;file:/usr/local/src/hadoop-2.6.1/dfs/data&lt;/value&gt;&lt;/property&gt;&lt;property&gt;     &lt;name&gt;dfs.replication&lt;/name&gt;     &lt;value&gt;1&lt;/value&gt;&lt;/property&gt;&lt;property&gt;     &lt;name&gt;dfs.permissions&lt;/name&gt;     &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;######</code></pre><pre><code class="xml">vim mapred-site.xml     ⬅ mapreduce# mapred-site.xml&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;        &lt;value&gt;yarn&lt;/value&gt;    &lt;/property&gt;     &lt;property&gt;         &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;         &lt;value&gt;http://master:10020&lt;/value&gt;     &lt;/property&gt;     &lt;property&gt;         &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;         &lt;value&gt;http://master:19888&lt;/value&gt;     &lt;/property&gt;&lt;/configuration&gt;######</code></pre><pre><code class="xml">vim yarn-site.xml     ⬅ YARN（RM）配置# yarn-site.xml    &lt;property&gt;          &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;          &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;          &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;          &lt;value&gt;master:8032&lt;/value&gt;          &lt;description&gt;Yarn(RM)所在节点位置&amp;对应网络端口&lt;/description&gt;    &lt;/property&gt;    &lt;property&gt;          &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;          &lt;value&gt;master:8030&lt;/value&gt;    &lt;/property&gt;          &lt;property&gt;          &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;          &lt;value&gt;master:8035&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;          &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;          &lt;value&gt;master:8033&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;          &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;          &lt;value&gt;master:8088&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 关闭虚拟内存检查--&gt;    &lt;property&gt;          &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;          &lt;value&gt;false&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;######</code></pre><pre><code class="bash"># 根据上述配置的各个路径，创建好对应的文件目录mkdir /usr/local/src/hadoop-2.6.1/tmpmkdir -p /usr/local/src/hadoop-2.6.1/dfs/namemkdir -p /usr/local/src/hadoop-2.6.1/dfs/data</code></pre><h5 id="2-4-主节点配置好的-Hadoop-源码分发至集群其余节点"><a href="#2-4-主节点配置好的-Hadoop-源码分发至集群其余节点" class="headerlink" title="2-4 主节点配置好的 Hadoop 源码分发至集群其余节点"></a>2-4 主节点配置好的 Hadoop 源码分发至集群其余节点</h5><pre><code class="bash">scp /usr/local/src/hadoop-2.6.1 root@slave1:/usr/local/src/hadoop-2.6.1scp /usr/local/src/hadoop-2.6.1 root@slave2:/usr/local/src/hadoop-2.6.1</code></pre><h3 id="03-Hadoop集群初始化-启动-关闭"><a href="#03-Hadoop集群初始化-启动-关闭" class="headerlink" title="03. Hadoop集群初始化/启动/关闭"></a>03. Hadoop集群初始化/启动/关闭</h3><h4 id="集群格式化-集群启动-JAVA进程查看-集群关闭"><a href="#集群格式化-集群启动-JAVA进程查看-集群关闭" class="headerlink" title="集群格式化/集群启动/JAVA进程查看/集群关闭"></a>集群格式化/集群启动/JAVA进程查看/集群关闭</h4><p><code>Tips</code> 初始化只需要做一次！！！</p><h5 id="3-1-格式化Hadoop-NN"><a href="#3-1-格式化Hadoop-NN" class="headerlink" title="3-1 格式化Hadoop(NN)"></a>3-1 格式化Hadoop(NN)</h5><pre><code class="bash">cd $HADOOP_HOME/binhadoop namenode -formatcd $HADOOP_HOME/sbin./start-all.sh# 关闭集群cd $HADOOP_HOME/sbin./stop-all.ssh</code></pre><h5 id="3-2-jps查看集群状态"><a href="#3-2-jps查看集群状态" class="headerlink" title="3-2 jps查看集群状态"></a>3-2 jps查看集群状态</h5><pre><code class="bash"># master[root@master bin]# jps2784 NameNode       ⬅ NN3122 ResourceManager    ⬅ RM13827 Jps6853 Master     ⬅ (这个为Spark集群启动标识，Hadoop启动时不包括)2943 SecondaryNameNode  ⬅ Secondary Bak NN# slave1/2[root@slave1 ~]# jps17378 nodemanager    ⬅ NM17270 DataNode       ⬅ DN18122 Jps</code></pre><hr><h3 id="04-HDFS-amp-YARN"><a href="#04-HDFS-amp-YARN" class="headerlink" title="04. HDFS &amp; YARN"></a>04. HDFS &amp; YARN</h3><h4 id="HDFS架构及变化"><a href="#HDFS架构及变化" class="headerlink" title="HDFS架构及变化"></a>HDFS架构及变化</h4><h5 id="4-1-HDFS架构（分布式存储）"><a href="#4-1-HDFS架构（分布式存储）" class="headerlink" title="4-1 HDFS架构（分布式存储）"></a>4-1 HDFS架构（分布式存储）</h5><p><code>狭义Hadoop</code> 也就是指Hadoop2.x下的4个组件，Common（RPC框架）/HDFS/YARN/MapReduce.</p><p><code>Block</code> Hadoop 2.x中，默认Block大小为：1 Block = 128MB；Hadoop 1.x中默认Block大小为：1 Block = 64MB.<br><code>NameNode</code> 集群老大，处理客户端的读写请求，NN的<strong>内存</strong>元信息中，包含文件路径、文件副本数(根据配置或-setrep来确定副本数)、blockid、每个Block所在DataNode的信息（动态），NN主要处理HDFS中的<strong>两个重要关系</strong>：</p><ol><li><strong>元信息持久化</strong>，持久化到物理内存中，也就是命名空间镜像<code>fsimage</code>，但<strong>不包含每个Block的位置信息</strong>（这也就是为什么集群在启动初期会进入安全模式的原因，因为集群启动时，NN读取的<code>fsimage</code>中不包含Block所在DN的具体位置），存放元信息的文件是<code>fsimage</code>，而系统运行期间所有对元信息的<strong>操作</strong>都保存在内存中，并被持久化到另一个文件<code>edits</code>（编辑日志）中，两者合二为一提供了HDFS的<strong>第一重要关系</strong>.</li><li><strong>（持久）</strong>管理文件系统目录树，即文件系统目录与数据块的对应关系（持久化到物理内存文件<code>fsimage</code>中），简单来说就是HDFS目录树/元信息和文件数据块的索引.</li><li><strong>（动态）DN和数据块的对应关系</strong>（不会持久化到物理内存文件中，NN启动时，由DN动态上报而得的<strong>第二个重要关系</strong>），简单来说就是用来记录块数据（Block）所在DN的具体位置（形似索引）；同时NN还可以通过DN获取HDFS整体运行状态的一些信息.</li></ol><p><code>SecondNameNode</code> 不算是NN的备份节点，只是协助NN，分担压力，SNN会定时去NN中获取<code>edits</code>，更新至<strong>SNN中的fsimage</strong>中，SNN一旦有了新的fsimage，便会以<code>fsimage.ckpt</code>的形式覆盖NN中的fsimage（合并），便于NN在下次启动时使用，减少重启时间；同理，合并后的fsimage可以用来恢复故障的NN，但由于SNN生成的fsimage本身就具有滞后性，故存在数据丢失的风险.</p><p><code>DataNode</code> 整个集群真正的数据块，负责存储数据块，负责为系统客户端提供数据块的读写服务，负责各个DN间的通信（副本处理，replication）；<code>心跳机制</code>定期报告文件块列表信息至NN，确保DN正常存活.</p><h5 id="4-2-四个核心机制"><a href="#4-2-四个核心机制" class="headerlink" title="4-2 四个核心机制"></a>4-2 四个核心机制</h5><p><strong><code>心跳机制</code></strong> </p><ol><li>NN启动后，开启IPC（通信方式）.</li><li>DN需要连接NN，并在特定间隔时间下(3s)汇报自身的状况给NN.</li><li>NN发布命令也是通过心跳发送给DN.</li><li>Namenode 感知到 Datanode 掉线死亡的时长计算：HDFS 默认的超时时间为10分钟+30 秒.</li><li>计算公式为：timeout = 2 * dfs.namenode.heartbeat.recheck-interval + 10 * dfs.heartbeat.interval.</li><li>默认的 <code>heartbeat.recheck.interval</code> 大小为5分钟，<code>dfs.heartbeat.interval</code> 默认的大小为3秒.</li><li>需要注意的是 hdfs-site.xml 配置文件中的heartbeat.recheck-interval的单位为毫秒，dfs.heartbeat.interval 的单位为秒.</li></ol><p><strong><code>安全模式</code></strong></p><ol><li></li><li></li></ol><p><strong><code>副本存放策略</code></strong></p><ol><li>高可靠：切分+副本；首先是数据按照Block块大小切分，并按照副本配置（dfs.replication = 3）或临时指定（-setrep），生成各自的副本.</li><li></li><li>数据按副本数量进行备份的过程是动态的，什么意思呢，就是说我们定义副本数为3时，则系统会在备份不足3的时候，再新添新的副本数据.</li></ol><p><strong><code>负载均衡</code></strong></p><h5 id="4-3-YARN-VS-Hadoop-1-x（运算资源调度）"><a href="#4-3-YARN-VS-Hadoop-1-x（运算资源调度）" class="headerlink" title="4-3 YARN VS Hadoop 1.x（运算资源调度）"></a>4-3 YARN VS Hadoop 1.x（运算资源调度）</h5><p><code>YARN (RM)</code> Cluster Resource Manager 集群Hadoop1.x中的集群管理器<code>JobTracker</code>的替代者.<br><code>NodeManager (NM)</code> <code>TaskTracker</code>的替代者，各个节点上的NM与RM通过心跳通信，以确保自身的健康性.<br><code>ApplicationMaster(AM)</code> 一个专用且短暂的<code>JobTracker</code>，RM收集来自NM的状态信息，选择合适的NM启动<code>Container</code>（任务的运行资源&lt;节点/内存/CPU&gt;）并运行AM；而AM会根据实际情况向RM申请更多<code>Container</code>资源，以完成分布式计算.<br><code>Container</code> YARN中资源的抽象封装，包含某个节点上一定量的资源（CPU和内存两类资源）.<br><img src="YARN.png" srcset="/img/loading.gif" title="YARN资源调度过程（八斗教材）"></p><h5 id="4-4-Tips-传统JobTracker的资源管理-任务协调被分开为两种不同类型的进程来反馈"><a href="#4-4-Tips-传统JobTracker的资源管理-任务协调被分开为两种不同类型的进程来反馈" class="headerlink" title="4-4 Tips 传统JobTracker的资源管理/任务协调被分开为两种不同类型的进程来反馈."></a>4-4 <code>Tips</code> 传统<code>JobTracker</code>的<code>资源管理</code>/<code>任务协调</code>被分开为两种不同类型的进程来反馈.</h5><ol><li><u>集群资源管理</u>：也就是ResourceManager（RM/YARN）.</li><li><u>任务协调</u>：对于提交给集群的每个<code>Application</code>（<strong>2.X</strong>中<strong>Job</strong>的概念被<strong>Application</strong>替代），都会启动一个专用且短暂的JobTracker来控制，这类专用且短暂的管理器就是Hadoop 2.x中的<code>ApplicationMaster (App Mstr)</code>，从属节点的AM由各Slaves上的<code>NodeManager</code>（TaskTracker）启动.<img src="HDFS.png" srcset="/img/loading.gif" title="HDFS（八斗教材）"></li></ol><hr><h4 id="HDFS基本操作"><a href="#HDFS基本操作" class="headerlink" title="HDFS基本操作"></a>HDFS基本操作</h4><h5 id="4-5-命中率较高且暂未练习过的shell命令"><a href="#4-5-命中率较高且暂未练习过的shell命令" class="headerlink" title="4-5 命中率较高且暂未练习过的shell命令"></a>4-5 命中率较高且暂未练习过的shell命令</h5><pre><code class="shell"># 功能：设置 HDFS 中文件的副本数量hdfs dfs -setrep 3 /aaa/jdk.tar.gz# 功能：查看 dfs 集群工作状态的命令hdfs dfsadmin -report# 安全模式hdfs dfsadmin -safemode get/leave/enter/wait# 上传hdfs dfs -put /data/hive/student.txt /bbb/jdk.tar.gz.2# Drophdfs dfs -rm /data/hive_hdfs/student.txt&gt;&gt;20/02/14 17:52:02 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion &gt;&gt;interval = 0 minutes, Emptier interval = 0 minutes.&gt;&gt;Deleted /data/hive_hdfs/student.txt# 查询并杀死任务hadoop job -list   # version &lt; 2.3.0hadoop job -kill $Application-Idyarn application -list   # version &gt; 2.3.0yarn application -kill $Application-Id</code></pre><pre><code class="shell"># 常用命令参数介绍-help# 功能：输出这个命令参数手册hadoop -helphadoop fs -helphadoop fs -help ls-ls# 功能：显示目录信息hadoop fs -ls HDFS://master:9000/hadoop fs -ls /  # 这些参数中，所有的 HDFS 路径都可以简写成 hadoop fs -ls / 等同上条命令的效果-mkdir# 功能：在 HDFS 上创建目录hadoop fs -mkdir -p /aa/bb/cc/dd-put# 功能：等同于 -copyFromLocal 从本地上传文件至HDFS                 ⬇ 本地路径       ⬇ HDFS路径hadoop fs -put /aaa/jdk.tar.gz /bbb/jdk.tar.gz.2-get# 功能：等同于 copyToLocal，就是从 HDFS 下载文件到本地hadoop fs -get /aaa/jdk.tar.gz-getmerge# 功能：合并下载多个文件# 例如：getmerge 如 HDFS 的目录 /aaa/下有多个文件:log.1, log.2,log.3,...hadoop fs -getmerge /aaa/log.* ./log.sum-cp# 功能：从 HDFS 的一个路径拷贝 HDFS 的另一个路径hadoop fs -cp /aaa/jdk.tar.gz /bbb/jdk.tar.gz-mv# 功能：在 HDFS 目录中移动文件hadoop fs -mv /aaa/jdk.tar.gz /ccc-rm# 功能：删除文件或文件夹hadoop fs -rm -r /aaa/bbb/-rmdir# 功能：删除空目录hadoop fs -rmdir /aaa/bbb/ccc-moveFromLocal# 功能：从本地剪切到 HDFShadoop fs - moveFromLocal /data/hadoop/a.txt /aa/bb/cc/dd-moveToLocal# 功能：从 HDFS 剪切到本地hadoop fs - moveToLocal /aa/bb/cc/dd /home/hadoop/a.txt-copyFromLocal# 功能：从本地文件系统中拷贝文件到 HDFS 文件系统去hadoop fs -copyFromLocal ./jdk.tar.gz /aaa/-copyToLocal# 功能：从 HDFS 拷贝到本地hadoop fs -copyToLocal /aaa/jdk.tar.gz-appendToFile# 功能：追加一个文件到已经存在的文件末尾hadoop fs -appendToFile ./hello.txt HDFS://hadoop-server01:9000/hello.txt可以简写为：hadoop fs -appendToFile ./hello.txt /hello.txt-cat# 功能：显示文件内容hadoop fs -cat /hello.txt-tail# 功能：显示一个文件的末尾hadoop fs -tail /weblog/access_log.1-text# 功能：以字符形式打印一个文件的内容hadoop fs -text /weblog/access_log.1-chgrp-chmod-chown# 功能： linux 文件系统中的用法一样，对文件所属权限hadoop fs -chmod 666 /hello.txthadoop fs -chown someuser:somegrp /hello.txt-df# 功能：统计文件系统的可用空间信息hadoop fs -df -h /-du# 功能：统计文件夹的大小信息hadoop fs -du -s -h /aaa/*-count# 功能：统计一个指定目录下的文件节点数量hadoop fs -count /aaa/-setrep# 功能：设置 HDFS 中文件的副本数量hadoop fs -setrep 3 /aaa/jdk.tar.gz# 补充：查看 dfs 集群工作状态的命令HDFS dfsadmin -report</code></pre><hr><h4 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h4><h5 id="5-1-Map"><a href="#5-1-Map" class="headerlink" title="5-1 Map"></a>5-1 Map</h5><img src="MapReduce.png" srcset="/img/loading.gif" title="MapReduce（八斗教材）"><img src="MapReduce2.png" srcset="/img/loading.gif" title="MapReduce（八斗教材）"><img src="MapReduce3.png" srcset="/img/loading.gif" title="MapReduce（八斗教材）"><p><strong><code>Map</code></strong></p><ol><li>数据切分: 大型数据载入HDFS，并以BLOCK（64/128）形式，冗余存储于HDFS磁盘内，这也是Map端输入的最小粒度文件（FileSplit）.</li><li>数据块载入内存，达到溢写阈值（80%）时，进行磁盘写入，生成多个进行快排的分区数据（kv-&gt;pkv），写入磁盘，每个分区内的数据<strong>局部有序</strong>，按照pkv（partition, key, value）的顺序进行局部排序，同时，如果设置了<strong>Combiner</strong>参数，则在Map阶段就进行Reduce操作，即<strong>mapjoin</strong>，减少shuffler阶段，各Map节点间的数据传递I/O.</li><li>分区生成结束后，各个分区进行归并merge dump，由于局部有序，所以整个过程是归并排序的过程，非常快，并拼接成一个<strong>新的有序数据分区集合 =&gt; union == partition-0/1/2</strong>.</li><li>每个Map均进行了上述Map操作，生成各自的<strong>有序数据分区集合</strong>，供后续shuffle后进行Reduce.</li><li>压缩: 通过参数对Map的结果进行压缩，可以较少shuffle过程中数据I/O.   <pre><code class="bash">mapred.compress.map.output=true    # map stage compressionmapred.map.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec</code></pre></li></ol><p><strong><code>Reduce</code></strong></p><ol><li>每个Reduce节点仅处理一组hash(key)所对应的所有值，各个Map节点生成的有序数据分区集合，按照排序的结果各自切块，并shuffle至各自对应的Reduce节点下，Reduce-0/1/2.</li><li>两两合并，最后进行Reduce合并，生成结果集，<strong>partition相同的数据进入同一个的Reduce</strong>，所以各Reduce间的数据key不会重复.</li><li>多个Reduce节点下的结果集再进行最后的合并.</li><li>压缩: 通过参数对Reduce的结果进行压缩，可以较少输出结果占用HDFS存储.<pre><code class="bash">mapred.output.compress=true    # reduce output compressionmapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec</code></pre><img src="MapReduce4.png" srcset="/img/loading.gif" title="MapReduce（八斗教材）"></li></ol><p><code>MR任务调度</code> 详见上文<strong>4-3/4-4</strong>.</p><h5 id="5-2-Hadoop-Streaming"><a href="#5-2-Hadoop-Streaming" class="headerlink" title="5-2 Hadoop Streaming"></a>5-2 Hadoop Streaming</h5><p><code>Tips</code> hadoop fs -text /xxx</p>]]></content>
    
    
    
    <tags>
      
      <tag>Hadoop 2.X</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20200831_1200_数仓面试（小记）</title>
    <link href="/2020/09/07/20200831_1200_%E6%95%B0%E4%BB%93%E9%9D%A2%E8%AF%95%EF%BC%88%E5%B0%8F%E8%AE%B0%EF%BC%89/"/>
    <url>/2020/09/07/20200831_1200_%E6%95%B0%E4%BB%93%E9%9D%A2%E8%AF%95%EF%BC%88%E5%B0%8F%E8%AE%B0%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h2 id="数据仓库"><a href="#数据仓库" class="headerlink" title="数据仓库"></a>数据仓库</h2><h3 id="01-面试"><a href="#01-面试" class="headerlink" title="01. 面试"></a>01. 面试</h3><ul><li>Blog Point</li></ul><h4 id="OLAP"><a href="#OLAP" class="headerlink" title="OLAP"></a>OLAP</h4><ol><li>DB数据扫描模式（行/列，别想太多算法，就是基本模式）.</li><li>Hive存储文件的几种格式及其区别（ORC行集合做列式存储/Parquet列示数据存储/sequence file<strong>BLOCK&gt;NONE&gt;RECORD</strong>）.</li><li>Hive的关联算法和普通关系型数据库的区别（Hive特有的分桶使其支持Bucket join/Sort-Bucket-Merge Mapjoin），即<strong>分桶排序后的大表切片</strong>，是解决大表关联大表的关键！！！非常重要，两张分桶clustered（distrubute by sorted by）的表，join阶段的性能会表现得很优秀，<strong>也可以自主设置mapred.reduce.tasks去适配bucket个数，推荐使用’set hive.enforce.bucketing = true’</strong>.</li><li>数据治理（除了完整性外，还需关注及时性&lt;<strong>insert_time，之前一直不理解多时间戳的意义</strong>&gt;，及时性&lt;<strong>数据到达/数据处理</strong>&gt;，有效性&lt;<strong>数据指标发生反转，如何监控和规避</strong>&gt;）.</li><li>数据仓库主题（这部分一直回答的不理想，主要是语言组织，如何将现有模型归并至仓库概念，是需要思考的）.</li></ol><h4 id="OLTP"><a href="#OLTP" class="headerlink" title="OLTP"></a>OLTP</h4><ol><li>Join算法的区别和关联（Nested Loop Join/Hash Join/Sort-Merge Join），切记将此处的归并排序和MapReduce中的大大关联操作，联系起来理解.</li><li>OLAP系统不会在意exist和not exist的过多区别，但是OLTP侧需要关注，严重影响效率.</li><li>Hash Join并不是最快的，Hash Join Semi才是比较优化的半连接，即使用<strong>in</strong>和<strong>exists</strong>关键字.</li></ol><h4 id="CAP-BASE-ACID"><a href="#CAP-BASE-ACID" class="headerlink" title="CAP/BASE/ACID"></a>CAP/BASE/ACID</h4><ol><li>CAP（BASE VS ACID）.</li><li>BASE.</li><li>ACID.</li><li>数据治理过程中，如何第一时间发现数据质量出现问题.</li></ol><h3 id="02-理论"><a href="#02-理论" class="headerlink" title="02. 理论"></a>02. 理论</h3><pre><code class="bash"></code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>数据仓库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20200323_1205_Python3-Kafka-Producer</title>
    <link href="/2020/08/06/20200323-1205-Python3-Kafka-Producer/"/>
    <url>/2020/08/06/20200323-1205-Python3-Kafka-Producer/</url>
    
    <content type="html"><![CDATA[<h2 id="Python3-KafkaProducerWithPostgreSQL"><a href="#Python3-KafkaProducerWithPostgreSQL" class="headerlink" title="Python3 KafkaProducerWithPostgreSQL"></a>Python3 KafkaProducerWithPostgreSQL</h2><h3 id="01-API"><a href="#01-API" class="headerlink" title="01. API"></a>01. API</h3><ul><li>Repo Point<br>  kafka-python<br>  psycopg2</li></ul><h3 id="02-Kafka类"><a href="#02-Kafka类" class="headerlink" title="02. Kafka类"></a>02. Kafka类</h3><ul><li>class_Kafka_producer_linux_section2.py</li></ul><pre><code class="python"># -*- coding:utf-8 -*-# @Author: Shin# @Date: 20/3/23 10:13# @File: class_Kafka_producer_linux_section2.py# @Usage: Kafka producer for kafka_2.11-0.10.2.1from kafka import KafkaProducerimport threading# Self Repo# from src.func_demo.func_f import clog2, errclog2from func_demo.func_f import clog2, errclog2class KafkaProducerPython(object):    def __init__(self, topic, bootstrap_servers=&#39;localhost:9092&#39;):        &quot;&quot;&quot;        :param bootstrap_servers:   host:port for Kafka        :param topic:   Kafka topic        &quot;&quot;&quot;        self.bootstrap_servers = bootstrap_servers        self.topic = topic        self.producer = None        self.future_record_metadata = None    def producer_send(self, value_byte):        &quot;&quot;&quot;        :param value_byte:  str or json_str in bytes        :return:    &lt;class &#39;kafka.producer.future.FutureRecordMetadata&#39;&gt;        &quot;&quot;&quot;        try:            self.producer = KafkaProducer(bootstrap_servers=self.bootstrap_servers)            # clog2.logger.warning(f&#39;Kafka Producer Begin: {self.producer}&#39;)            self.future_record_metadata = self.producer.send(self.topic, value_byte)   # &lt;class &#39;kafka.producer.future.FutureRecordMetadata&#39;&gt;            # print(type(self.future_record_metadata))            return self.future_record_metadata        except Exception as e:            clog2.logger.error(f&#39;Kafka producer send error... {e}&#39;)        finally:            self.producer.flush()    def producer_send_callback(self, future_record_metadata):        &quot;&quot;&quot;        :param future_record_metadata: &lt;class &#39;kafka.producer.future.FutureRecordMetadata&#39;&gt; producer.send 返回        :return:         &quot;&quot;&quot;        # log = Logger(filename = f&#39;./logs/all_bokeh.log&#39;, level=&#39;info&#39;, when=&#39;H&#39;)        # clog2.logger.info(f&#39;future_record_metadata.topic: {future_record_metadata.topic}&#39;)        clog2.logger.info(f&#39;future_record_metadata.offset: {future_record_metadata.offset}&#39;)        # clog2.logger.info(f&#39;future_record_metadata.partition: {future_record_metadata.partition}&#39;)        # clog2.logger.warning(f&#39;Kafka Producer End: {self.producer}\n&#39;)    # def on_send_err(self):    # clog2.logger.info(&#39;I am an errback&#39;, exc_info=excp)    def producer_send_log(self, info):        &quot;&quot;&quot;        :param info: &lt;class json2byte&#39;&gt;        :return:         &quot;&quot;&quot;        clog2.logger.info(f&#39;future_record_metadata.info: {info}&#39;)    def to_bytes(self, v):        v_byte = bytes(str(v).strip(), encoding=&#39;utf-8&#39;)        return v_byte        # return str(v)class MyThread(threading.Thread):    def __init__(self, func=None, args=()):        super().__init__()        self.func = func        self.args = args        self.result = []    def run(self):        self.result = self.func(*self.args)    def get_result(self):        # noinspection PyBroadException        try:            return self.result        except Exception as e:            errclog2.logger.error(f&#39;Results within threading. {e}&#39;)            return 1</code></pre><hr><h3 id="03-Database类"><a href="#03-Database类" class="headerlink" title="03. Database类"></a>03. Database类</h3><ul><li>class_DB_demo.py</li></ul><pre><code class="python"># -*- coding:utf-8 -*-# @Author: Shin# @Date: 20/3/29 12:19# @File: class_DB_demo.py# @Usage: import pymysql as pmimport psycopg2 as pgfrom psycopg2.extras import execute_values# Self Repofrom src.func_demo.logging_f import *# 下面的三行包其实是main函数中使用，有部分重复# Self Repo# from src.class_DB_demo import UseMySQL, UsePostgreSQL, ExecutePostgreSQLfrom src.conf.kafka_conf import mysql_conn, pgsql_conn, pgsql_conn_pp, pgsql_conn_prodfrom src.func_demo.logging_f import *class UseMySQL(object):    def __init__(self, config:dict):        self.configurantion = config    def __enter__(self):        self.conn = pm.connect(**self.configurantion)        self.cursor = self.conn.cursor()        return self.cursor    def __exit__(self, exc_type, exc_val, exc_tb):        self.conn.commit()        self.cursor.close()        self.conn.close()class UsePostgreSQL(object):    def __init__(self, config:dict):        self.config = config    def __enter__(self):        self.conn = pg.connect(**self.config)        self.cursor = self.conn.cursor()        return self.cursor    def __exit__(self, exc_type, exc_val, exc_tb):        self.conn.commit()        self.cursor.close()        self.conn.close()class ExecutePostgreSQL(object):    def __init__(self, kwargs:dict):        self.cur = kwargs[&#39;cur&#39;]        self.sql = kwargs[&#39;sql&#39;]        self.argslist = kwargs[&#39;argslist&#39;]    def pg_insert(self):        # super(ExecutePostgreSQL, self).__enter__()        try:            log.logger.warning(f&#39;UsePostgreSQL.pg_insert Begin.&#39;)            log.logger.info(f&#39;Total rows for current rollover: {len(self.argslist)}&#39;)            execute_values(cur=self.cur, sql=self.sql, argslist=self.argslist)            log.logger.warning(f&#39;UsePostgreSQL.pg_insert End.&#39;)        except Exception as e:            errlog.logger.error(f&#39;pg_insert: {e}&#39;)# Demoif __name__ == &#39;__main__&#39;:    # MySQL cursor    try:        log.logger.warning(f&#39;MySQL Begin...&#39;)        with UseMySQL(mysql_conn) as cursor:            sql_1 = &quot;&quot;&quot;show tables&quot;&quot;&quot;            sql_1 = &quot;&quot;&quot;select * from MURA.t_pm_alarm_tst&quot;&quot;&quot;            cursor.execute(sql_1)            data = cursor.fetchall()            col0 = [x for x in data]    # [(),(),()]            k = 0            for rs in col0:                k += 1                print(f&#39;k:{k}   v:{rs}\nc1:{rs[0]} c2:{rs[1]} c3:{rs[2]} c4:{rs[3]} c5:{rs[4]} c6:{rs[5]} c7:{rs[6]}\n&#39;)     except Exception as e:        log.logger.error(f&#39;{e}\n&#39;)        errlog.logger.error(f&#39;{e}&#39;)    # PostgreSQL cursor     try:        log.logger.warning(f&#39;PostgreSQL Begin!!!&#39;)        with UsePostgreSQL(pgsql_conn_pp) as cursor:            sql_1 = &quot;&quot;&quot;SELECT * FROM public.t_fm_alarm_fields&quot;&quot;&quot;            # sql_2 = &quot;&quot;&quot;SELECT COUNT(1) FROM public.t_fm_alarm_fields&quot;&quot;&quot;            sql_3 = &quot;&quot;&quot;INSERT INTO public.t_fm_alarm_fields VALUES %s&quot;&quot;&quot;            sql_4 = &quot;&quot;&quot;TRUNCATE TABLE public.t_fm_alarm_fields&quot;&quot;&quot;            size = 100            log.logger.info(f&#39;Current execution &lt;{sql_1}&gt;&#39;)            cursor.execute(sql_1)            rowcnt = cursor.rowcount            log.logger.info(f&#39;Total rows for current execution &lt;{sql_1}&gt;: {rowcnt}&#39;)            limitcnt, limitlast = divmod(rowcnt, size)            log.logger.info(f&#39;Based on interval: {size}, total rows for rollover: {limitcnt}, ending number: {limitlast}&#39;)            with UsePostgreSQL(pgsql_conn_prod) as cursor2_1:                cursor2_1.execute(sql_4)  # 清理            with UsePostgreSQL(pgsql_conn_prod) as cursor2_2:                for rn in range(limitcnt + 1):                    data = cursor.fetchmany(size=size)                    exe_dict = {                                    &quot;cur&quot;: cursor2_2,                                    &quot;sql&quot;: sql_3,                                    &quot;argslist&quot;: data                    }                    pg_prod = ExecutePostgreSQL(exe_dict)   # 批插入，更效率                    pg_prod.pg_insert()            with UsePostgreSQL(pgsql_conn_prod) as cursor3:                cursor3.execute(sql_1)                # result = cursor3.fetchall()                log.logger.info(f&#39;Total rows for execution: {cursor3.rowcount}&#39;)        log.logger.warning(f&#39;PostgreSQL End!!!&#39;)    except Exception as e:        log.logger.error(f&#39;{e}\n&#39;)        errlog.logger.error(f&#39;{e}&#39;)</code></pre><hr><h3 id="04-function工具函数"><a href="#04-function工具函数" class="headerlink" title="04. function工具函数"></a>04. function工具函数</h3><ul><li>func_f.py</li></ul><pre><code class="python"># -*- coding:utf-8 -*-# @Author: Shin# @Date: 20/4/10 15:19# @File: func_f.pyimport datetimeimport osimport reimport shutilimport loggingfrom logging import handlersfrom sys import exc_info# Self Repodef date_f(daysdelta=0, hoursdelta=0):    &quot;&quot;&quot;    :param timedelta: Day Intervals    :param hoursdelta: Hour Intervals    :return: date &lt;class:tuple&gt;            Ex: (&#39;20191205&#39;, &#39;2019120522&#39;, &#39;20191205 22:40:51&#39;,                 {&#39;year&#39;: &#39;2019&#39;, &#39;month&#39;: &#39;12&#39;, &#39;day&#39;: &#39;05&#39;, &#39;hour&#39;: &#39;22&#39;}, &#39;src.func_demo.func_f&#39;)    &quot;&quot;&quot;    date_str = (datetime.datetime.now() + datetime.timedelta(days=daysdelta, hours=hoursdelta)).strftime(&#39;%Y%m%d&#39;)    date_str_h = (datetime.datetime.now() + datetime.timedelta(days=daysdelta, hours=hoursdelta)).strftime(&#39;%Y%m%d%H&#39;)    date_str_s = (datetime.datetime.now() + datetime.timedelta(days=daysdelta, hours=hoursdelta)).strftime(&#39;%Y%m%d %H:%M:%S&#39;)    # 各维度时间戳段拆分    year = (datetime.datetime.now() + datetime.timedelta(days=daysdelta, hours=hoursdelta)).strftime(&#39;%Y&#39;)    month = (datetime.datetime.now() + datetime.timedelta(days=daysdelta, hours=hoursdelta)).strftime(&#39;%m&#39;)    day = (datetime.datetime.now() + datetime.timedelta(days=daysdelta, hours=hoursdelta)).strftime(&#39;%d&#39;)    hour = (datetime.datetime.now() + datetime.timedelta(days=daysdelta, hours=hoursdelta)).strftime(&#39;%H&#39;)    minute = (datetime.datetime.now() + datetime.timedelta(days=daysdelta, hours=hoursdelta)).strftime(&#39;%M&#39;)    date_dict = dict(year=year, month=month, day=day, hour=hour, minute=minute)    # 所有已知时间格式组合    date = (date_str, date_str_h, date_str_s, date_dict, __name__)    return datedef callback(funcname, *args, **kwargs):    &quot;&quot;&quot;    :param funcname:    name for function    :param args:    :param kwargs:    :return:            None: judgement for function operation results    &quot;&quot;&quot;    try:        result = funcname(*args, **kwargs)        if not result:            clog.logger.info(f&#39;Execute successfully: &lt;{funcname.__name__}&gt;&#39;)        else:            clog.logger.info(f&#39;Exception: &lt;{funcname.__name__}&gt;.{result}&#39;)    except Exception as e:        errclog.logger.error(f&#39;{e}&#39;)def file_mkdir(path, dir):    &quot;&quot;&quot;    :param path:    Target file path    Ex: r&#39;D:\Hadoop\PyFloder\bokeh_test\src\data&#39;    :param dir:     Target dir name     Ex: 20200412    :return:        0: regular                    1: exception                    2: redundant    &quot;&quot;&quot;    try:        for rw in os.walk(path):            for rs in rw:                if len(rs) &gt; 0 and rs == rw[1]:  # 跳过文件夹名列表，os.walk会直接遍历所有子文件夹                    try:                        if dir not in rw[1]:                            clog.logger.info(f&#39;Directory {dir} not existed, generated.&#39;)                            os.chdir(path)                            os.mkdir(dir)                            return 0                        else:                            clog.logger.info(f&#39;Directory already existed: {path}/{dir}&#39;)                            return 2                    except FileExistsError as e:                        clog.logger.error(f&#39;Directory generated error: {e}&#39;)                        errclog.logger.error(f&#39;Directory generated error: {e}&#39;)                        continue                else:                    continue    except Exception as e:        errclog.logger.error(f&#39;{e}&#39;)        return 1def file_rm_dir(path, pattern):    &quot;&quot;&quot;    :param path:        path contained the directory needed to be droped.    :param pattern:     pattern expression    :return:            0: regular                        1: exception    &quot;&quot;&quot;    # try:    #     for rw in os.walk(path):    #         for rs in rw:    #             if len(rs) &lt; 1:    #                 continue    #             elif isinstance(rs, str):    #                 # clog.logger.info(f&#39;Current path: {rs}&#39;)    #                 p = re.compile(pattern=pattern)    #                 match_result = &#39;&#39;.join(p.findall(rs))   # &lt;class &#39;list&#39;&gt; ---&gt; &lt;class &#39;str&#39;&gt;    #                 if match_result:    #                     # os.chdir(path)    #                     shutil.rmtree(match_result)    #                     clog.logger.info(f&#39;Path dropped: {match_result}&#39;)    #             elif isinstance(rs, list):    #                 pass    #    # except Exception as e:    #     errlog.logger.error(f&#39;{e}&#39;)    try:        for rs, _, _ in os.walk(path):            p = re.compile(pattern=pattern)            match_result = &#39;&#39;.join(p.findall(rs))   # &lt;class &#39;list&#39;&gt; ---&gt; &lt;class &#39;str&#39;&gt;            if match_result:                shutil.rmtree(match_result)                clog.logger.info(f&#39;Path has been dropped: {match_result}&#39;)            else:                clog.logger.info(f&#39;Targer path not existed in {rs}&#39;)        return 0    except Exception as e:        errclog.logger.error(f&#39;{e}&#39;)        return 1def file_rm_file(path, pattern):    &quot;&quot;&quot;    :param path:        path contained the files needed to be droped.    :param pattern:     pattern expression    :return:            0: regular                        1: no match                        2: exception    &quot;&quot;&quot;    try:        match_result = []        for _, _, file in os.walk(path):            for rs in file:                p = re.compile(pattern=pattern)                os.remove(rs)                match_result.extend(p.findall(rs))            if match_result:                clog.logger.info(f&#39;Match results: {match_result}&#39;)                return 0            else:                clog.logger.info(f&#39;Nothing matched.&#39;)                return 1    except Exception as e:        errclog.logger.error(f&#39;{e}&#39;)        return 2def file_copy(src, dst, flag):    &quot;&quot;&quot;    :param src:         src(including file name)    :param dst:         destination(path name)    :param flag:        f: file                        d: directory    :return:            0: regular                        1: I/O error                        2: exception    &quot;&quot;&quot;    try:        if flag == &#39;f&#39;: # file type            shutil.copy(src, dst)            clog.logger.info(f&#39;File copy done. {src} -&gt; {dst}&#39;)            return 0        else:   # path type            shutil.copytree(src, dst)            clog.logger.info(f&#39;Path copy done. {src} -&gt; {dst}&#39;)    except IOError as e:        errclog.logger.info(f&#39;Unable to copy file. {e}&#39;)   # print(&#39;Unable to copy file. %s&#39; % e)        return 1    except:        errclog.logger.error(f&#39;Unexpected errors. {exc_info()}&#39;)        return 2# def move(n, a, b, c):#     if n == 1:#         print(a + &#39;--&gt;&#39; + c)#     else:#         move(n-1,a, c, b)#         move(1, a, b, c)#         move(n-1, b, a, c)def re_match(list_input, re_pattern):    &quot;&quot;&quot;    :param list_input:  &lt;class: list&gt;    :param re_pattern:  RE    :return:            返回正则匹配结果集 re.compile(re_pattern).findall() &lt;class: list&gt;                        1: exception    &quot;&quot;&quot;    clog.logger.info(f&#39;***RE***&#39;)    try:        p = re.compile(re_pattern)        match_result = [rs for rs in list_input if p.findall(rs)]  # 别忘了，findall返回的是 &lt;class &#39;list&#39;&gt;        clog.logger.info(f&#39;{match_result}&#39;)        return match_result    except Exception as e:        errclog.logger.error(f&#39;{e}&#39;)        return 1class Logger(object):    # 日志级别关系映射，这种定义类的变量x，并使用self.x运用于类方法内，还是第一次见    ---&gt;    level_relations = {        &#39;debug&#39;: logging.DEBUG,        &#39;info&#39;: logging.INFO,        &#39;warning&#39;: logging.WARNING,        &#39;error&#39;: logging.ERROR,        &#39;critical&#39;: logging.CRITICAL    }    def __init__(self, filename, level=&#39;warning&#39;, when=&#39;D&#39;, backup_count=7, interval=1,                 fmt=&#39;%(asctime)s %(module)s.%(funcName)s -&gt; %(threadName)s -&gt; [line:%(lineno)d][%(levelname)s] ⬇⬇⬇\n&#39;                     &#39;%(asctime)s ⬆⬆⬆ [%(levelname)s]: %(message)s&#39;):        &quot;&quot;&quot;        :param filename:        log file name with absolute path        :param level:           log level:  debug/info/warning/error/critical        :param when:            interval unit:                                S - Seconds                                M - Minutes                                H - Hours                                D - Days                                midnight - roll over at midnight                                W{0-6} - roll over on a certain day; 0 - Monday        :param backup_count:    interval backup num        :param interval:        interval value with @param when        :param fmt:             formation        &quot;&quot;&quot;        # 1.最低级别logging对象设置        self.logger = logging.getLogger(filename)        self.format_str = logging.Formatter(fmt, datefmt=&#39;%Y-%m-%d %H:%M:%S&#39;) # 设置日志格式        self.logger.setLevel(self.level_relations.get(level))   # 设置日志级别    &lt;---        # 2.声明不同类型的Handler对象：sh &amp; th        sh = logging.StreamHandler()  # 控制台流式输出        sh.setFormatter(self.format_str)   # 设置屏幕上显示的格式        # logging.handlers.RotatingFileHandler  # 按大小切分        # logging.FileHandler # 文件输出        # 声明间隔时间自动生成文件的对象        th = handlers.TimedRotatingFileHandler(filename=filename,                                               when=when,                   # 间隔的时间单位: S/M/H/D/W/midnight                                               backupCount=backup_count,    # 冗余备份文件数                                               encoding=&#39;utf-8&#39;,                                               interval=interval,           # 时间间隔                                               delay=True                   # Flask线程问题，需开启读写时延                                               )    # 文件写入        th.setFormatter(self.format_str)    # 设置文件写入格式        # 3.从对象处理器内添加        self.logger.addHandler(sh)          # 把对象加到logger里        self.logger.addHandler(th)        # self.logger.removeHandler(sh)    # def test(self):    #     print(self.level_relations)    def err_log_test(self):        print(1 + &#39;_2&#39;)current_path = f&#39;./logs/&#39;clog = Logger(filename=current_path + f&#39;all_bokeh.log&#39;, level=&#39;info&#39;, when=&#39;H&#39;,              fmt=&#39;%(asctime)s %(module)s.%(funcName)s -&gt; %(threadName)s -&gt; [line:%(lineno)d][%(levelname)s]: %(message)s&#39;)errclog = Logger(filename=current_path + f&#39;err_bokeh.log&#39;, level=&#39;error&#39;, when=&#39;H&#39;,                 fmt=&#39;%(asctime)s %(module)s.%(funcName)s -&gt; %(threadName)s -&gt; [line:%(lineno)d][%(levelname)s]: %(message)s&#39;)clog2 = Logger(filename=current_path + f&#39;all_bokeh_section2.log&#39;, level=&#39;info&#39;, when=&#39;H&#39;,              fmt=&#39;%(asctime)s %(module)s.%(funcName)s -&gt; %(threadName)s -&gt; [line:%(lineno)d][%(levelname)s]: %(message)s&#39;)errclog2 = Logger(filename=current_path + f&#39;err_bokeh_section2.log&#39;, level=&#39;error&#39;, when=&#39;H&#39;,                 fmt=&#39;%(asctime)s %(module)s.%(funcName)s -&gt; %(threadName)s -&gt; [line:%(lineno)d][%(levelname)s]: %(message)s&#39;)clog3 = Logger(filename=current_path + f&#39;all_bokeh_section3.log&#39;, level=&#39;info&#39;, when=&#39;H&#39;,              fmt=&#39;%(asctime)s %(module)s.%(funcName)s -&gt; %(threadName)s -&gt; [line:%(lineno)d][%(levelname)s]: %(message)s&#39;)errclog3 = Logger(filename=current_path + f&#39;err_bokeh_section3.log&#39;, level=&#39;error&#39;, when=&#39;H&#39;,                 fmt=&#39;%(asctime)s %(module)s.%(funcName)s -&gt; %(threadName)s -&gt; [line:%(lineno)d][%(levelname)s]: %(message)s&#39;)clog4 = Logger(filename=current_path + f&#39;all_bokeh_section4.log&#39;, level=&#39;info&#39;, when=&#39;H&#39;,              fmt=&#39;%(asctime)s %(module)s.%(funcName)s -&gt; %(threadName)s -&gt; [line:%(lineno)d][%(levelname)s]: %(message)s&#39;)errclog4 = Logger(filename=current_path + f&#39;err_bokeh_section4.log&#39;, level=&#39;error&#39;, when=&#39;H&#39;,                 fmt=&#39;%(asctime)s %(module)s.%(funcName)s -&gt; %(threadName)s -&gt; [line:%(lineno)d][%(levelname)s]: %(message)s&#39;)clog5 = Logger(filename=current_path + f&#39;all_bokeh_section5.log&#39;, level=&#39;info&#39;, when=&#39;H&#39;,              fmt=&#39;%(asctime)s %(module)s.%(funcName)s -&gt; %(threadName)s -&gt; [line:%(lineno)d][%(levelname)s]: %(message)s&#39;)errclog5 = Logger(filename=current_path + f&#39;err_bokeh_section5.log&#39;, level=&#39;error&#39;, when=&#39;H&#39;,                 fmt=&#39;%(asctime)s %(module)s.%(funcName)s -&gt; %(threadName)s -&gt; [line:%(lineno)d][%(levelname)s]: %(message)s&#39;)# clog1_2 = Logger(filename=current_path + f&#39;all_bokeh_section1_2.log&#39;, level=&#39;info&#39;, when=&#39;H&#39;,              fmt=&#39;%(asctime)s %(module)s.%(funcName)s -&gt; %(threadName)s -&gt; [line:%(lineno)d][%(levelname)s]: %(message)s&#39;)errclog1_2 = Logger(filename=current_path + f&#39;err_bokeh_section1_2.log&#39;, level=&#39;error&#39;, when=&#39;H&#39;,                 fmt=&#39;%(asctime)s %(module)s.%(funcName)s -&gt; %(threadName)s -&gt; [line:%(lineno)d][%(levelname)s]: %(message)s&#39;)clog2_2 = Logger(filename=current_path + f&#39;all_bokeh_section2_2.log&#39;, level=&#39;info&#39;, when=&#39;H&#39;,              fmt=&#39;%(asctime)s %(module)s.%(funcName)s -&gt; %(threadName)s -&gt; [line:%(lineno)d][%(levelname)s]: %(message)s&#39;)errclog2_2 = Logger(filename=current_path + f&#39;err_bokeh_section2_2.log&#39;, level=&#39;error&#39;, when=&#39;H&#39;,                 fmt=&#39;%(asctime)s %(module)s.%(funcName)s -&gt; %(threadName)s -&gt; [line:%(lineno)d][%(levelname)s]: %(message)s&#39;)clog3_2 = Logger(filename=current_path + f&#39;all_bokeh_section3_2.log&#39;, level=&#39;info&#39;, when=&#39;H&#39;,              fmt=&#39;%(asctime)s %(module)s.%(funcName)s -&gt; %(threadName)s -&gt; [line:%(lineno)d][%(levelname)s]: %(message)s&#39;)errclog3_2 = Logger(filename=current_path + f&#39;err_bokeh_section3_2.log&#39;, level=&#39;error&#39;, when=&#39;H&#39;,                 fmt=&#39;%(asctime)s %(module)s.%(funcName)s -&gt; %(threadName)s -&gt; [line:%(lineno)d][%(levelname)s]: %(message)s&#39;)if __name__ == &#39;__main__&#39;:    pass</code></pre><hr><h3 id="05-参数配置"><a href="#05-参数配置" class="headerlink" title="05. 参数配置"></a>05. 参数配置</h3><ul><li>kafka_conf.py</li></ul><pre><code class="python"># -*- coding:utf-8 -*-# @Author: Shin# @Date: 20/3/23 15:46# @File: kafka_cfg.py# @Usage: cfg for kafka &amp; MySQL &amp; PostgreSQL# Self Repo# kafka configurationkafka_cfg_dict = {    &#39;bootstrap_servers&#39;: [&quot;10.72.8.30:9092&quot;, &quot;10.72.8.46:9092&quot;, &quot;10.72.8.38:9092&quot;],# &quot;10.72.8.46:9092&quot;,    &#39;topic&#39;: &quot;PREDATA&quot;}kafka_cfg_dict_section2 = {    &#39;bootstrap_servers&#39;: [&quot;10.72.8.30:9092&quot;, &quot;10.72.8.46:9092&quot;, &quot;10.72.8.38:9092&quot;],# &quot;10.72.8.30:9092&quot;,    &#39;topic&#39;: &quot;PREDATA&quot;}kafka_cfg_dict_section3 = {    &#39;bootstrap_servers&#39;: [&quot;10.72.8.30:9092&quot;, &quot;10.72.8.46:9092&quot;, &quot;10.72.8.38:9092&quot;],# &quot;10.72.8.38:9092&quot;,    &#39;topic&#39;: &quot;PREDATA&quot;}#################################################################################################################### postgres configuration# mysql_conn = {#     # oltp_mysql_matser = pymysql.connect(host=&#39;192.168.73.21&#39;, user=&#39;root&#39;, passwd=&#39;123456&#39;, db=&#39;MURA&#39;, port=3306)#     &quot;host&quot;: &#39;192.168.73.21&#39;,#     &quot;user&quot;: &quot;root&quot;,#     &quot;password&quot;: &#39;123456&#39;,#     &quot;database&quot;: &quot;MURA&quot;,#     &quot;port&quot;: 3306# }## mysql_conn_pp = {#     # oltp_mysql_matser = pymysql.connect(host=&#39;192.168.73.21&#39;, user=&#39;root&#39;, passwd=&#39;123456&#39;, db=&#39;MURA&#39;, port=3306)#     &quot;host&quot;: &#39;192.168.62.74&#39;,#     &quot;user&quot;: &quot;root&quot;,#     &quot;password&quot;: &#39;Inspur*()890&#39;,#     &quot;database&quot;: &quot;wrnop&quot;,#     &quot;port&quot;: 5029# }# sqlmap_mysql = {#     &quot;sql1&quot;: &quot;select * from MURA.t_pm_alarm_tst&quot;,#     &quot;sql2&quot;: &quot;select count(1) from MURA.t_pm_alarm_tst&quot;,# }###################################################################################################################SQLMAP_PGSQL_SECTION2 = {    &quot;v_alarm_06_0&quot;: &quot;SELECT * FROM public.m_v_alarm_06 WHERE hash_value = 0&quot;,    &quot;v_alarm_06_1&quot;: &quot;SELECT * FROM public.m_v_alarm_06 WHERE hash_value = 1&quot;,    &quot;v_alarm_06_2&quot;: &quot;SELECT * FROM public.m_v_alarm_06 WHERE hash_value = 2&quot;,    &quot;v_alarm_06_3&quot;: &quot;SELECT * FROM public.m_v_alarm_06 WHERE hash_value = 3&quot;,    &quot;v_alarm_06_4&quot;: &quot;SELECT * FROM public.m_v_alarm_06 WHERE hash_value = 4&quot;,    &quot;v_alarm_06_5&quot;: &quot;SELECT * FROM public.m_v_alarm_06 WHERE hash_value = 5&quot;,    &quot;v_alarm_06_6&quot;: &quot;SELECT * FROM public.m_v_alarm_06 WHERE hash_value = 6&quot;,    &quot;v_alarm_06_7&quot;: &quot;SELECT * FROM public.m_v_alarm_06 WHERE hash_value = 7&quot;,    &quot;v_alarm_06_8&quot;: &quot;SELECT * FROM public.m_v_alarm_06 WHERE hash_value = 8&quot;,    &quot;v_alarm_06_9&quot;: &quot;SELECT * FROM public.m_v_alarm_06 WHERE hash_value = 9&quot;,    #&quot;v_alarm_07_0&quot;: &quot;SELECT * FROM public.v_alarm_07 WHERE hash_value = 0&quot;,    #&quot;v_alarm_07_1&quot;: &quot;SELECT * FROM public.v_alarm_07 WHERE hash_value = 1&quot;,    #&quot;v_alarm_07_2&quot;: &quot;SELECT * FROM public.v_alarm_07 WHERE hash_value = 2&quot;,    #&quot;v_alarm_07_3&quot;: &quot;SELECT * FROM public.v_alarm_07 WHERE hash_value = 3&quot;,    #&quot;v_alarm_07_4&quot;: &quot;SELECT * FROM public.v_alarm_07 WHERE hash_value = 4&quot;,    #&quot;v_alarm_07_5&quot;: &quot;SELECT * FROM public.v_alarm_07 WHERE hash_value = 5&quot;,    #&quot;v_alarm_07_6&quot;: &quot;SELECT * FROM public.v_alarm_07 WHERE hash_value = 6&quot;,    #&quot;v_alarm_07_7&quot;: &quot;SELECT * FROM public.v_alarm_07 WHERE hash_value = 7&quot;,    #&quot;v_alarm_07_8&quot;: &quot;SELECT * FROM public.v_alarm_07 WHERE hash_value = 8&quot;,    #&quot;v_alarm_07_9&quot;: &quot;SELECT * FROM public.v_alarm_07 WHERE hash_value = 9&quot;,}sqlmap_pgsql = {    &quot;v_alarm_13_0&quot;: &quot;SELECT * FROM public.m_v_alarm_13 WHERE hash_value = 0&quot;,    &quot;v_alarm_13_1&quot;: &quot;SELECT * FROM public.m_v_alarm_13 WHERE hash_value = 1&quot;,    &quot;v_alarm_13_2&quot;: &quot;SELECT * FROM public.m_v_alarm_13 WHERE hash_value = 2&quot;,    &quot;v_alarm_13_3&quot;: &quot;SELECT * FROM public.m_v_alarm_13 WHERE hash_value = 3&quot;,    &quot;v_alarm_13_4&quot;: &quot;SELECT * FROM public.m_v_alarm_13 WHERE hash_value = 4&quot;,    &quot;v_alarm_13_5&quot;: &quot;SELECT * FROM public.m_v_alarm_13 WHERE hash_value = 5&quot;,    &quot;v_alarm_13_6&quot;: &quot;SELECT * FROM public.m_v_alarm_13 WHERE hash_value = 6&quot;,    &quot;v_alarm_13_7&quot;: &quot;SELECT * FROM public.m_v_alarm_13 WHERE hash_value = 7&quot;,    &quot;v_alarm_13_8&quot;: &quot;SELECT * FROM public.m_v_alarm_13 WHERE hash_value = 8&quot;,    &quot;v_alarm_13_9&quot;: &quot;SELECT * FROM public.m_v_alarm_13 WHERE hash_value = 9&quot;,    &quot;v_alarm_14_0&quot;: &quot;SELECT * FROM public.m_v_alarm_14 WHERE hash_value = 0&quot;,    &quot;v_alarm_14_1&quot;: &quot;SELECT * FROM public.m_v_alarm_14 WHERE hash_value = 1&quot;,    &quot;v_alarm_14_2&quot;: &quot;SELECT * FROM public.m_v_alarm_14 WHERE hash_value = 2&quot;,    &quot;v_alarm_14_3&quot;: &quot;SELECT * FROM public.m_v_alarm_14 WHERE hash_value = 3&quot;,    &quot;v_alarm_14_4&quot;: &quot;SELECT * FROM public.m_v_alarm_14 WHERE hash_value = 4&quot;,    &quot;v_alarm_14_5&quot;: &quot;SELECT * FROM public.m_v_alarm_14 WHERE hash_value = 5&quot;,    &quot;v_alarm_14_6&quot;: &quot;SELECT * FROM public.m_v_alarm_14 WHERE hash_value = 6&quot;,    &quot;v_alarm_14_7&quot;: &quot;SELECT * FROM public.m_v_alarm_14 WHERE hash_value = 7&quot;,    &quot;v_alarm_14_8&quot;: &quot;SELECT * FROM public.m_v_alarm_14 WHERE hash_value = 8&quot;,    &quot;v_alarm_14_9&quot;: &quot;SELECT * FROM public.m_v_alarm_14 WHERE hash_value = 9&quot;,}sqlmap_pgsql_section3 = {    &quot;v_alarm_12_0&quot;: &quot;SELECT * FROM public.m_v_alarm_12 WHERE hash_value = 0&quot;,    &quot;v_alarm_12_1&quot;: &quot;SELECT * FROM public.m_v_alarm_12 WHERE hash_value = 1&quot;,    &quot;v_alarm_12_2&quot;: &quot;SELECT * FROM public.m_v_alarm_12 WHERE hash_value = 2&quot;,    &quot;v_alarm_12_3&quot;: &quot;SELECT * FROM public.m_v_alarm_12 WHERE hash_value = 3&quot;,    &quot;v_alarm_12_4&quot;: &quot;SELECT * FROM public.m_v_alarm_12 WHERE hash_value = 4&quot;,    &quot;v_alarm_12_5&quot;: &quot;SELECT * FROM public.m_v_alarm_12 WHERE hash_value = 5&quot;,    &quot;v_alarm_12_6&quot;: &quot;SELECT * FROM public.m_v_alarm_12 WHERE hash_value = 6&quot;,    &quot;v_alarm_12_7&quot;: &quot;SELECT * FROM public.m_v_alarm_12 WHERE hash_value = 7&quot;,    &quot;v_alarm_12_8&quot;: &quot;SELECT * FROM public.m_v_alarm_12 WHERE hash_value = 8&quot;,    &quot;v_alarm_12_9&quot;: &quot;SELECT * FROM public.m_v_alarm_12 WHERE hash_value = 9&quot;,    &quot;v_alarm_01_0&quot;: &quot;SELECT * FROM public.m_v_alarm_01 WHERE hash_value = 0&quot;,    &quot;v_alarm_01_1&quot;: &quot;SELECT * FROM public.m_v_alarm_01 WHERE hash_value = 1&quot;,    &quot;v_alarm_01_2&quot;: &quot;SELECT * FROM public.m_v_alarm_01 WHERE hash_value = 2&quot;,    &quot;v_alarm_01_3&quot;: &quot;SELECT * FROM public.m_v_alarm_01 WHERE hash_value = 3&quot;,    &quot;v_alarm_01_4&quot;: &quot;SELECT * FROM public.m_v_alarm_01 WHERE hash_value = 4&quot;,    &quot;v_alarm_01_5&quot;: &quot;SELECT * FROM public.m_v_alarm_01 WHERE hash_value = 5&quot;,    &quot;v_alarm_01_6&quot;: &quot;SELECT * FROM public.m_v_alarm_01 WHERE hash_value = 6&quot;,    &quot;v_alarm_01_7&quot;: &quot;SELECT * FROM public.m_v_alarm_01 WHERE hash_value = 7&quot;,    &quot;v_alarm_01_8&quot;: &quot;SELECT * FROM public.m_v_alarm_01 WHERE hash_value = 8&quot;,    &quot;v_alarm_01_9&quot;: &quot;SELECT * FROM public.m_v_alarm_01 WHERE hash_value = 9&quot;,}sqlmap_pgsql1_2 = {    &quot;v_alarm_04_0&quot;: &quot;SELECT * FROM public.m_v_alarm_04 WHERE hash_value = 0&quot;,    &quot;v_alarm_04_1&quot;: &quot;SELECT * FROM public.m_v_alarm_04 WHERE hash_value = 1&quot;,    &quot;v_alarm_04_2&quot;: &quot;SELECT * FROM public.m_v_alarm_04 WHERE hash_value = 2&quot;,    &quot;v_alarm_04_3&quot;: &quot;SELECT * FROM public.m_v_alarm_04 WHERE hash_value = 3&quot;,    &quot;v_alarm_04_4&quot;: &quot;SELECT * FROM public.m_v_alarm_04 WHERE hash_value = 4&quot;,    &quot;v_alarm_04_5&quot;: &quot;SELECT * FROM public.m_v_alarm_04 WHERE hash_value = 5&quot;,    &quot;v_alarm_04_6&quot;: &quot;SELECT * FROM public.m_v_alarm_04 WHERE hash_value = 6&quot;,    &quot;v_alarm_04_7&quot;: &quot;SELECT * FROM public.m_v_alarm_04 WHERE hash_value = 7&quot;,    &quot;v_alarm_04_8&quot;: &quot;SELECT * FROM public.m_v_alarm_04 WHERE hash_value = 8&quot;,    &quot;v_alarm_04_9&quot;: &quot;SELECT * FROM public.m_v_alarm_04 WHERE hash_value = 9&quot;,    &quot;v_alarm_05_0&quot;: &quot;SELECT * FROM public.m_v_alarm_05 WHERE hash_value = 0&quot;,    &quot;v_alarm_05_1&quot;: &quot;SELECT * FROM public.m_v_alarm_05 WHERE hash_value = 1&quot;,    &quot;v_alarm_05_2&quot;: &quot;SELECT * FROM public.m_v_alarm_05 WHERE hash_value = 2&quot;,    &quot;v_alarm_05_3&quot;: &quot;SELECT * FROM public.m_v_alarm_05 WHERE hash_value = 3&quot;,    &quot;v_alarm_05_4&quot;: &quot;SELECT * FROM public.m_v_alarm_05 WHERE hash_value = 4&quot;,    &quot;v_alarm_05_5&quot;: &quot;SELECT * FROM public.m_v_alarm_05 WHERE hash_value = 5&quot;,    &quot;v_alarm_05_6&quot;: &quot;SELECT * FROM public.m_v_alarm_05 WHERE hash_value = 6&quot;,    &quot;v_alarm_05_7&quot;: &quot;SELECT * FROM public.m_v_alarm_05 WHERE hash_value = 7&quot;,    &quot;v_alarm_05_8&quot;: &quot;SELECT * FROM public.m_v_alarm_05 WHERE hash_value = 8&quot;,    &quot;v_alarm_05_9&quot;: &quot;SELECT * FROM public.m_v_alarm_05 WHERE hash_value = 9&quot;,}sqlmap_pgsql_section2_2 = {    &quot;v_alarm_02_0&quot;: &quot;SELECT * FROM public.m_v_alarm_02 WHERE hash_value = 0&quot;,    &quot;v_alarm_02_1&quot;: &quot;SELECT * FROM public.m_v_alarm_02 WHERE hash_value = 1&quot;,    &quot;v_alarm_02_2&quot;: &quot;SELECT * FROM public.m_v_alarm_02 WHERE hash_value = 2&quot;,    &quot;v_alarm_02_3&quot;: &quot;SELECT * FROM public.m_v_alarm_02 WHERE hash_value = 3&quot;,    &quot;v_alarm_02_4&quot;: &quot;SELECT * FROM public.m_v_alarm_02 WHERE hash_value = 4&quot;,    &quot;v_alarm_02_5&quot;: &quot;SELECT * FROM public.m_v_alarm_02 WHERE hash_value = 5&quot;,    &quot;v_alarm_02_6&quot;: &quot;SELECT * FROM public.m_v_alarm_02 WHERE hash_value = 6&quot;,    &quot;v_alarm_02_7&quot;: &quot;SELECT * FROM public.m_v_alarm_02 WHERE hash_value = 7&quot;,    &quot;v_alarm_02_8&quot;: &quot;SELECT * FROM public.m_v_alarm_02 WHERE hash_value = 8&quot;,    &quot;v_alarm_02_9&quot;: &quot;SELECT * FROM public.m_v_alarm_02 WHERE hash_value = 9&quot;,    &quot;v_alarm_03_0&quot;: &quot;SELECT * FROM public.m_v_alarm_03 WHERE hash_value = 0&quot;,    &quot;v_alarm_03_1&quot;: &quot;SELECT * FROM public.m_v_alarm_03 WHERE hash_value = 1&quot;,    &quot;v_alarm_03_2&quot;: &quot;SELECT * FROM public.m_v_alarm_03 WHERE hash_value = 2&quot;,    &quot;v_alarm_03_3&quot;: &quot;SELECT * FROM public.m_v_alarm_03 WHERE hash_value = 3&quot;,    &quot;v_alarm_03_4&quot;: &quot;SELECT * FROM public.m_v_alarm_03 WHERE hash_value = 4&quot;,    &quot;v_alarm_03_5&quot;: &quot;SELECT * FROM public.m_v_alarm_03 WHERE hash_value = 5&quot;,    &quot;v_alarm_03_6&quot;: &quot;SELECT * FROM public.m_v_alarm_03 WHERE hash_value = 6&quot;,    &quot;v_alarm_03_7&quot;: &quot;SELECT * FROM public.m_v_alarm_03 WHERE hash_value = 7&quot;,    &quot;v_alarm_03_8&quot;: &quot;SELECT * FROM public.m_v_alarm_03 WHERE hash_value = 8&quot;,    &quot;v_alarm_03_9&quot;: &quot;SELECT * FROM public.m_v_alarm_03 WHERE hash_value = 9&quot;,}sqlmap_pgsql_section3_2 = {    &quot;v_alarm_20_0&quot;: &quot;SELECT * FROM public.m_v_alarm_20 WHERE hash_value = 0&quot;,    &quot;v_alarm_20_1&quot;: &quot;SELECT * FROM public.m_v_alarm_20 WHERE hash_value = 1&quot;,    &quot;v_alarm_20_2&quot;: &quot;SELECT * FROM public.m_v_alarm_20 WHERE hash_value = 2&quot;,    &quot;v_alarm_20_3&quot;: &quot;SELECT * FROM public.m_v_alarm_20 WHERE hash_value = 3&quot;,    &quot;v_alarm_20_4&quot;: &quot;SELECT * FROM public.m_v_alarm_20 WHERE hash_value = 4&quot;,    &quot;v_alarm_20_5&quot;: &quot;SELECT * FROM public.m_v_alarm_20 WHERE hash_value = 5&quot;,    &quot;v_alarm_20_6&quot;: &quot;SELECT * FROM public.m_v_alarm_20 WHERE hash_value = 6&quot;,    &quot;v_alarm_20_7&quot;: &quot;SELECT * FROM public.m_v_alarm_20 WHERE hash_value = 7&quot;,    &quot;v_alarm_20_8&quot;: &quot;SELECT * FROM public.m_v_alarm_20 WHERE hash_value = 8&quot;,    &quot;v_alarm_20_9&quot;: &quot;SELECT * FROM public.m_v_alarm_20 WHERE hash_value = 9&quot;,    &quot;v_alarm_21_0&quot;: &quot;SELECT * FROM public.m_v_alarm_21 WHERE hash_value = 0&quot;,    &quot;v_alarm_21_1&quot;: &quot;SELECT * FROM public.m_v_alarm_21 WHERE hash_value = 1&quot;,    &quot;v_alarm_21_2&quot;: &quot;SELECT * FROM public.m_v_alarm_21 WHERE hash_value = 2&quot;,    &quot;v_alarm_21_3&quot;: &quot;SELECT * FROM public.m_v_alarm_21 WHERE hash_value = 3&quot;,    &quot;v_alarm_21_4&quot;: &quot;SELECT * FROM public.m_v_alarm_21 WHERE hash_value = 4&quot;,    &quot;v_alarm_21_5&quot;: &quot;SELECT * FROM public.m_v_alarm_21 WHERE hash_value = 5&quot;,    &quot;v_alarm_21_6&quot;: &quot;SELECT * FROM public.m_v_alarm_21 WHERE hash_value = 6&quot;,    &quot;v_alarm_21_7&quot;: &quot;SELECT * FROM public.m_v_alarm_21 WHERE hash_value = 7&quot;,    &quot;v_alarm_21_8&quot;: &quot;SELECT * FROM public.m_v_alarm_21 WHERE hash_value = 8&quot;,    &quot;v_alarm_21_9&quot;: &quot;SELECT * FROM public.m_v_alarm_21 WHERE hash_value = 9&quot;,    &quot;v_alarm_07_0&quot;: &quot;SELECT * FROM public.m_v_alarm_07 WHERE hash_value = 0&quot;,    &quot;v_alarm_07_1&quot;: &quot;SELECT * FROM public.m_v_alarm_07 WHERE hash_value = 1&quot;,    &quot;v_alarm_07_2&quot;: &quot;SELECT * FROM public.m_v_alarm_07 WHERE hash_value = 2&quot;,    &quot;v_alarm_07_3&quot;: &quot;SELECT * FROM public.m_v_alarm_07 WHERE hash_value = 3&quot;,    &quot;v_alarm_07_4&quot;: &quot;SELECT * FROM public.m_v_alarm_07 WHERE hash_value = 4&quot;,    &quot;v_alarm_07_5&quot;: &quot;SELECT * FROM public.m_v_alarm_07 WHERE hash_value = 5&quot;,    &quot;v_alarm_07_6&quot;: &quot;SELECT * FROM public.m_v_alarm_07 WHERE hash_value = 6&quot;,    &quot;v_alarm_07_7&quot;: &quot;SELECT * FROM public.m_v_alarm_07 WHERE hash_value = 7&quot;,    &quot;v_alarm_07_8&quot;: &quot;SELECT * FROM public.m_v_alarm_07 WHERE hash_value = 8&quot;,    &quot;v_alarm_07_9&quot;: &quot;SELECT * FROM public.m_v_alarm_07 WHERE hash_value = 9&quot;,}# 天级告警:8~11sqlmap_pgsql_section4 = {    #&quot;v_alarm_08_0&quot;: &quot;SELECT * FROM public.m_v_alarm_08 WHERE hash_value = 0&quot;,    #&quot;v_alarm_08_1&quot;: &quot;SELECT * FROM public.m_v_alarm_08 WHERE hash_value = 1&quot;,    #&quot;v_alarm_08_2&quot;: &quot;SELECT * FROM public.m_v_alarm_08 WHERE hash_value = 2&quot;,    #&quot;v_alarm_08_3&quot;: &quot;SELECT * FROM public.m_v_alarm_08 WHERE hash_value = 3&quot;,    #&quot;v_alarm_08_4&quot;: &quot;SELECT * FROM public.m_v_alarm_08 WHERE hash_value = 4&quot;,    #&quot;v_alarm_08_5&quot;: &quot;SELECT * FROM public.m_v_alarm_08 WHERE hash_value = 5&quot;,    #&quot;v_alarm_08_6&quot;: &quot;SELECT * FROM public.m_v_alarm_08 WHERE hash_value = 6&quot;,    #&quot;v_alarm_08_7&quot;: &quot;SELECT * FROM public.m_v_alarm_08 WHERE hash_value = 7&quot;,    #&quot;v_alarm_08_8&quot;: &quot;SELECT * FROM public.m_v_alarm_08 WHERE hash_value = 8&quot;,    #&quot;v_alarm_08_9&quot;: &quot;SELECT * FROM public.m_v_alarm_08 WHERE hash_value = 9&quot;,    #&quot;v_alarm_09_0&quot;: &quot;SELECT * FROM public.m_v_alarm_09 WHERE hash_value = 0&quot;,    #&quot;v_alarm_09_1&quot;: &quot;SELECT * FROM public.m_v_alarm_09 WHERE hash_value = 1&quot;,    #&quot;v_alarm_09_2&quot;: &quot;SELECT * FROM public.m_v_alarm_09 WHERE hash_value = 2&quot;,    #&quot;v_alarm_09_3&quot;: &quot;SELECT * FROM public.m_v_alarm_09 WHERE hash_value = 3&quot;,    #&quot;v_alarm_09_4&quot;: &quot;SELECT * FROM public.m_v_alarm_09 WHERE hash_value = 4&quot;,    #&quot;v_alarm_09_5&quot;: &quot;SELECT * FROM public.m_v_alarm_09 WHERE hash_value = 5&quot;,    #&quot;v_alarm_09_6&quot;: &quot;SELECT * FROM public.m_v_alarm_09 WHERE hash_value = 6&quot;,    #&quot;v_alarm_09_7&quot;: &quot;SELECT * FROM public.m_v_alarm_09 WHERE hash_value = 7&quot;,    #&quot;v_alarm_09_8&quot;: &quot;SELECT * FROM public.m_v_alarm_09 WHERE hash_value = 8&quot;,    #&quot;v_alarm_09_9&quot;: &quot;SELECT * FROM public.m_v_alarm_09 WHERE hash_value = 9&quot;,    #&quot;v_alarm_10_0&quot;: &quot;SELECT * FROM public.m_v_alarm_10 WHERE hash_value = 0&quot;,    #&quot;v_alarm_10_1&quot;: &quot;SELECT * FROM public.m_v_alarm_10 WHERE hash_value = 1&quot;,    #&quot;v_alarm_10_2&quot;: &quot;SELECT * FROM public.m_v_alarm_10 WHERE hash_value = 2&quot;,    #&quot;v_alarm_10_3&quot;: &quot;SELECT * FROM public.m_v_alarm_10 WHERE hash_value = 3&quot;,    #&quot;v_alarm_10_4&quot;: &quot;SELECT * FROM public.m_v_alarm_10 WHERE hash_value = 4&quot;,    #&quot;v_alarm_10_5&quot;: &quot;SELECT * FROM public.m_v_alarm_10 WHERE hash_value = 5&quot;,    #&quot;v_alarm_10_6&quot;: &quot;SELECT * FROM public.m_v_alarm_10 WHERE hash_value = 6&quot;,    #&quot;v_alarm_10_7&quot;: &quot;SELECT * FROM public.m_v_alarm_10 WHERE hash_value = 7&quot;,    #&quot;v_alarm_10_8&quot;: &quot;SELECT * FROM public.m_v_alarm_10 WHERE hash_value = 8&quot;,    #&quot;v_alarm_10_9&quot;: &quot;SELECT * FROM public.m_v_alarm_10 WHERE hash_value = 9&quot;    #&quot;v_alarm_11_0&quot;: &quot;SELECT * FROM public.m_v_alarm_11 WHERE hash_value = 0&quot;,    #&quot;v_alarm_11_1&quot;: &quot;SELECT * FROM public.m_v_alarm_11 WHERE hash_value = 1&quot;,    #&quot;v_alarm_11_2&quot;: &quot;SELECT * FROM public.m_v_alarm_11 WHERE hash_value = 2&quot;,    #&quot;v_alarm_11_3&quot;: &quot;SELECT * FROM public.m_v_alarm_11 WHERE hash_value = 3&quot;,    #&quot;v_alarm_11_4&quot;: &quot;SELECT * FROM public.m_v_alarm_11 WHERE hash_value = 4&quot;,    #&quot;v_alarm_11_5&quot;: &quot;SELECT * FROM public.m_v_alarm_11 WHERE hash_value = 5&quot;,    #&quot;v_alarm_11_6&quot;: &quot;SELECT * FROM public.m_v_alarm_11 WHERE hash_value = 6&quot;,    #&quot;v_alarm_11_7&quot;: &quot;SELECT * FROM public.m_v_alarm_11 WHERE hash_value = 7&quot;,    #&quot;v_alarm_11_8&quot;: &quot;SELECT * FROM public.m_v_alarm_11 WHERE hash_value = 8&quot;,    #&quot;v_alarm_11_9&quot;: &quot;SELECT * FROM public.m_v_alarm_11 WHERE hash_value = 9&quot;,}sqlmap_pgsql_section_check = {    &quot;v_alarm_06_0&quot;: &quot;SELECT * FROM public.m_v_alarm_06 WHERE hash_value = 0&quot;,    &quot;v_alarm_06_1&quot;: &quot;SELECT * FROM public.m_v_alarm_06 WHERE hash_value = 1&quot;,    &quot;v_alarm_06_2&quot;: &quot;SELECT * FROM public.m_v_alarm_06 WHERE hash_value = 2&quot;,    &quot;v_alarm_06_3&quot;: &quot;SELECT * FROM public.m_v_alarm_06 WHERE hash_value = 3&quot;,    &quot;v_alarm_06_4&quot;: &quot;SELECT * FROM public.m_v_alarm_06 WHERE hash_value = 4&quot;,    &quot;v_alarm_06_5&quot;: &quot;SELECT * FROM public.m_v_alarm_06 WHERE hash_value = 5&quot;,    &quot;v_alarm_06_6&quot;: &quot;SELECT * FROM public.m_v_alarm_06 WHERE hash_value = 6&quot;,    &quot;v_alarm_06_7&quot;: &quot;SELECT * FROM public.m_v_alarm_06 WHERE hash_value = 7&quot;,    &quot;v_alarm_06_8&quot;: &quot;SELECT * FROM public.m_v_alarm_06 WHERE hash_value = 8&quot;,    &quot;v_alarm_06_9&quot;: &quot;SELECT * FROM public.m_v_alarm_06 WHERE hash_value = 9&quot;,    #&quot;v_alarm_06_9&quot;: &quot;SELECT * FROM public.m_v_alarm_06&quot;,}# 天级告警:16~19sqlmap_pgsql_section5 = {    #&quot;v_alarm_16_0&quot;: &quot;SELECT * FROM public.m_v_alarm_16 WHERE hash_value = 0&quot;,    #&quot;v_alarm_16_1&quot;: &quot;SELECT * FROM public.m_v_alarm_16 WHERE hash_value = 1&quot;,    #&quot;v_alarm_16_2&quot;: &quot;SELECT * FROM public.m_v_alarm_16 WHERE hash_value = 2&quot;,    #&quot;v_alarm_16_3&quot;: &quot;SELECT * FROM public.m_v_alarm_16 WHERE hash_value = 3&quot;,    #&quot;v_alarm_16_4&quot;: &quot;SELECT * FROM public.m_v_alarm_16 WHERE hash_value = 4&quot;,    #&quot;v_alarm_16_5&quot;: &quot;SELECT * FROM public.m_v_alarm_16 WHERE hash_value = 5&quot;,    #&quot;v_alarm_16_6&quot;: &quot;SELECT * FROM public.m_v_alarm_16 WHERE hash_value = 6&quot;,    #&quot;v_alarm_16_7&quot;: &quot;SELECT * FROM public.m_v_alarm_16 WHERE hash_value = 7&quot;,    #&quot;v_alarm_16_8&quot;: &quot;SELECT * FROM public.m_v_alarm_16 WHERE hash_value = 8&quot;,    #&quot;v_alarm_16_9&quot;: &quot;SELECT * FROM public.m_v_alarm_16 WHERE hash_value = 9&quot;,    #&quot;v_alarm_17_0&quot;: &quot;SELECT * FROM public.m_v_alarm_17 WHERE hash_value = 0&quot;,    #&quot;v_alarm_17_1&quot;: &quot;SELECT * FROM public.m_v_alarm_17 WHERE hash_value = 1&quot;,    #&quot;v_alarm_17_2&quot;: &quot;SELECT * FROM public.m_v_alarm_17 WHERE hash_value = 2&quot;,    #&quot;v_alarm_17_3&quot;: &quot;SELECT * FROM public.m_v_alarm_17 WHERE hash_value = 3&quot;,    #&quot;v_alarm_17_4&quot;: &quot;SELECT * FROM public.m_v_alarm_17 WHERE hash_value = 4&quot;,    #&quot;v_alarm_17_5&quot;: &quot;SELECT * FROM public.m_v_alarm_17 WHERE hash_value = 5&quot;,    #&quot;v_alarm_17_6&quot;: &quot;SELECT * FROM public.m_v_alarm_17 WHERE hash_value = 6&quot;,    #&quot;v_alarm_17_7&quot;: &quot;SELECT * FROM public.m_v_alarm_17 WHERE hash_value = 7&quot;,    #&quot;v_alarm_17_8&quot;: &quot;SELECT * FROM public.m_v_alarm_17 WHERE hash_value = 8&quot;,    #&quot;v_alarm_17_9&quot;: &quot;SELECT * FROM public.m_v_alarm_17 WHERE hash_value = 9&quot;,    #&quot;v_alarm_18_0&quot;: &quot;SELECT * FROM public.m_v_alarm_18 WHERE hash_value = 0&quot;,    #&quot;v_alarm_18_1&quot;: &quot;SELECT * FROM public.m_v_alarm_18 WHERE hash_value = 1&quot;,    #&quot;v_alarm_18_2&quot;: &quot;SELECT * FROM public.m_v_alarm_18 WHERE hash_value = 2&quot;,    #&quot;v_alarm_18_3&quot;: &quot;SELECT * FROM public.m_v_alarm_18 WHERE hash_value = 3&quot;,    #&quot;v_alarm_18_4&quot;: &quot;SELECT * FROM public.m_v_alarm_18 WHERE hash_value = 4&quot;,    #&quot;v_alarm_18_5&quot;: &quot;SELECT * FROM public.m_v_alarm_18 WHERE hash_value = 5&quot;,    #&quot;v_alarm_18_6&quot;: &quot;SELECT * FROM public.m_v_alarm_18 WHERE hash_value = 6&quot;,    #&quot;v_alarm_18_7&quot;: &quot;SELECT * FROM public.m_v_alarm_18 WHERE hash_value = 7&quot;,    #&quot;v_alarm_18_8&quot;: &quot;SELECT * FROM public.m_v_alarm_18 WHERE hash_value = 8&quot;,    #&quot;v_alarm_18_9&quot;: &quot;SELECT * FROM public.m_v_alarm_18 WHERE hash_value = 9&quot;    #&quot;v_alarm_19_0&quot;: &quot;SELECT * FROM public.m_v_alarm_19 WHERE hash_value = 0&quot;,    #&quot;v_alarm_19_1&quot;: &quot;SELECT * FROM public.m_v_alarm_19 WHERE hash_value = 1&quot;,    #&quot;v_alarm_19_2&quot;: &quot;SELECT * FROM public.m_v_alarm_19 WHERE hash_value = 2&quot;,    #&quot;v_alarm_19_3&quot;: &quot;SELECT * FROM public.m_v_alarm_19 WHERE hash_value = 3&quot;,    #&quot;v_alarm_19_4&quot;: &quot;SELECT * FROM public.m_v_alarm_19 WHERE hash_value = 4&quot;,    #&quot;v_alarm_19_5&quot;: &quot;SELECT * FROM public.m_v_alarm_19 WHERE hash_value = 5&quot;,    #&quot;v_alarm_19_6&quot;: &quot;SELECT * FROM public.m_v_alarm_19 WHERE hash_value = 6&quot;,    #&quot;v_alarm_19_7&quot;: &quot;SELECT * FROM public.m_v_alarm_19 WHERE hash_value = 7&quot;,    #&quot;v_alarm_19_8&quot;: &quot;SELECT * FROM public.m_v_alarm_19 WHERE hash_value = 8&quot;,    #&quot;v_alarm_19_9&quot;: &quot;SELECT * FROM public.m_v_alarm_19 WHERE hash_value = 9&quot;,}#pgsql_conn_pp = {#    &quot;host&quot;: &#39;10.73.8.28&#39;,#    &quot;user&quot;: &quot;postgres&quot;,#    &quot;password&quot;: &#39;1qaz)P(O&#39;,#    &quot;database&quot;: &quot;postgres&quot;,#    &quot;port&quot;: 5432#}pgsql_conn_prod = {    &quot;host&quot;: &#39;10.72.8.50&#39;,    &quot;user&quot;: &quot;postgres&quot;,    &quot;password&quot;: &#39;xxxx&#39;,    &quot;database&quot;: &quot;postgres&quot;,    &quot;port&quot;: 5432}# sqlmap_pgsql = {#     &quot;p_cna_hap_maint_user&quot;: &quot;CALL p_cna_hap_maint_user()&quot;,#     # &quot;p_fm_app_exception_detail_info&quot;: &quot;CALL p_fm_app_exception_detail_info()&quot;,#     # &quot;p_fm_server_performence_info&quot;: &quot;CALL p_fm_server_performence_info()&quot;,#     # &quot;p_fm_server_runnning_info&quot;: &quot;CALL p_fm_server_runnning_info()&quot;,#     # &quot;p_fm_system_general_info&quot;: &quot;CALL p_fm_system_general_info()&quot;# }if __name__ == &#39;__main__&#39;:    # partition_lst = [k for k in sqlmap_pgsql_section3_2.keys()]    # print(len(partition_lst))    pass</code></pre><ul><li>main_kafka_section2.py</li></ul><pre><code class="python"># -*- coding:utf-8 -*-# @Author: Shinnosuke# @Date: 20/5/12 16:32# @File: main_kafka_section2.py# @Usage: main_kafka_section2import jsonimport psycopg2 as pgfrom time import ctime, timefrom multiprocessing.dummy import Pool as ThreadPoolfrom tqdm import tqdm# Self Repo# from src.class_Kafka_producer_linux_section2 import *# from src.cfg import kafka_cfg as cfgfrom class_Kafka_producer_linux_section2 import *from cfg import kafka_cfg as cfgfrom func_demo.func_f import date_f# global balancedef lock_f(lock_flag=&#39;N&#39;):    def threading_f(f):        def inner_f(*value):            clog2.logger.warning(f&#39;decorator job begin!&#39;)            if lock_flag == &#39;Y&#39;:                # i += 1                lock = threading.RLock()                with lock:                    # r_lock.acquire()                    clog2.logger.info(f&#39;Thread {threading.current_thread().getName()} is running. Time: {ctime()}&#39;)                    dt_start = int(time())                    result = f(*value)                    # print(type(results))    # &lt;class &#39;tuple&#39;&gt;                    # balance.append(result)                    # r_lock.release()                    clog2.logger.info(f&#39;Thread {threading.current_thread().getName()} end. Time: {ctime()}&#39;)                    clog2.logger.warning(f&#39;job: &lt;{result}&gt; closed. time consume：{int(time()) - dt_start}s.\n&#39;)            else:                clog2.logger.warning(f&#39;decorator job not chosen&#39;)                clog2.logger.info(f&#39;Thread {threading.current_thread().getName()} is running. Time: {ctime()}&#39;)                f(*value)                # balance.append(result)                clog2.logger.info(f&#39;Thread {threading.current_thread().getName()} end. Time: {ctime()}&#39;)            # return balance        return inner_f    return threading_f@lock_f(&#39;Y&#39;)  # RLockdef kafka_job(sql, pg_connect_dict=cfg.pgsql_conn_prod, kafka_cfg_dict=cfg.kafka_cfg_dict_section2):    global oltp_pgsql_master    sql_return = sql    # global cursor    try:        # PostgreSQL结果转json        # step2 PostgreSQL execute        oltp_pgsql_master = pg.connect(            host=pg_connect_dict[&quot;host&quot;], user=pg_connect_dict[&quot;user&quot;],            password=pg_connect_dict[&quot;password&quot;], database=pg_connect_dict[&quot;database&quot;],            port=pg_connect_dict[&quot;port&quot;]        )        # clog2.logger.warning(f&#39;PostgreSQL Begin...&#39;)        cursor = oltp_pgsql_master.cursor()        cursor.execute(sql)        # 可分批，不知是否合理        # rows = cursor2.fetchone()        rowcnt = cursor.rowcount        clog2.logger.info(f&#39;Total rows for current execution &lt;{sql}&gt;: {rowcnt}&#39;)        execute_result = cursor.fetchmany(rowcnt)    # &lt;class &#39;tuple&#39;&gt;        #cursor.close()        #oltp_pgsql_master.close()        # clog2.logger.warning(f&#39;PostgreSQL End: {oltp_pgsql_master}&#39;)        # step3 Kafka json        clog2.logger.warning(f&#39;Kafka Python Begin...&#39;)        prod = KafkaProducerPython(            topic=kafka_cfg_dict[&#39;topic&#39;],            bootstrap_servers=kafka_cfg_dict[&#39;bootstrap_servers&#39;]        )        dict2byte_jsonlist = []     # dict ---&gt; json ---&gt; byte ---&gt; list.append        # i = 0        for rn in execute_result:            pgsql_dict = {}            pgsql_dict[&quot;res_type&quot;] = str(rn[0])            pgsql_dict[&quot;res_key&quot;] = rn[1]            pgsql_dict[&quot;kpi_id&quot;] = rn[2]            pgsql_dict[&quot;time&quot;] = str(rn[3])            pgsql_dict[&quot;value&quot;] = str(rn[4])            pgsql_dict[&quot;vendor&quot;] = rn[5]            pgsql_dict[&quot;specialty&quot;] = rn[6]            pgsql_dict[&quot;province&quot;] = str(rn[7])            pgsql_dict[&quot;city&quot;] = str(rn[8])            # pgsql_dict[&quot;value_1&quot;] = str(rn[9])            # pgsql_dict[&quot;clear&quot;] = str(rn[10])            # dict append            # dict ---&gt; json ---&gt; byte ---&gt; list.append            # json序列化时关闭中文的ascii编码            dict2byte_jsonlist.append(prod.to_bytes(json.dumps([pgsql_dict], ensure_ascii=False)))            # clog2.logger.info(f&#39;sql: {sql}, pgsql_dict: {pgsql_dict}&#39;)            # if i == 0:            #     current_dt = rn[3]            #     i += 1        # clog2.logger.info(f&#39;dt for current round: {current_dt}&#39;)        # clog2.logger.info(f&#39;round time: {}&#39;)        # step4 Kakfa sender        #pbar = tqdm(dict2byte_jsonlist, ncols=200)        #for rb in pbar:        #    prod.producer_send(rb)        #    # pbar.set_description(f&#39;{json.loads(bytes.decode(rb))[0][&quot;kpi_id&quot;]}: {json.loads(bytes.decode(rb))[0][&quot;res_key&quot;]}&#39;)        #    pbar.set_description(f&#39;{sql_return}: {json.loads(bytes.decode(rb))[0][&quot;res_key&quot;]}&#39;)        for rb in dict2byte_jsonlist:            prod.producer_send(rb).add_callback(prod.producer_send_callback)            rj = json.loads(rb)[0]            # print(rj)            tmp = {}            tmp[&quot;res_key&quot;] = rj[&quot;res_key&quot;]            tmp[&quot;kpi_id&quot;] = rj[&quot;kpi_id&quot;]            tmp[&quot;dt&quot;] = rj[&quot;time&quot;]            prod.producer_send_log(tmp)        clog2.logger.warning(f&#39;Kafka Python End...&#39;)        # # dict ---&gt; json ---&gt; bytes        # # Chinese in Consumer need to be decoded  # ××××××        # for rj in mysql_dict_list_2:        #     rj_json2bytes = json.dumps(rj).encode(&#39;utf-8&#39;)        #     prod.producer_send(rj_json2bytes).add_callback(prod.producer_send_callback)        #     print(f&#39;rj: {rj} {type(rj)}\n&#39;)    except Exception as e:        # cursor.close()        errclog2.logger.error(f&#39;{e}&#39;)    finally:    #    # oltp_pgsql_master.close()        return sql_return@lock_f(&#39;Y&#39;)  # RLockdef threading_job():    # threads initial    threads = []    for rv in cfg.SQLMAP_PGSQL_SECTION2.values():        t = MyThread(kafka_job, (rv, cfg.pgsql_conn_prod, cfg.kafka_cfg_dict))        # clog2.logger.info(f&#39;Current sql: {rv}&#39;)        threads.append(t)    # 线程批量启动    # rt = None    for rt in threads:        rt.start()        clog2.logger.warning(f&#39;PostgreSQL Begin: {rt}&#39;)        # rt.join()    for rt in threads:        rt.join()        # clog2.logger.warning(f&#39;PostgreSQL join: {rt}&#39;)if __name__ == &#39;__main__&#39;:    cutline = &#39;--------------------------------------------------------------&#39;    clog2.logger.info(cutline)    # Make the Pool of workers    partition_lst = [v for v in cfg.SQLMAP_PGSQL_SECTION2.values()]    pool = ThreadPool(len(partition_lst))    # print(partition_lst, len(partition_lst))    # Open the urls in their own threads and return the results    results = pool.map(kafka_job, partition_lst)    # close the pool and wait for the work to finish    pool.close()    pool.join()    clog2.logger.info(f&#39;{cutline}\n&#39;)</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>Kafka Python3 PG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20200424_0242_PG搭建（11.2）</title>
    <link href="/2020/07/28/20200311-1238-PG%E6%90%AD%E5%BB%BA%EF%BC%8811-2%EF%BC%89/"/>
    <url>/2020/07/28/20200311-1238-PG%E6%90%AD%E5%BB%BA%EF%BC%8811-2%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h2 id="PG"><a href="#PG" class="headerlink" title="PG"></a>PG</h2><h3 id="01-PG-11-2"><a href="#01-PG-11-2" class="headerlink" title="01. PG 11.2"></a>01. PG 11.2</h3><ul><li>Repo Point<br><a href="https://www.postgresql.org/download/" target="_blank" rel="noopener">Postgresql</a></li></ul><h3 id="02-环境配置"><a href="#02-环境配置" class="headerlink" title="02. 环境配置"></a>02. 环境配置</h3><ul><li>软件源码/几个基本底层组件包&amp;离线环境rpm安装</li></ul><pre><code class="bash">/db/install_pg/postgresql-11.2.tar.gzyum install ncurses* readline* zlib* -y</code></pre><ul><li>源码编译&amp;安装</li></ul><pre><code class="bash"># 归属路径创建mkdir -p /db/pgsqlmkdir -p /db/pgsql/datamkdir -p /db/pgsql/logmkdir -p /db/install_pg# 用户创建（用户编号&lt;1102&gt;视情况而定）useradd -u 1102 -g postgres -d /homr/postgres -c &quot;postgres.&quot; -m -s /bin/bash postgres chown -R postgres:postgres /db/install_pgsu - postgres chmod -R 755 /db/install_pg# 切回root，创建postgres用户密码exitpasswd postgres# 源码编译&amp;安装（中途视编译情况，解决报错，基本上都是缺上面的三类环境包）cd /db/install_pg/tar -xvf postgresql-11.2.tar.gzcd ./postgresql-11.2./configure --prefix=/db/pgsql --with-python# 编译时间会比较久，服务器上速度较快makemake install# 由于安装时需使用root用户，所以后续需要更改PGDATA目录的归属和权限chown -R postgres:postgres /db/pgsql# 切换至postgres用户，初始化数据库su - postgresinitdb /db/pgsql/data# 相关参数配置（PG的初始参数都比较保守，更新log和实例内存即可满足大部分需求）cd /db/pgsql/data# 备份原始参数文件cp pg_hba.conf pg_hba.conf.bak20200318cp postgres.conf postgres.conf.bak20200318cp pg_ident.conf pg_ident.conf.bak20200318cp postgresql.auto.conf postgresql.auto.conf.bak# 修改端口权限/监听/基本实例属性</code></pre><img src="pg_hba.png" srcset="/img/loading.gif"  width="300" title="pg_hba.conf"><img src="监听.png" srcset="/img/loading.gif"  width="300" title="监听所有IP"><img src="memory.png" srcset="/img/loading.gif"  width="300" title="内存配置"><img src="log.png" srcset="/img/loading.gif"  width="300" title="7天循环覆盖"><img src="逻辑备份配置.png" srcset="/img/loading.gif"  width="300" title="主要在使用逻辑增量同步时，需要配置master的这个参数"><img src="时区.png" srcset="/img/loading.gif"  width="300" title="这里未更改，可以修改为'Asia/Shanghai'"><pre><code class="bash"># 服务启停pg_ctl start -l /db/pgsql/log/pg_server.log  # 不指定log路径时，会从配置文件中读取pg_ctl stop -D /db/pgsql/data -m fastpg_ctl -D /db/pgsql/data status# 确认服务是否存在lsof -i:5432netstat -lnutp | grep postgresps -ef | grep postgres# 一般都是在测试环境配置一个debugger# install pldebugger extensiontar zxvf pldebugger.tar.gzmake &amp;&amp; make install # 之前遇到过一次报错，尝试使用下面的参数编译安装USE_PGXS=1 make cleanUSE_PGXS=1 make USE_PGXS=1 make install # Edit your postgresql.conf file, and modify the shared_preload_libraries configshared_preload_libraries = &#39;/db/pgsql/lib/plugin_debugger.so&#39;# 激活psql -c &#39;CREATE EXTENSION pldbgapi&#39;;</code></pre><h3 id="03-基本操作"><a href="#03-基本操作" class="headerlink" title="03. 基本操作"></a>03. 基本操作</h3><ul><li>修改属性</li></ul><pre><code class="sql">ALTER TABLE &lt;old_tbname&gt; RENAME TO &lt;new_tbname&gt;;ALTER TABLE &lt;old_tbname&gt; RENAME &lt;old_colname&gt; TO &lt;new_tbname&gt;;ALTER INDEX IF EXISTS &lt;old_name&gt; RENAME TO &lt;new_index_name&gt;;ALTER TABLE NAME RENAME CONSTRAINT &lt;constraint_name&gt; TO &lt;new_constraint_name&gt;;ALTER TABLE rms.hap_shift_withflag ALTER COLUMN hap_code TYPE varchar(128) USING hap_code::varchar;ALTER TABLE public.t_fm_server_app_status ALTER COLUMN server_status TYPE NUMERIC USING server_status::varchar;ALTER TABLE &lt;tbname&gt; ADD COLUMN x TEXT, ADD COLUMN y TEXT, ...</code></pre><ul><li>约束</li></ul><pre><code class="sql">--主键约束ALTER TABLE XXX ADD PRIMARY KEY (column1, column2);</code></pre><ul><li>分区约束</li></ul><pre><code class="sql">--分区表（11之后支持指定字段分区，省略触发器）DROP TABLE IF EXISTS postgres.t_pm_o_self_monitor_no_partition;DROP TABLE IF EXISTS postgres.t_pm_o_self_monitor;CREATE TABLE postgres.t_pm_o_self_monitor_no_partition(    record_id int4 NULL,    rlt_state varchar NULL,    workerid varchar NULL,    task_dec varchar NULL,    task_collect_msg varchar NULL,    task_type varchar NULL,    ipaddr varchar NULL,    task_id varchar NULL,    task_name varchar NULL,    task_val varchar NULL,    task_collect_time timestamp WITHOUT time ZONE,    task_flag varchar NULL);CREATE TABLE postgres.t_pm_o_self_monitor(    record_id int4 NULL,    rlt_state varchar NULL,    workerid varchar NULL,    task_dec varchar NULL,    task_collect_msg varchar NULL,    task_type varchar NULL,    ipaddr varchar NULL,    task_id varchar NULL,    task_name varchar NULL,    task_val varchar NULL,    task_collect_time timestamp WITHOUT time ZONE,    task_flag varchar NULL) PARTITION BY RANGE (task_collect_time);CREATE INDEX idx_pm_o_self_monitor on postgres.t_pm_o_self_monitor_no_partition using btree (task_collect_time);--这句有问题--CREATE TABLE postgres.t_pm_o_self_monitor_par_his PARTITION OF postgres.t_pm_o_self_monitor FOR VALUES FROM (UNBOUNDED) TO (&#39;2020-01-01&#39;);--默认分区CREATE TABLE postgres.t_pm_o_self_monitor_par_default PARTITION OF postgres.t_pm_o_self_monitor DEFAULT;--默认分区会出现一个问题，若有默认分区时，若新添加的分区范围中的值在默认分区中存在，则该分区不能被创建，如果想让新添加的分区范围中的值在默认分区中存在，需要解绑默认分区，然后添加新分区postgres=# create table part_11_201902 partition of part_11(crt_time) for values from (‘2019-02-01’) to (‘2019-03-01’);ERROR: updated partition constraint for default partition “part_11_default” would be violated by some row--下面开始创建分区和分区索引CREATE TABLE postgres.t_pm_o_self_monitor_par_his PARTITION OF postgres.t_pm_o_self_monitor FOR VALUES FROM (&#39;2019-12-01&#39;) TO (&#39;2020-01-01&#39;);CREATE TABLE postgres.t_pm_o_self_monitor_par_202001 PARTITION OF postgres.t_pm_o_self_monitor FOR VALUES FROM (&#39;2020-01-01&#39;)     TO (&#39;2020-02-01&#39;);CREATE TABLE postgres.t_pm_o_self_monitor_par_202002 PARTITION OF postgres.t_pm_o_self_monitor FOR VALUES FROM (&#39;2020-02-01&#39;)     TO (&#39;2020-03-01&#39;);CREATE INDEX idx_pm_o_self_monitor_par_his on postgres.t_pm_o_self_monitor_par_his using btree (task_collect_time);CREATE INDEX idx_pm_o_self_monitor_par_202001 on postgres.t_pm_o_self_monitor_par_202001 using btree (task_collect_time);CREATE INDEX idx_pm_o_self_monitor_par_202002 on postgres.t_pm_o_self_monitor_par_202002 using btree (task_collect_time);--测试数据SELECT count(1) FROM postgres.t_pm_o_self_monitor_no_partitionSELECT count(1) FROM postgres.t_pm_o_self_monitorINSERT INTO postgres.t_pm_o_self_monitorSELECT --* FROM postgres.postgres.t_pm_o_self_monitor_no_partition round(1*random()) , &#39;正常&#39;, &#39;10.73.8.28&#39;, &#39; &#39;, &#39; &#39;, 0, &#39;10.73.8.28&#39;, &#39;TEST&#39;, &#39;mem&#39;, round(100*random()), generate_series(&#39;2020-04-01&#39;::timestamp, &#39;2022-04-01&#39;::timestamp, &#39;1 minute&#39;), 0; --执行计划比较EXPLAIN ANALYZE SELECT * FROM postgres.postgres.t_pm_o_self_monitor tposmnp  WHERE task_collect_time &gt; &#39;2021-01-02&#39; AND task_collect_time &lt; &#39;2021-01-03&#39;;EXPLAIN ANALYZE SELECT * FROM postgres.postgres.t_pm_o_self_monitor_no_partition tposmnp  WHERE task_collect_time &gt; &#39;2021-01-02&#39; AND task_collect_time &lt; &#39;2021-01-03&#39;;SET max_parallel_workers_per_gather = 4; EXPLAIN ANALYZE SELECT * FROM postgres.postgres.t_pm_o_self_monitor_no_partition tposm WHERE task_name  = &#39;PGX&#39;;SET max_parallel_workers_per_gather = 4; EXPLAIN ANALYZE SELECT * FROM postgres.postgres.t_pm_o_self_monitor_par_202003 tposm WHERE task_name  = &#39;PGX&#39;;--添加分区和上述创建分区过程无异--分区的丢弃--删除分区有两种方法，第一种方法通过 DROP 分区的方式来删除，如下所示DROP TABLE postgres.t_pm_o_self_monitor_par_202101;--DROP 方式直接将分 和分区数据删除，删除前需确认分区数据是否需要备份，避免数据丢失；另 种推荐的方法是解绑分区， 如下所示ALTER TABLE postgres.t_pm_o_self_monitor DETACH PARTITION postgres.t_pm_o_self_monitor_par_202101;--解绑分区只是将分区 父表间 的关系断开 ，分区和分区数据依然保留 ，这种方式比较稳妥，如果后续需要恢复这个分区，通过连接分区方式恢复分区即可，如下所示--连接分区时需要指定分区上的约束ALTER TABLE postgres.t_pm_o_self_monitor ATTACH PARTITION postgres.t_pm_o_self_monitor_par_202101 FOR VALUES FROM (&#39;2021-01-01&#39;) TO (&#39;2021-02-01&#39;);</code></pre><ul><li>外键约束</li></ul><pre><code class="sql">--2.code是3的，关联至3.codeCREATE TABLE public.hr_test_2 (    dt timestamp NULL,    code varchar(100) NULL,    explanation text NULL,    CONSTRAINT hr_test_2_code_fkey FOREIGN KEY (code) REFERENCES hr_test_3(code) ON UPDATE RESTRICT ON DELETE RESTRICT);CREATE TABLE public.hr_test_3 (    dt timestamp NULL,    code varchar(100) NOT NULL,    explanation text NULL,    explanation2 text NULL,    explanation3 text NULL,    CONSTRAINT hr_test_3_pkey PRIMARY KEY (code));</code></pre><ul><li>锁<br>```sql<br>SELECT * FROM PG_STAT_ACTIVITY;<br>SELECT PG_TERMINATE_BACKEND(PID);</li></ul><p>–杀死所有IDLE进程<br>SELECT PG_TERMINATE_BACKEND(PID) FROM PG_STAT_ACTIVITY WHERE STATE = ‘IDLE’;</p><pre><code>* 各种时间戳```sql--时间戳select to_char(clock_timestamp(), &#39;YYYY-MM-DD hh24:MI:SS&#39;);select now()::timestamp(0)without time zone;    --::timestamp(x)    cast as timestamp and keep x bit accuracy--时/分/秒截取，拼接SELECT current_timestamp AS dt,dt||&#39; &#39;||dt_hh AS dt_stamp, --2020-04-01 23dt_hh, dt_miFROM (    SELECT     current_timestamp, --2020-04-01 23:43:52    CAST (current_date AS varchar) AS dt,   --2020-04-01    EXTRACT(HOUR FROM current_timestamp) AS dt_hh,    --23    EXTRACT(MINUTE FROM current_timestamp) AS dt_mi,    --43    TRUNC(EXTRACT(SECOND FROM current_timestamp)) AS dt_si,    --52    CURRENT_DATE + INTERVAL &#39;15 minutes&#39;    --2020-04-01 00:15:00)a--更为简便的时间戳trunc，类似于Oracle.truncSELECT now(),                     --2020-04-21 15:16:06current_date,                    --2020-04-21date_trunc(&#39;minute&#39;, now()),    --2020-04-21 15:16:00date_trunc(&#39;hour&#39;, now()),        --2020-04-21 15:00:00date_trunc(&#39;day&#39;, now()),         --2020-04-21 00:00:00date_trunc(&#39;month&#39;, now()),     --2020-04-01 00:00:00date_trunc(&#39;year&#39;, now()),        --2020-01-01 00:00:00    date_part(&#39;year&#39;, now()),        --2020to_char(current_date, &#39;YYYY-MM-DD HH24:MI:SS&#39;),        --2020-04-21 00:00:00to_char(current_date, &#39;YYYYMMDD&#39;)    --20200421</code></pre><ul><li>正则截取<br>```sql</li></ul><p>–字段正则<br>SELECT v.*, max_partition, SUBSTRING(max_partition, ‘_?([0-9]{6,8})’) FROM v_sys_manage_cfg v</p><pre><code>* 行列转换```sql-- 多列转多行SELECT * FROM rms.cna_hap_site_maintain chsm WHERE hap_code IN (                --数组拆分为多行        SELECT UNNEST(ary) AS ncell_code FROM         (            SELECT ARRAY[&#39;inspur_1&#39;, &#39;inspur_2&#39;, &#39;inspur_3&#39;] AS ary            --由字段拼接为数组            -- SELECT ARRAY[hap_code, vendor_id::varchar, vendor_name] FROM rms.cna_hap_site_maintain chsm         )x)</code></pre><ul><li>注释</li></ul><pre><code class="sql">COMMENT ON COLUMN public.sys_manage_cfg.relkind IS &#39;r = 普通表, i = 索引, p = 分区表, I = 分区索引, v = 视图, m = 物化视图, S = 序列, t = TOAST表, m = 物化视图, c = 组合类型, f = 外部表&#39;COMMENT ON TABLE public.sys_manage_cfg IS &#39;系统管理表&#39;</code></pre><h3 id="04-利用psql备份"><a href="#04-利用psql备份" class="headerlink" title="04. 利用psql备份"></a>04. 利用psql备份</h3><ul><li>恢复postgres初始库基本流程</li></ul><pre><code class="bash"># 1 如果还原对象为初始数据库postgres，那么就需要建立一个新库作为还原的跳板psql -c &quot;DROP DATABASE IF EXISTS INIT&quot;;psql -P&quot;border&quot; -c &quot;CREATE DATABASE init OWNER postgres&quot;;# 2 备份pg_dump -c -C --if-exists -Upostgres -dpostgres -f./postgres.dump# 3 还原前清理数据库连接 &amp; 还原psql -dinit -P&quot;border&quot; -c &quot;SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname =&#39;postgres&#39;&quot;; psql -dinit -f./postgres.dump -L ./log/20200320.log</code></pre><ul><li>一些BUG：实际生产环境还原时，发现运行至上述第4步还原时发现，如果第3步清理连接不干净（多个进程连接未能释放，导致已释放的连接又重新还原），还原会失败，若出现失败，可以多次尝试下列手动操作，临时删除初始数据库后，再还原.<br><code>Tips</code>这种还原的最大不好就是进行了删库再恢复，生产环境频繁使用，不够友好.</li></ul><pre><code class="bash"># 手动还原metedatapsql -dinit -P&quot;border&quot; -c &quot;SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname =&#39;postgres&#39;&quot;; psql -dinit -P&quot;border&quot; -c &quot;DROP DATABASE IF EXISTS postgres&quot;; psql -dinit -P&quot;border&quot; -c &quot;CREATE DATABASE postgres OWNER postgres&quot;;psql -dinit -f ./postgres.dump</code></pre><h3 id="05-利用pg-dump-pg-restore备份"><a href="#05-利用pg-dump-pg-restore备份" class="headerlink" title="05. 利用pg_dump/pg_restore备份"></a>05. 利用pg_dump/pg_restore备份</h3><p><code>Tips</code>pg_dump拥有多种模式，搭配pg_restore可以快速重建数据库，但是-F归档模式的导出，之前有一次不包含索引信息，这部分使用<code>**存疑！！**</code> -&gt; <code>**check**</code><br><code>Tips Check</code>针对上述遗留问题进行了下列测试，未复现上次出现的无非备份索引的问题</p><h4 id="merely-metadata"><a href="#merely-metadata" class="headerlink" title="merely metadata"></a>merely metadata</h4><ul><li>无库重建</li></ul><pre><code class="bash"># -Fd归档路径模式转储，并生成列出归档内容的表格，这个操作的输出能被用作-L选项的输入# pg_dump -Upostgres -dpostgres -C -c --if-exists -s -Fd -j4 -f./postgres_Fd_j4.dump &amp;&amp; pg_restore -l postgres_Fd.dump &gt; postgres.listpg_dump -Upostgres -dpostgres -C -c --if-exists -Fd -j2 -f./postgres_Fd_j2.dump &amp;&amp; pg_restore -l postgres_Fd_j2.dump &gt; postgres.list# 按照归档表格的顺序和指定内容进行恢复，表格内可以进行修改# 选择一个已存在的数据库作为跳板库psql -dinit -P&quot;border&quot; -c &quot;SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname =&#39;postgres&#39;&quot;; pg_restore -dinit -C postgres_Fd.dump# 无库时选择跳板数据库激活psql -dinit -P&quot;border&quot; -c &quot;SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname =&#39;postgres&#39;&quot;; pg_restore -dinit -C -c --if-exists -j4 -L db.list postgres_Fd.dump</code></pre><ul><li>伪增量重建（？），不删库仅重建对象</li></ul><pre><code class="bash"># -Fd归档路径模式转储，并生成列出归档内容的表格，这个操作的输出能被用作-L选项的输入pg_dump -Upostgres -dpostgres -C -c --if-exists -Fd -j4 -f./postgres_Fd_j4.dump &amp;&amp; pg_restore -l postgres_Fd_j4.dump &gt; postgres.list# 按照归档list的顺序和指定内容进行恢复，归档list内的内容可以进行修改# 目标库存在时，选择目标库作为跳板库，在不删除源库的情况下进行对象重建psql -dinit -P&quot;border&quot; -c &quot;SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname =&#39;postgres&#39;&quot;; pg_restore -dpostgres -c --if-exists -j4 -L db.list postgres_Fd.dump</code></pre><ul><li>删库重建</li></ul><pre><code class="bash">pg_dump -Upostgres -dpostgres -C -c --if-exists -s -Fd -j4 -f./postgres_Fd_C.dump &amp;&amp; pg_restore -l postgres_Fd_C.dump &gt; db_C.list# 删除目标库，再选择跳板库重建目标库psql -dinit -P&quot;border&quot; -c &quot;SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname =&#39;postgres&#39;&quot;; pg_restore -dinit -C -c --if-exists postgres_Fd_C.dump# 或psql -dinit -P&quot;border&quot; -c &quot;SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname =&#39;postgres&#39;&quot;; pg_restore -dinit -C -c --if-exists -L db_C.list postgres_Fd_C.dump</code></pre><hr><h4 id="with-data"><a href="#with-data" class="headerlink" title="with data"></a>with data</h4><ul><li>含数据（暂未测试）</li></ul><pre><code class="bash">pg_dump --if-exists -c --create -Fd -j4 -Upostgres -dpostgres -tpublic.t_pm_o_self_monitor -f./t_pm_o_self_monitor_Fd.dump &amp;&amp; pg_restore -l t_pm_o_self_monitor_Fd.dump &gt; db.listpg_restore -dinit -c --if-exists -L db.list t_pm_o_self_monitor_Fd.dump</code></pre><hr><h3 id="06-分区参数优化"><a href="#06-分区参数优化" class="headerlink" title="06. 分区参数优化"></a>06. 分区参数优化</h3><p><code>enable_partitionwise_join</code>如果两个表是兼容的分区，并且连接在分区列上，那么我们可以分别连接每对分区。 即一个大连接被分解成一堆小连接，从而提高了效率。兼容的分区指：相同的分区模式，子分区数目、表定义、分区键字段类型.<br><code>enable_partitionwise_aggregate</code>它允许对为每个分区分别进行分组或聚合。 如果GROUPBY子句不包括分区键，则只能在每个分区的基础上执行部分聚合.</p><p><code>VACUUM</code>垃圾收集并根据需要分析一个数据库/表，不建议FULL参数.<br><code>ANALYZE</code>表分析.</p><pre><code class="SQL">-- VERBOSE：为每个表打印一份详细的清理活动报告-- ANALYZE：更新优化器用以决定最有效执行一个查询的方法的统计信息VACUUM (VERBOSE, ANALYZE) postgres.rms.hap_info;psql -c &quot;VACUUM (VERBOSE, ANALYZE) postgres.rms.hap_info&quot;;--表分析，收集表的统计信息ANALYZE postgres.rms.hap_info;</code></pre><hr><h3 id="07-pgAgent"><a href="#07-pgAgent" class="headerlink" title="07. pgAgent"></a>07. pgAgent</h3><h4 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h4><ul><li>Repo Point (3个必要组件)<br><a href="http://cmake.org/download/" target="_blank" rel="noopener">cmake-3.18.0.tar.gz</a><br><a href="https://github.com/wxWidgets/wxWidgets/releases/tag/v3.1.4" target="_blank" rel="noopener">wxWidgets-3.1.4.tar.bz2</a><br><a href="https://www.pgadmin.org/download/pgagent-source-code/" target="_blank" rel="noopener">pgAgent-4.0.0-Source.tar.gz</a></li></ul><pre><code class="bash"># step 0 Linux环境依赖安装(基于光盘yum安装即可)# wxWidgets组件依赖yum install gtk*    # wxWidgets# pgAgent组件依赖yum install boost*    # pgAgent# step 1 cmakecd cmake-3.18.0./bootstrapgmake &amp;&amp; gmake install# make &amp;&amp; make install# CentOS7下cmake默认不会在/usr/bin下生成软链接，记得手动添加ln -s /usr/local/bin/cmake /usr/bin/cmake</code></pre><img src="pgAgent_cmake.png" srcset="/img/loading.gif"  width="300" title="cmake编译"><img src="pgAgent_cmake_install.png" srcset="/img/loading.gif"  width="300" title="cmake安装，可用make && make install"><pre><code class="bash"># step 2 wxWidgets# root用户私有环境变量修改# ~/.bash_profilePATH=$PATH:$HOME/bin:/usr/local/wxWidgets-xxx/binexport LD_LIBRARY_PATH=/usr/local/wxWidgets-3.1.1/lib:$LD_LIBRARY_PATH# 生效参数文件source .bash_profile# 命令报错时，记得检查Linux是否安装了bzip2，并基于光盘yum安装tar -jxvf wxWidgets-3.1.4.tar.bz2cd wxWidgets-3.1.4# 编译安装 wxWidgets 组件./configure --enable-shared=no --enable-unicode=yes --prefix=/db/pgAgent/wxWidgets-3.1.1make &amp;&amp; make install</code></pre><img src="pgAgent_cmake.png" srcset="/img/loading.gif"  width="300" title="cmake编译"><pre><code class="bash"># stpe 3 pgAgentcd pgAgent-4.0.0-Sourcecmake ./make &amp;&amp; make install</code></pre><img src="pgAgent_complie.png" srcset="/img/loading.gif"  width="300" title="pgAgent编译"><img src="pgAgent_done.png" srcset="/img/loading.gif"  width="300" title="pgAgent安装"><pre><code class="bash"># step 4 切换用户，修改数据共享库，不然会报*.so文件找不到的错误su - postgresvim ~/.bashrc# bashrc# User specific aliases and functionsexport LD_LIBRARY_PATH=/db/pgsql/lib/####### step 5 简单创建PG扩展su - postgresCREATE EXTENSION PGAGENT;</code></pre><img src="pgAgent_extension_create.png" srcset="/img/loading.gif"  width="300" title="pgAgent扩展创建"><pre><code class="bash"># step 6 启动pgagent hostaddr=10.72.8.50 dbname=postgres user=postgres password=&#39;xxxxxx&#39;</code></pre><p><code>Tips</code> CentOS7下cmake默认不会在 <strong>/usr/bin</strong> 下生成cmake的软链接，记得手动添加！！！<br><code>Tips</code> wxWidgets源码包为tar.bz2格式，<strong>tar -jxvf</strong>命令在线解压；命令报错时，记得检查Linux是否安装了<strong>bzip2</strong>，并基于光盘yum安装，<strong>yum -y install bzip2</strong>.</p><hr><h4 id="pgAgent任务配置"><a href="#pgAgent任务配置" class="headerlink" title="pgAgent任务配置"></a>pgAgent任务配置</h4><p><a href="https://www.cnblogs.com/aegis1019/p/9085133.html" target="_blank" rel="noopener">pgAgent Blog</a></p><!-- <img src="pgAgent_complie.png" srcset="/img/loading.gif"  width="300" title="pgAgent编译"> -->]]></content>
    
    
    
    <tags>
      
      <tag>PG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20200519_1545_Tableau</title>
    <link href="/2020/05/19/20200519-Tableau/"/>
    <url>/2020/05/19/20200519-Tableau/</url>
    
    <content type="html"><![CDATA[<p>表格间隔分级：工作表（默认标题/行分级区&amp;标题）—&gt; 行（默认全部取消）<br>地图+条形柱状图</p>]]></content>
    
    
    
    <tags>
      
      <tag>Tableau</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20191224_1414_Hive-1.x配置</title>
    <link href="/2020/04/08/20191219-2254-Hive-1-x/"/>
    <url>/2020/04/08/20191219-2254-Hive-1-x/</url>
    
    <content type="html"><![CDATA[<h2 id="Hive-配置"><a href="#Hive-配置" class="headerlink" title="Hive 配置"></a>Hive 配置</h2><h3 id="01-Hive-1-x"><a href="#01-Hive-1-x" class="headerlink" title="01. Hive 1.x"></a>01. Hive 1.x</h3><ul><li>Repo Point<br><a href="http://mirror.bit.edu.cn/apache/hive/hive-1.2.2/apache-hive-1.2.2-bin.tar.gz" target="_blank" rel="noopener">apache-hive-1.2.2-bin.tar.gz</a><br><a href="https://downloads.mysql.com/archives/get/file/mysql-connector-java-5.1.44.tar.gz" target="_blank" rel="noopener">mysql-connector-java-5.1.44.tar</a></li></ul><h3 id="02-环境配置"><a href="#02-环境配置" class="headerlink" title="02. 环境配置"></a>02. 环境配置</h3><h4 id="2-1-组件"><a href="#2-1-组件" class="headerlink" title="2-1 组件"></a>2-1 组件</h4><p><code>Tips</code> 记得安装JDBC连接器mysql-connector-java-5.1.44-bin.jar，并将Hive和Hadoop侧的jline版本同步！！！</p><pre><code class="bash"># 解压并拷贝MySQL-jdbc的jar包至 $Hive/libs下：cd /usr/local/srctar -zxvf mysql-connector-java-5.1.44.tarcp /usr/local/src/mysql-connector-java-5.1.44/mysql-connector-java-5.1.44-bin.jar $HIVE_HOME/lib/# 进入yarn下的lib查看Hadoop-2.6.1自带的jline版本：ll $HADOOP_HOME/share/hadoop/yarn/lib/*jline*#发现Jjline版本过低：jline-0.9.94.jar# 进入Hive下的lib查看Hive-1.2.2自带的jline版本：ll $HIVE_HOME/lib/*jline*# jline-2.12.jar# 备份Hadoop侧的jline，用Hive侧的jline替代：cd $HADOOP_HOME/share/hadoop/yarn/lib/mv jline-0.9.94.jar jline-0.9.94.jar.oldcp $HIVE_HOME/lib/jline-2.12.jar $HADOOP_HOME/share/hadoop/yarn/lib/# 替换后检查ll $HADOOP_HOME/share/hadoop/yarn/lib/jline*# 这一步针对如下组合时进行：（Hadoop2.7.x &amp;&amp; Hive2.x）# 将Hadoop下的两组配置文件复制至Hive的配置文件目录下cd $HADOOP_HOME/etc/hadoop/cp core-site.xml hdfs-site.xml $HIVE_HOME/conf</code></pre><h4 id="2-2-配置文件修改-amp-相关文件夹创建"><a href="#2-2-配置文件修改-amp-相关文件夹创建" class="headerlink" title="2-2 配置文件修改&amp;相关文件夹创建"></a>2-2 配置文件修改&amp;相关文件夹创建</h4><pre><code class="bash"># 对应创建Hive配置文件中配置的文件夹：mkdir /usr/local/src/apache-hive-1.2.2-bin/warehousemkdir /usr/local/src/apache-hive-1.2.2-bin/tmpmkdir /usr/local/src/apache-hive-1.2.2-bin/log# 用户环境变量，添加Hive路径：vim ~/.bashrc# ~/.bashrc# SET JAVA PATHexport JAVA_HOME=/usr/local/src/jdk1.8.0_201export JRE_HOME=${JAVA_HOME}/jreexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/libexport PATH=${JAVA_HOME}/bin:$PATH# SET HADOOP PATHexport HADOOP_HOME=/usr/local/src/hadoop-2.6.1export PATH=$PATH:$HADOOP_HOME/bin# SET HIVE PATHexport HIVE_HOME=/usr/local/src/apache-hive-1.2.2-binexport PATH=$PATH:$HIVE_HOME/bin# SET SCALA PATHexport SCALA_HOME=/usr/local/src/scala-2.11.8export PATH=$PATH:$SCALA_HOME/bin# SET INI PATHexport INI_PATH=/usr/local/src# SET SPARK PATHexport SPARK_HOME=/usr/local/src/spark-2.0.2-bin-hadoop2.6export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin####### 生效source ~/.bashrc# Hive log位置配置mv hive-log4j.properties.template hive-log4j.propertie# Hive exec-log位置配置(貌似没有任务执行日志生成？？？)mv hive-exec-log4j.properties.template hive-exec-log4j.properties# 添加JDK/Hadoop/Hive家目录/Hive配置文件路径 cp hive-env.sh.template hive-env.sh# hive-env.sh# Folder containing extra ibraries required for hive compilation/execution can be controlled by:export JAVA_HOME=/usr/local/src/jdk1.8.0_201export HADOOP_HOME=/usr/local/src/hadoop-2.6.1export HIVE_HOME=/usr/local/src/apache-hive-1.2.2-binexport HIVE_CONF_DIR=/usr/local/src/apache-hive-1.2.2-bin/conf######</code></pre><h4 id="2-3-Hive配置的文件需要手动新建，修改MySQL连接信息-amp-更新多组文件夹路径"><a href="#2-3-Hive配置的文件需要手动新建，修改MySQL连接信息-amp-更新多组文件夹路径" class="headerlink" title="2-3 Hive配置的文件需要手动新建，修改MySQL连接信息&amp;更新多组文件夹路径"></a>2-3 Hive配置的文件需要手动新建，修改MySQL连接信息&amp;更新多组文件夹路径</h4><pre><code class="bash">touch hive-site.xml # hive-site.xml# 目前仅配置这么多，还有其他优化类的配置尚未添加# javax.jdo.option.ConnectionURL: Hive使用MySQL作为 Metadata 存储时(MySql为5.7.12版本)，需要在连接串中指定是否采用SSL连接Tips：mysql -V    java -version&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;                &lt;value&gt;jdbc:mysql://master:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt;                &lt;description&gt;Warning disabled.&lt;/description&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;                &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;                &lt;value&gt;root&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;                &lt;value&gt;123456&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;                &lt;value&gt;/usr/local/src/apache-hive-1.2.2-bin/warehouse&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;hive.exec.scratchdir&lt;/name&gt;                &lt;value&gt;/usr/local/src/apache-hive-1.2.2-bin/tmp&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;hive.querylog.location&lt;/name&gt;                &lt;value&gt;/usr/local/src/apache-hive-1.2.2-bin/log&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;hive.cli.print.header&lt;/name&gt;                &lt;value&gt;true&lt;/value&gt;                &lt;description&gt;Whether to print the names of the columns in query output.&lt;/description&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;hive.cli.print.current.db&lt;/name&gt;                &lt;value&gt;true&lt;/value&gt;                &lt;description&gt;Whether to include the current database in the Hive prompt.&lt;/description&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;hive.metastore.uris&lt;/name&gt;                &lt;value&gt;thrift://master:9083&lt;/value&gt;                &lt;description&gt;Path for Hive metadata.&lt;/description&gt;        &lt;/property&gt;&lt;/configuration&gt;</code></pre><h4 id="2-4-Hive-Log配置"><a href="#2-4-Hive-Log配置" class="headerlink" title="2-4 Hive Log配置"></a>2-4 Hive Log配置</h4><ul><li>Hive中的日志分为两种:<br><code>系统日志</code>记录了hive的运行情况，错误状况，在<code>hive-log4j.properties</code>文件中记录了Hive日志的存储情况.<br><code>Job日志</code>记录了Hive 中job的执行的历史过程.<br><code>Tips</code>进入到用户的主目录，使用命令 cat /root/.hivehistory 可以查看到Hive执行的历史命令.</li></ul><pre><code class="bash">vim /usr/local/src/apache-hive-1.2.2-bin/conf/hive-log4j.properties# hive-log4j.properties 修改相关路径# 默认的存储情况：hive.root.logger=WARN,DRFAhive.log.dir=/tmp/${user.name} # 默认的存储位置hive.log.file=hive.log  # 默认的文件名# 修改# hive.log.dir=${java.io.tmpdir}/${user.name}# SET Hive log pathhive.root.logger=WARN,DRFAhive.log.dir=/usr/local/src/apache-hive-1.2.2-bin/loghive.log.file=hive.log######</code></pre><h4 id="2-5-Hive适配的MySQL元数据库初始化"><a href="#2-5-Hive适配的MySQL元数据库初始化" class="headerlink" title="2-5 Hive适配的MySQL元数据库初始化"></a>2-5 Hive适配的MySQL元数据库初始化</h4><p><code>Tips</code>这里和之前安装Hive1.x有些区别，当使用的Hive版本 &lt; 2.x 时，不做初始化也是OK的，Hive1.x系列第一次启动的时候会自动进行初始化，只不过不会一开始就生成足够多的元数据库中的表，而是在使用过程中慢慢生成，并在最后进行初始化；当使用的Hive版本 &gt;= 2.x 时，必须手动初始化元数据库.</p><pre><code class="bash">schematool -dbType mysql -initSchema</code></pre><img src="Hive2.x初始化.png" srcset="/img/loading.gif" title="Hive2.x初始化."><h4 id="2-6-Beeline连接Hive"><a href="#2-6-Beeline连接Hive" class="headerlink" title="2-6 Beeline连接Hive"></a>2-6 Beeline连接Hive</h4><ul><li>这里使用的是<code>Hive Metastore ➡ hiveserver2 ➡ beeline</code>的连接方式. </li><li>也可以使用<code>Hive Metastore ➡ hive（CLI）</code>的连接方式.</li><li>无Metastore的情况下，纯<code>hive --service cli</code>启动，每启动一个Hive窗口，便会加载一个Hive Metastore服务进程，资源占用上升，不推荐.</li></ul><pre><code class="bash"># hive-site.xml中配置了hive.metastore.uris后，无论是HS2或是Hive CLI开启前，都需开启Metastorehive --service metastore  1&gt;/dev/null  2&gt;&amp;1  &amp; # 正常前台打印重定向进trash，错误重定向至管道1，同样进trash，metastore开启9083端口# 启动hiveserver2服务，beeline方式连接Hive，默认端口为10000hive --service hiveserver2 &amp;# beeline# !connect jdbc:hive2://master:10000 root 123456beeline -u jdbc:hive2://master:10000 -n root -p 123456 --color=true</code></pre><img src="metastore&hiveserver2&beeline.png" srcset="/img/loading.gif" title="metastore+hs2+beeline."><p><code>Tips</code> <strong>退出当前jdbc:</strong> !close    <strong>退出Beeline:</strong> !q  <strong>查询表:</strong>!tables</p><h4 id="2-7-Hive的其它连接方式"><a href="#2-7-Hive的其它连接方式" class="headerlink" title="2-7 Hive的其它连接方式"></a>2-7 Hive的其它连接方式</h4><p><code>HUE</code>WebUI<br><code>DataGrip</code>JetBrain旗下的HQL调试工具<br><code>DBeaver</code>用于调试HQL的测试工具</p><ul><li>使用DBeaver连接Hive，主要呈现驱动的选择，本例使用的Hive版本为1.2.2，DBeaver版本为CE-7.0.1.</li></ul><img src="DBeaver.png" srcset="/img/loading.gif" title="DBeaver驱动选择.">]]></content>
    
    
    
    <tags>
      
      <tag>Hive</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20200203_2323_Python3-Pandas（持续更新）</title>
    <link href="/2020/03/30/20200201-1150-Python3-Pandas/"/>
    <url>/2020/03/30/20200201-1150-Python3-Pandas/</url>
    
    <content type="html"><![CDATA[<h2 id="Pandas-VS-SQL"><a href="#Pandas-VS-SQL" class="headerlink" title="Pandas VS SQL"></a>Pandas VS SQL</h2><h3 id="数据处理Pandas常用模块"><a href="#数据处理Pandas常用模块" class="headerlink" title="数据处理Pandas常用模块"></a>数据处理Pandas常用模块</h3><ul><li><code>Repo Point</code><br>  pandas<br>  numpy</li></ul><hr><h4 id="用法比较"><a href="#用法比较" class="headerlink" title="用法比较"></a>用法比较</h4><h5 id="1-选择"><a href="#1-选择" class="headerlink" title="1. 选择"></a>1. 选择</h5><p><code>SQL</code></p><pre><code class="sql">select * from xxx;</code></pre><p><code>Pandas</code></p><pre><code class="python">data_path = rf&#39;./&#39;file_name = f&#39;20200115_86_NSN_CM.csv&#39;order_data = pd.read_csv(data_path + file_name, encoding=&#39;GBK&#39;)print(order_data.head(10))</code></pre><h5 id="2-查询特定列的数据"><a href="#2-查询特定列的数据" class="headerlink" title="2. 查询特定列的数据"></a>2. 查询特定列的数据</h5><p><code>SQL</code></p><pre><code class="sql">select flag from xxx;</code></pre><p><code>Pandas</code></p><pre><code class="python"># 2.1 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;print(type(order_data[[&#39;flag&#39;, &#39;86_NSN_CM&#39;]]))print(order_data[[&#39;flag&#39;, &#39;86_NSN_CM&#39;]], &#39;\n&#39;)print(type(order_data[[&#39;86_NSN_CM&#39;]]))print(order_data[[&#39;86_NSN_CM&#39;]])# 2.2 &lt;class &#39;pandas.core.series.Series&#39;&gt;print(type(order_data[&#39;86_NSN_CM&#39;]))print(order_data[&#39;86_NSN_CM&#39;], &#39;\n&#39;)# 3 loc: 按字段定位数据print(type(order_data.loc[:, [&#39;flag&#39;]]))print(order_data.loc[:, [&#39;flag&#39;]])# 4 iloc: 按索引号索引数据，自0开始print(type(order_data.iloc[:, [1]]))print(order_data.iloc[:, [1]])  # 先行再列# 5 综合# 列的增删改查：df[&quot;f&quot;] = [1,2,3,4,5]  当f不存在del df[&quot;f&quot;]    df.pop(&quot;e&quot;)    df.drop(&quot;e&quot;)df[&quot;f&quot;] = [1,2,3,4,5]  当f存在df[&quot;a&quot;]    df[[&quot;a&quot;]]  df[[&quot;a&quot;, &quot;b&quot;]]   df.loc[:, &quot;a&quot;]#行的增删改查df.append(line)df.drop([&quot;h&quot;, &quot;j&quot;])df.loc[&quot;k&quot;] = pd.Series([1,1,1,1,1], index=list(&quot;abcde&quot;))df.loc[&quot;a&quot;]   df.iloc[0]#    行列混合操作# 先列再行：df[&quot;i&quot;].loc[&quot;a&quot;]# 先行再列：df.loc[&quot;a&quot;][&quot;i&quot;]# 多行多列：df[[&quot;i&quot;, &quot;o&quot;, &quot;p&quot;]].loc[&quot;b&quot;:&quot;d&quot;]# 获取多行：df[&quot;a&quot;:&quot;c&quot;]  df[0:2]   df[[True, False, False]]# 获取多列：df[[&quot;a&quot;, &quot;b&quot;]]# 复杂的混合操作：df[df[&quot;i&quot;] &gt; 0.5] # 把所有的i列的大于0.5的行，都拿出来！</code></pre><p><code>Tips</code>inplace不管在那个地方都一样，但是axis也常见，但是意义可能不一样！<br><code>Tips</code><strong>直接df[x]先取列，df.loc/df.iloc先取行</strong></p><h5 id="3-查询特定列去重后的数据"><a href="#3-查询特定列去重后的数据" class="headerlink" title="3. 查询特定列去重后的数据"></a>3. 查询特定列去重后的数据</h5><p><code>SQL</code></p><pre><code class="sql">select column1, column2 from xxx group by column1, column2;select distinct column1, column2 from xxx;</code></pre><p><code>Pandas</code></p><pre><code class="python">print(type(order_data[&#39;flag&#39;].unique()))print(order_data[&#39;flag&#39;].unique(), &#39;\n&#39;)</code></pre><h5 id="4-单一条件查询特定数据"><a href="#4-单一条件查询特定数据" class="headerlink" title="4. 单一条件查询特定数据"></a>4. 单一条件查询特定数据</h5><p><code>SQL</code></p><pre><code class="sql">select * from xxx x where x.column = xxx;</code></pre><p><code>Pandas</code></p><pre><code class="python"># print(type(order_data[order_data[&#39;86_NSN_CM&#39;] == &#39;4G_output_20200115.xml.gz&#39;]))print(f&#39;结果1：\n{order_data[order_data[&quot;86_NSN_CM&quot;] == &quot;4G_output_20200115.xml.gz&quot;]}&#39;)print(f&#39;结果2：\n{order_data[order_data.iloc[:, [1]] == &quot;4G_output_20200115.xml.gz&quot;]} \n&#39;)结果1：     flag                  86_NSN_CM0  NSN_CM  4G_output_20200115.xml.gz结果2：  flag                  86_NSN_CM0  NaN  4G_output_20200115.xml.gz1  NaN                        NaN</code></pre><h5 id="5-多条件查询特定数据"><a href="#5-多条件查询特定数据" class="headerlink" title="5. 多条件查询特定数据"></a>5. 多条件查询特定数据</h5><p><code>SQL</code></p><pre><code class="sql">select * from xxx x where x.column1 = xxx and x.column2 = yyy;</code></pre><p><code>Pandas</code></p><pre><code class="python"># &amp;# print(f&#39;结果1：\n{type(order_data[(order_data[&quot;86_NSN_CM&quot;] == &quot;4G_output_20200115.xml.gz&quot;) &amp; (order_data[&quot;flag&quot;] == &quot;NSN_CM&quot;) ])}&#39;)print(f&#39;结果 &amp;：\n{order_data[(order_data[&quot;86_NSN_CM&quot;] == &quot;4G_output_20200115.xml.gz&quot;) &amp; (order_data[&quot;flag&quot;] == &quot;NSN_CM&quot;) ]}&#39;)# |print(f&#39;结果 |：\n{order_data[(order_data[&quot;flag&quot;] == &quot;NSN_CM1&quot;) | (order_data[&quot;flag&quot;] == &quot;NSN_CM&quot;) ]}&#39;)# SQL模式查询print(order_data.query(&#39;flag==&quot;NSN_CM&quot;&#39;))  # 按照类似sql的规则筛选数据</code></pre><h5 id="6-数据连接"><a href="#6-数据连接" class="headerlink" title="6. 数据连接"></a>6. 数据连接</h5><p><code>SQL</code></p><pre><code class="sql">select a.* from a left join b on a.id = b.id;</code></pre><p><code>Pandas</code></p><pre><code class="python">df1 = pd.DataFrame([[1, 2, 3], [3, 4, 5], [7, 8, 9]])df2 = pd.DataFrame([[1, 2, 4], [3, 4, 6], [10, 11, 12]], columns=[0, 1, 3])# display(df1, df2)df3 = df1.merge(df2)print(df3)</code></pre><h5 id="7-分组聚合"><a href="#7-分组聚合" class="headerlink" title="7. 分组聚合"></a>7. 分组聚合</h5><pre><code class="SQL">SELECT Column1, Column2, mean(Column3), sum(Column4)FROM SomeTableWHERE Condition 1GROUP BY Column1, Column2HAVING Condition2</code></pre><pre><code class="python">df[Condition1].groupby([Column1, Column2], as_index=False).agg({Column3: &quot;mean&quot;, Column4:&quot;sum&quot;}).filter(Condition2)</code></pre><h4 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h4><h5 id="8-列-lt-—-gt-行索引"><a href="#8-列-lt-—-gt-行索引" class="headerlink" title="8. 列&lt;—&gt;行索引"></a>8. 列&lt;—&gt;行索引</h5><pre><code class="python">df3_mutiidx = pd.DataFrame(np.random.randint(100,1000,size=(4,4)),                   index=[[&quot;上半年&quot;,&quot;上半年&quot;,&quot;下半年&quot;,&quot;下半年&quot;], [&quot;第一季度&quot;,&quot;第二季度&quot;,&quot;第三季度&quot;, &quot;第四季度&quot;]],                  columns=[&quot;芯片&quot;, &quot;药品&quot;, &quot;粮食&quot;, &quot;石油&quot;])# display(df3_mutiidx, df3_mutiidx.index, df3_mutiidx.loc[&quot;上半年&quot;])df3_mutiidx[&quot;idx&quot;] = df3_mutiidx.indexdf3_mutiidx[&quot;idx2&quot;] = df3_mutiidx.index.get_level_values(level=0)display(df3_mutiidx, df3_mutiidx)# 1/2级索引交换df3_mutiidx.reset_index(level=0, inplace=True)df3_mutiidx# df3_mutiidx.info()</code></pre><h5 id="9-类型变更"><a href="#9-类型变更" class="headerlink" title="9. 类型变更"></a>9. 类型变更</h5><pre><code class="python">df[&quot;x&quot;] = df[&quot;x&quot;].astype(&quot;int64&quot;)</code></pre><h5 id="10-字段增删"><a href="#10-字段增删" class="headerlink" title="10. 字段增删"></a>10. 字段增删</h5><pre><code class="python">df1= df.drop([&quot;test&quot;], axis=1, inplace=False)</code></pre><hr><h4 id="numpy"><a href="#numpy" class="headerlink" title="numpy"></a>numpy</h4><h5 id="广播运算"><a href="#广播运算" class="headerlink" title="广播运算"></a>广播运算</h5><p><code>Tips</code>不同维度，低维度的列数必须与高纬度的相同，且高维度的行数必须是低维度的整数倍. (2,3,4) — (4)/(3,4)/(2,3,4) 或(2,3,4) — (3,1)</p><pre><code class="python"># 原生Python处理数组list22 = list(range(10))print(list22)# 把这个数据序列 0-9 的每个元素都进行 + 1 操作result_list = []for i in list22:    result_list.append(i + 1)print(result_list)# numpy处理数组（广播机制）data = np.arange(10)  # &lt;class &#39;numpy.ndarray&#39;&gt;data += 1    # [1,2,3,4] + 1 =&gt; [1,2,3,4] + [1,1,1,1] = [2,3,4,5]display(data)  </code></pre><h5 id="reshape-VS-数组扁平化"><a href="#reshape-VS-数组扁平化" class="headerlink" title="reshape VS 数组扁平化"></a>reshape VS 数组扁平化</h5><p><code>reshape</code>维度重构</p><pre><code class="python">a = np.arange(1, 13).reshape((2, 3, 4)) # 2堆3行4列 = 三维，会被看作2个两维的，即2个3行4列b = [10, 20, 30, 40] # 1行1列的b，会被广播为 1个3行4列，再被扩展为2个3行4列display(a + b)a0 = np.arange(1, 10)a1 = a0.reshape((2, -1))  # 用 -1 来自动确定列数a2 = np.reshape(a0, (2, -1))</code></pre><p><code>扁平化</code>高维变低维. [[1,2], [3,4]] — [1,2,3,4]<br><code>ravel</code>返回的是视图，数据引用，影响原值.<br><code>flttern</code>返回的是新的<strong>浅拷贝</strong>，不影响原值.</p><pre><code class="python">x = np.arange(1, 13).reshape((3,4), order=&quot;C&quot;)  # C风格 左到右，上到下y = np.arange(1, 13).reshape((3,4), order=&quot;F&quot;)  # F风格 上到下，左到右# raveldisplay(x)display(y)z1 = x.ravel()z2 = y.ravel()z1[0] = 111display(z1, z2)# flattenm1 = x.flatten()m2 = y.flatten()m1[1] = 222display(m1, x)</code></pre><h5 id="索引和切片"><a href="#索引和切片" class="headerlink" title="索引和切片"></a>索引和切片</h5><pre><code class="python">x = np.arange(10)display(x)# 索引display(x[1]) # 除了基础索引语法外，还支持更多语法的索引display(x[[1,2,3,4]]) # 取1~4display(x[[True, True, True, True, False, False, False, False, False, False]]# 切片display(x[3:7:2]) # 和原生Python切片用法一样# 大综合a = np.arange(1, 10)print(a)  # 1 2 3 4 5 6 7 8 9print(a[:3])  # 1 2 3print(a[3:6])   # 4 5 6print(a[6:])  # 7 8 9print(a[::-1])  # 9 8 7 6 5 4 3 2 1print(a[:-4:-1])  # 9 8 7print(a[-4:-7:-1])  # 6 5 4print(a[-7::-1])  # 3 2 1print(a[::])  # 1 2 3 4 5 6 7 8 9print(a[:])  # 1 2 3 4 5 6 7 8 9print(a[::3])  # 1 4 7print(a[1::3])  # 2 5 8print(a[2::3])  # 3 6 9</code></pre><h5 id="属性操作"><a href="#属性操作" class="headerlink" title="属性操作"></a>属性操作</h5><p>ndim - 维度数<br>shape - 维度/形状<br>dtype - 元数数据类型<br>size - 元素个数<br>itemsize - 元素长度、字节数<br>nbytes - 总字节数 = size x itemsize<br>real - 复数数组的实部数组<br>imag - 复数数组的虚部数组<br>T - 数组对象的转置视图<br>flat - 扁平迭代器</p><pre><code class="python"># 构建一个数组x = np.array([1.5, 2.3, 3.4, -1.2, -1.8, -1.5])display(x.dtype)display(x)# 进行类型转换y = x.astype(np.int64)display(y)# 直接修改dtype属性，尝试修改数组类型.  不建议 ××××××x.dtype = np.int32display(x)</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>Python3</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20200104_0956-Oracle12cR2（持续更新）</title>
    <link href="/2020/03/20/20191221-1006-Oracle12cR2/"/>
    <url>/2020/03/20/20191221-1006-Oracle12cR2/</url>
    
    <content type="html"><![CDATA[<h2 id="OCP12c"><a href="#OCP12c" class="headerlink" title="OCP12c"></a>OCP12c</h2><h3 id="01-Oracle12cR2"><a href="#01-Oracle12cR2" class="headerlink" title="01. Oracle12cR2"></a>01. Oracle12cR2</h3><ul><li>Repo Point<br><a href="https://www.oracle.com/downloads/software-license-agreement.html#license-lightbox" target="_blank" rel="noopener">&lt;Oracle-12.2&gt;linuxx64_12201_database.zip</a><img src="640.gif" srcset="/img/loading.gif" width="300" ></li></ul><p><code>Tips</code>以下水印截图均转自方力扬老师，仅供学习，特此声明.</p><hr><h3 id="02-runInstaller"><a href="#02-runInstaller" class="headerlink" title="02. ./runInstaller"></a>02. ./runInstaller</h3><p><code>To be continued...</code></p><ul><li>先单独点出一个本地图形化的调用，困扰了好久，按下列指令完成后，会在本地弹出图形化界面，前提是本地的ssh软件需要开启<code>Xserver服务</code>!!!</li></ul><pre><code class="bash">[root@ocp12c ~]# xhost +access control disabled, clients can connect from any HOST   ⬅ 允许客户端访问主机IP的Xserver[root@ocp12c ~]# su - oracleLast login: Sat Jan 18 00:32:43 CST 2020 on pts/3[oracle@ocp12c ~]$ export DISPLAY=192.168.17.1:0.0[oracle@ocp12c ~]$ cd /soft/database[oracle@ocp12c ~]$ ./runInstallerStarting Oracle Universal Installer...Checking Temp space: must be greater than 500 MB.   Actual 146736 MB    PassedChecking swap space: must be greater than 150 MB.   Actual 7079 MB    PassedChecking monitor: must be configured to display at least 256 colors.    Actual 16777216    PassedPreparing to launch Oracle Universal Installer from /tmp/OraInstall2020-01-18_12-37-03AM. Please wait ...</code></pre><hr><h3 id="03-Oracle12c-架构"><a href="#03-Oracle12c-架构" class="headerlink" title="03. Oracle12c 架构"></a>03. Oracle12c 架构</h3><h4 id="实例和内存结构"><a href="#实例和内存结构" class="headerlink" title="实例和内存结构"></a>实例和内存结构</h4><p><code>Instance</code> 实例其实就是物理内存段的一部分，相当于软件进程.<br><code>kernel.shmall</code> 为共享内存大小，按照192.168.62.103测试库来看，可以配置为 <strong>4294967296</strong> 个page（4KB/page）.<br><code>kernel.shmmax</code> 用于定义单个共享内存段的最大值，<code>kernel.shmmax</code> 设置应该足够大，能在一个共享内存段下容纳下整个的 <code>SGA</code> ，设置的过低可能会导致需要创建多个共享内存段，这样可能导致系统性能的下降，最大值为16GB（在大多数情况下，该值应该比SGA大）.按照192.168.62.103测试库（内存128G）来看，可以配置为 <strong>88719476736</strong>（82GB）.<br>53477376 pages / 219043332096 bytes</p><p><code>Ex</code> 内核参数，系统内存4G，下面为对应内核配置大小（出自官方白皮书）<br><img src="Linux内核参数.png" srcset="/img/loading.gif" title="Linux内核参数" alt="Linux内核参数"></p><p>实例内存由下面<strong>两组内存参数</strong>动态调整：<br><code>SGA</code>（total_mem × 80%） × 80%，一般不超过物理内存的1/2.<br><code>PGA</code>（total_mem × 80%） × 20%.</p><table><thead><tr><th align="center">SGA参数</th><th align="center">×</th><th align="center">MEM参数</th></tr></thead><tbody><tr><td align="center">×</td><td align="center">×</td><td align="center"><code>MEMORY_MAX_TARGET</code>（静态）⬇</td></tr><tr><td align="center">x</td><td align="center"><code>SGA_MAX_SIZE</code></td><td align="center"><code>MEMORY_TARGET</code>（动态）⬇</td></tr><tr><td align="center">×</td><td align="center"><code>SGA_TARGET</code>⬇</td><td align="center"><code>PGA_AGGREGATE_TARGET</code>（PGA_AGGREGATE_LIMIT）⬇</td></tr><tr><td align="center">×</td><td align="center"><code>DB_CACHE</code> <code>SHARED_POOL</code> <code>LARGE_POOL</code> …</td><td align="center"></td></tr></tbody></table><img src="RAM.png" srcset="/img/loading.gif" title="内存架构" alt="内存架构"><hr><h4 id="关于SGA和PGA的一点配置总结"><a href="#关于SGA和PGA的一点配置总结" class="headerlink" title="关于SGA和PGA的一点配置总结"></a>关于SGA和PGA的一点配置总结</h4><p><code>Tips</code> 相关总结不一定正确，仍需实验验证.<br>[相关博客：关于oracle11G的自动内存管理MEMORY_TARGET和MEMORY_MAX_TARGET]（<a href="https://blog.csdn.net/fjseryi/article/details/50818843）" target="_blank" rel="noopener">https://blog.csdn.net/fjseryi/article/details/50818843）</a><br><code>MEMORY_MAX_TARGET</code> 参数定义了 <code>MEMORY_TARGET</code> 可以达到的最大值，若未设置，则默认等于 <code>MEMORY_TARGET</code> 的值；该值为数据库初始化参数，不可动态调节，通过调整Spfile中的<code>MEMORY_MAX_TARGET</code>并重启实例，可以达到调整的目的.<br><code>MEMORY_TARGET</code> SGA + PGA. Oracle总共可以使用的共享内存大小，不可超过 <code>MEMORY_MAX_TARGET</code> 的大小，默认为0；该值可以动态调节，无需重启实例.<br><code>动态内存管理</code> 使用动态内存管理时，<code>MEMORY_TARGET</code>下的 <code>SGA_TARGET</code> 和 <code>PGA_AGGREGATE_TARGET</code> 代表它们各自内存区域的<strong>最小设置</strong>，要让Oracle完全控制内存管理，上述两个参数应该设置为0.</p><ul><li><p><code>MEMORY_TARGET</code>设置为非0值：<br>  → 设置了<code>SGA</code>/<code>PGA_AGGREGATE_TARGET</code>，则两个参数将各自作为最小值，作为各自的初始化目标值.<br>  → 未设置<code>SGA</code>/<code>PGA_AGGREGATE_TARGET</code>，则根据DB状态按照一个固定比例分配：<br>  SGA = MEMORY_TARGET * 60%.<br>  PGA_AGGREGATE_TARGET = MEMORY_TARGET * 40%.<br>  → 仅设置了两个中的一个，则 SGA = MEMORY_TARGET - PGA_AGGREGATE_TARGET；反之类似.</p></li><li><p><code>MEMORY_TARGET</code>设置为0或未设置：<br>  → 设置了<code>SGA</code>/<code>PGA_AGGREGATE_TARGET</code>，则自动调节 <code>SGA</code> 中的 Shared pool、Buffer Cache、Redo Log Buffer、Java Pool、Larger Pool等内存空间的大小；PGA 则依赖 <code>PGA_AGGREGATE_TARGET</code> 的大小。 <code>SGA</code>/<code>PGA_AGGREGATE_TARGET</code> 不能自动增长和自动缩小.<br>  → 未设置<code>SGA</code>/<code>PGA_AGGREGATE_TARGET</code>，第1点<code>SGA</code> 中的各二级内存配置需要被明确设定，<code>SGA</code>/<code>PGA_AGGREGATE_TARGET</code> 不能自动增长和自动缩小.<br>  → <code>MEMORY_MAX_TARGET</code> 设置而 <code>MEMORY_TARGET</code> = 0，这种情况不太懂？？？</p></li></ul><hr><h4 id="SGA下的各种缓冲区（二级内存配置）"><a href="#SGA下的各种缓冲区（二级内存配置）" class="headerlink" title="SGA下的各种缓冲区（二级内存配置）"></a>SGA下的各种缓冲区（二级内存配置）</h4><ul><li><code>Data Buffer Cache</code> 数据高速缓冲区，又分级为：Dirty Buffer/Free Buffer/Pinned Buffer<br>  → <code>Dirty Buffer</code> 脏缓冲区，当数据库发生 DML（Insert、Update、Delete）操作时，会对缓冲区内容进行修改，这样缓冲区的内容就会和相对应的数据文件不一致，这时，缓冲区标识为“脏缓冲区”.<br>  → <code>Free Buffer</code> 自由缓冲区，当“脏缓冲区”的内容被写入数据文件后，因为该缓冲区与相应数据文件部分内容一致，所以将这些缓冲区称为“自由缓冲区”；当执行 SELECT 语句时，会将对应数据文件部分数据读取到数据高速缓存的相应缓冲区，因为缓冲区与数据块内容完全一致，所以这些缓冲区也被称为“自由缓冲区”.<br>  → <code>Pinned Buffer</code> 忙缓冲区，指服务器进程正在访问的缓冲区.<br>  → 为了防止数据库高速缓冲区空间不够用，Oracle 会将脏缓冲区中的数据写入对应的数据文件中（<code>Redo.log</code>），以腾出空间给新的数据.</li></ul><p><code>Ex</code> 高速缓冲区的大小管理</p><pre><code class="bash"># 显示高速缓冲区的大小# &quot;0&quot; 表示数据库自动管理，这里表示设置的是最小值。show parameter db_cache_size# 修改数据库高速缓冲区大小alter system set db_cache_size=500m; # flush缓冲区，生产库慎用alter system flush buffer_cache</code></pre><hr><ul><li><code>Redo Log Buffer</code> 重做日志缓冲区（循环文件，redo01.log/redo02.log/redo03.log），由一条条重做项构成，大小初始化参数为LOG_BUFFER.</li></ul><hr><ul><li><code>Shared Pool</code> SGA的共享池，内含库缓存、数字字典缓冲区（执行计划的依赖来源）等.<br>  <code>数据字典缓冲区</code> 数据库参考信息（数据库结构/用户等）<br>  <code>库高速缓存</code> 共享SQL区和共享PL/SQL区.</li></ul><p><code>Ex</code> Shared Pool 展示</p><pre><code class="SQL">--当用户执行语句时SELECT * FROM emp WHERE empno=7788;--Oracle 需要查询数据字典 dba_tables 确定表 emp 是否存在--如果该表已经存在，还需要查询数据字典 dba_tab_columns 确定列 empno 在表 emp 中是否存在SELECT * FROM dba_tab_columns WHERE column_name = &#39;EMPNO&#39;;--然后才能生成执行语句的过程（执行计划），这些定义在首次查询时存入数据字典高速缓冲区.</code></pre><hr><ul><li><code>大型池 Java池 流池</code><br>主要是大型池的大小影响数据备份效率.</li></ul><hr><h4 id="PGA（了解即可）"><a href="#PGA（了解即可）" class="headerlink" title="PGA（了解即可）"></a>PGA（了解即可）</h4><p><code>PGA</code> 私有SQL、会话内存、SQL工作区.<br><code>PGA_AGGREGATE_LIMIT</code> PGA_AGGREGATE_TARGET 以外的一个PGA参数，用于进行PGA使用率的硬性限制，当超过当前PGA时，Oracle会自动终止会话以保持合适的PGA内存.</p><pre><code class="SQL">ALTER SYSTEM SET PGA_AGGREGATE_LIMIT=2G;ALTER SYSTEM SET PGA_AGGREGATE_LIMIT=0; --disables the hard limit</code></pre><h4 id="In-Memory-Column-Store"><a href="#In-Memory-Column-Store" class="headerlink" title="In-Memory Column Store"></a>In-Memory Column Store</h4><ul><li><code>In-Memory area</code> 适用于：<br>  → 资源表中的行非常多，但查询结果行不多.<br>  → 资源表中的列很多，但查询结果的列很少.<br>  → 查询聚集数据</li></ul><h4 id="数据更新时相关进程的走势"><a href="#数据更新时相关进程的走势" class="headerlink" title="数据更新时相关进程的走势"></a>数据更新时相关进程的走势</h4><ul><li><p>用户进程：用户机器上的进程，在服务端体现为进程状态中的LOCAL和非LOCAL.</p><img src="用户进程.png" srcset="/img/loading.gif" title="用户进程" alt="用户进程"></li><li><p>服务进程：<code>LOCAL</code>进程，连接用户进程和实例</p></li><li><p>后台进程：<code>LGWR</code> <code>DBWn</code> <code>CKPT</code>/ <code>SMON</code> <code>PMON</code> 实例的一部分</p></li><li><p>实例：<code>SGA</code> <code>PGA</code> 等</p></li></ul><hr><ul><li><p>Origin ➡ Buffer_cache</p><img src="进程1.png" srcset="/img/loading.gif" title="Origin ➡ Buffer_cache" alt="Buffer_cache"></li><li><p>New ➡ Redo_buffer     Redo_buffer ➡ Buffer_cache</p><img src="进程2.png" srcset="/img/loading.gif" title="New ➡ Redo_buffer" alt="Redo_buffer"></li><li><p>Redo_buffer ➡ redo01~03.log       Buffer_cache ➡ Origin &amp;&amp; Update SCN &amp;&amp; 数据库同步</p><img src="进程3.png" srcset="/img/loading.gif" title="Redo_buffer ➡ Buffer_cache" alt="Buffer_cache"></li></ul><pre><code class="bash"># redo01~03.log[oracle@ocp12c prod1]$ ll /u01/app/oracle/oradata/prod1     # $ORACLE_BASE/oradata/prod1total 3607664-rw-r----- 1 oracle oinstall   10600448 Dec 28 12:53 control01.ctl-rw-r----- 1 oracle oinstall  209715712 Dec 28 12:52 redo01.log-rw-r----- 1 oracle oinstall  209715712 Dec 28 11:46 redo02.log-rw-r----- 1 oracle oinstall  209715712 Dec 28 11:47 redo03.log-rw-r----- 1 oracle oinstall  817897472 Dec 28 12:51 sysaux01.dbf-rw-r----- 1 oracle oinstall  933240832 Dec 28 12:50 system01.dbf-rw-r----- 1 oracle oinstall   67117056 Dec 28 09:36 temp01.dbf-rw-r----- 1 oracle oinstall 1289756672 Dec 28 12:50 undotbs01.dbf-rw-r----- 1 oracle oinstall    5251072 Dec 28 11:51 users01.dbf</code></pre><hr><h4 id="redo01-redo03-log-VS-undo-data"><a href="#redo01-redo03-log-VS-undo-data" class="headerlink" title="redo01~redo03.log VS undo.data"></a>redo01~redo03.log VS undo.data</h4><p><code>Tips</code> <code>redo.log/undo.data</code>之间的一点关联和解析，需结合上述后台进程进行理解<br><img src="redo_log&undo_data.png" srcset="/img/loading.gif" title="数据更新流程" alt="数据更新流程"></p><hr><h3 id="04-实例管理"><a href="#04-实例管理" class="headerlink" title="04. 实例管理"></a>04. 实例管理</h3><h4 id="实例加载"><a href="#实例加载" class="headerlink" title="实例加载"></a>实例加载</h4><ul><li><p><code>NOMOUNT</code> 进程启动<br>  $ORACLE_HOME/dbs/ 下的 spfile<SID>.ora/init<SID>.ora<br>  打开alter_<SID>.log/trace文件<br>  分配SGA，启动进程</p></li><li><p><code>MOUNT</code> 挂载状态<br>  定位所有控制文件<br>  通过加载控制文件，定位实例数据文件/Redo日志在哪里，但不介入（不判断是否真的存在），加载后台实例/动态性能视图，</p></li><li><p><code>OPEN</code> 介入数据文件，打开数据库数据文件/Redo日志</p></li></ul><pre><code class="SQL">--NOMOUNT&amp;OPENSYS@prod1&gt; startup nomountSYS@prod1&gt; alter database mountSYS@prod1&gt; alter database open--MOUNT&amp;OPENSYS@prod1&gt; startup mountSYS@prod1&gt; alter database open--STARTUPSYS@prod1&gt; startup</code></pre><h4 id="实例关闭"><a href="#实例关闭" class="headerlink" title="实例关闭"></a>实例关闭</h4><pre><code class="bash">SYS@prod1&gt; shutdown immediateSYS@prod1&gt; shutdown abort   ⬅断电，生产环境应避免使用，数据库不会进行同步</code></pre><h4 id="动态性能视图"><a href="#动态性能视图" class="headerlink" title="动态性能视图"></a>动态性能视图</h4><ul><li>数据库在MOUNT阶段就激活的功能，因为是动态视图，所以其数值的变化会贯穿整个数据库的启动状态.</li></ul><h4 id="ORACLE-HOME-dbs"><a href="#ORACLE-HOME-dbs" class="headerlink" title="$ORACLE_HOME/dbs"></a>$ORACLE_HOME/dbs</h4><ul><li><code>spfile&lt;sid&gt;.ora/init&lt;sid&gt;.ora</code> 初始化文件，后者为文本可读格式的静态文件，后续有详细介绍.</li></ul><pre><code class="bash">ll $ORACLE_HOME/dbs/total 10712-rw-rw---- 1 oracle oinstall     1544 Dec 28 19:03 hc_prod1.dat-rw-rw---- 1 oracle oinstall     1544 Dec  7 14:22 hc_prodcdb.dat-rw-r--r-- 1 oracle oinstall     3079 May 15  2015 init.ora-rw-r--r-- 1 oracle oinstall     1225 Dec 28 19:34 initprod1.ora-rw-r----- 1 oracle oinstall       24 Nov 13 09:28 lkPROD1-rw-r----- 1 oracle oinstall       24 Nov 13 10:27 lkPRODCDB-rw-r----- 1 oracle oinstall     3584 Nov 13 10:05 orapwprod1-rw-r----- 1 oracle oinstall     3584 Nov 13 11:21 orapwprodcdb-rw-r----- 1 oracle oinstall 10928128 Dec 28 18:08 snapcf_prod1.f-rw-r----- 1 oracle oinstall     3584 Jan  4 09:40 spfileprod1.ora-rw-r----- 1 oracle oinstall     3584 Dec 23 23:11 spfileprodcdb.ora</code></pre><hr><h3 id="05-实例文件详解"><a href="#05-实例文件详解" class="headerlink" title="05. 实例文件详解"></a>05. 实例文件详解</h3><h4 id="控制文件-Control-Files"><a href="#控制文件-Control-Files" class="headerlink" title="控制文件 Control Files"></a>控制文件 Control Files</h4><pre><code class="SQL">-- SYS@prod1&gt; DESC V$DATAFILE;-- SYS@prod1&gt; DESC DBA_DATA_FILES;--查看控制文件路径及数量SYS@prod1&gt; show parameter controlNAME                     TYPE     VALUE------------------------------------ ----------- ------------------------------control_file_record_keep_time        integer     7control_files                        string     /u01/app/oracle/oradata/prod1/control01.ctl, /u01/app/oracle/fast_recovery_area/prod1/control02.ctlcontrol_management_pack_access        string     DIAGNOSTIC+TUNING</code></pre><ul><li>控制/新增文件备份</li></ul><pre><code class="SQL">--二进制备份SYS@prod1&gt; ALTER DATABASE BACKUP CONTROLFILE TO &#39;/home/oracle/oracle_bakup/control.ctl&#39;;--文本备份SYS@prod1&gt; ALTER DATABASE BACKUP CONTROLFILE TO trace AS &#39;/home/oracle/oracle_bakup/control.ctl&#39;;</code></pre><img src="control_ctl.png" srcset="/img/loading.gif" title="数据更新流程" alt="数据更新流程"><pre><code class="SQL">--新增控制文件，数据库进入mountSYS@prod1&gt; startup mountORACLE instance started.Total System Global Area 1560281088 bytesFixed Size            8621088 bytesVariable Size          989856736 bytesDatabase Buffers      553648128 bytesRedo Buffers            8155136 bytesDatabase mounted.--查看现有控制文件地址及数量SYS@prod1&gt; show parameter controlNAME                     TYPE     VALUE------------------------------------ ----------- ------------------------------control_file_record_keep_time         integer     7control_files                 string     /u01/app/oracle/oradata/prod1/control01.ctl, /u01/app/oracle/fast_recovery_area/prod1/control02.ctl--新增控制文件control03.ctl 并关闭实例SYS@prod1&gt; alter system set control_files=&#39;/u01/app/oracle/oradata/prod1/control01.ctl&#39;,&#39;/u01/app/oracle/fast_recovery_area/prod1/control02.ctl&#39;,&#39;/u01/app/oracle/oradata/prod1/control03.ctl&#39; scope=spfile;SYS@prod1&gt; shutdown immediate--复制control01文件并rename，和参数中的配置同步--记住，复制的.ctl文件需是实例关闭后，同步过SNC号的.ctl文件cp /u01/app/oracle/oradata/prod1/control01.ctl /u01/app/oracle/oradata/prod1/control03.ctl--重启实例[oracle@ocp12c ~]$ sqlplus / as sysdbaSYS@prod1&gt; startup mountORACLE instance started.Total System Global Area 1560281088 bytesFixed Size            8621088 bytesVariable Size          989856736 bytesDatabase Buffers      553648128 bytesRedo Buffers            8155136 bytesDatabase mounted.--查看新增control文件后的control参数状态SYS@prod1&gt; show parameter controlNAME                     TYPE     VALUE------------------------------------ ----------- ------------------------------control_file_record_keep_time         integer     7control_files                         string         /u01/app/oracle/oradata/prod1/control01.ctl, /u01/app/oracle/fast_recovery_area/prod1/control02.ctl, /u01/app/oracle/oradata/prod1/control03.ctlSYS@prod1&gt; alter database open;</code></pre><hr><ul><li>控制文件部分丢失或完全丢失的情况下重建控制文件<br>  ➡ 关闭实例/删除/启动/查看告警/关闭实例<br>  ➡ 复现：先删除某control文件</li></ul><img src="control1_miss.png" srcset="/img/loading.gif" title="复现某control文件丢失的情况" alt="复现某control文件丢失的情况"><ul><li>启动到mount即可（mount阶段已开始介入control文件）</li></ul><pre><code class="SQL">[oracle@ocp12c ~]$ sqlplus / as sysdbaSQL*Plus: Release 12.2.0.1.0 Production on Sat Dec 28 18:43:07 2019Copyright (c) 1982, 2016, Oracle.  All rights reserved.Connected to an idle instance.  ➡ 再次强调，此时数据库是关闭的！！！后续才可以进行正确的copy！！！SYS@prod1&gt; startup mountORACLE instance started.Total System Global Area 1560281088 bytesFixed Size            8621088 bytesVariable Size          989856736 bytesDatabase Buffers      553648128 bytesRedo Buffers            8155136 bytesORA-00205: error in identifying control file, check alert log for more info</code></pre><ul><li><p>可以去按照提示，去告警日志里看下原因</p><img src="control1_miss2.png" srcset="/img/loading.gif" title="某control文件丢失" alt="某control文件丢失"></li><li><p>恢复丢失的控制文件（在数据库实例完全关闭的情况下！！！）</p></li></ul><pre><code class="BASH">SYS@prod1&gt; shutdown immediateORA-01507: database not mountedORACLE instance shut down# 进入到控制文件所在目录，该目录下也包括一些初始化参数文件/密码文件等cd $ORACLE_BASE/oradata/prod1/--复制未丢失的control文件并重命名至control01.ctlcp control03.ctl control01.ctl--再次重启SYS@prod1&gt; startup mount</code></pre><img src="control1_miss3.png" srcset="/img/loading.gif" title="某control文件丢失恢复" alt="某control文件丢失恢复"><hr><ul><li>全部控制文件丢失，需在nomount状态下恢复控制文件，前提是知道redo.log和相关datafile文件的路径</li></ul><pre><code class="SQL">--找到redo.log和相关datafile文件的路径！！！SYS@prod1&gt; shutdown immediate;SYS@prod1&gt; startup nomount;--执行以下语句SYS@prod1&gt;CREATE CONTROLFILE REUSE DATABASE &quot;PROD1&quot; NORESETLOGS ARCHIVELOG MAXLOGFILES 16 MAXLOGMEMBERS 3 MAXDATAFILES 100 MAXINSTANCES 8 MAXLOGHISTORY 292LOGFILE GROUP 1 &#39;/u01/app/oracle/oradata/prod1/redo01.log&#39;, GROUP 2 &#39;/u01/app/oracle/oradata/prod1/redo02.log&#39;, GROUP 3 &#39;/u01/app/oracle/oradata/prod1/redo03.log&#39;-- STANDBY LOGFILEDATAFILE &#39;/u01/app/oracle/oradata/prod1/system01.dbf&#39;, &#39;/u01/app/oracle/oradata/prod1/sysaux01.dbf&#39;, &#39;/u01/app/oracle/oradata/prod1/undotbs01.dbf&#39;, &#39;/u01/app/oracle/oradata/prod1/users01.dbf&#39;CHARACTER SET AL32UTF8;--打开数据库SYS@prod1&gt; alter database open;--下面两步可以在备份的control.ctl文件内找到--含RESETLOGS参数打开数据库--简单来说就是加载联机归档日志并重置其log sequence为1，前提是它们存在的话；不存在则创建.SYS@prod1&gt; ALTER DATABASE OPEN RESETLOGS;--重新启用临时表空间-- Commands to add tempfiles to temporary tablespaces.-- Online tempfiles have complete space information.-- Other tempfiles may require adjustment.SYS@prod1&gt; ALTER TABLESPACE TEMP ADD TEMPFILE &#39;/u01/app/oracle/oradata/prod1/temp01.dbf&#39;     SIZE 67108864  REUSE AUTOEXTEND ON NEXT 655360  MAXSIZE 32767M;</code></pre><ul><li><p><code>RESETLOGS</code> The RESETLOGS option is always required after incomplete media recovery or recovery using a backup control file. Resetting the redo log does the following:</p><ol><li><strong>Archives the current online redo logs (if they are accessible) and then erases the contents of the online redo logs and resets the log sequence number to 1.</strong> For example, if the current online redo logs are sequence 1000 and 1001 when you open RESETLOGS, then the database archives logs 1000 and 1001 and then resets the online logs to sequence 1 and 2.</li><li><strong>Creates</strong> the online <strong>redo log files</strong> if they do <strong>not currently exist</strong>.</li><li><strong>Reinitializes</strong> the control file metadata about online redo logs and redo threads.</li><li><strong>Updates all current datafiles and online redo logs</strong> and all subsequent archived redo logs with a new RESETLOGS SCN and time stamp.</li></ol><p>Because the database will not apply an archived log to a datafile unless the RESETLOGS SCN and time stamps match, the RESETLOGS <strong>prevents you from corrupting datafiles</strong> with archived logs that are not from direct parent incarnations of the current incarnation.</p></li></ul><hr><h4 id="数据文件-Data-Files"><a href="#数据文件-Data-Files" class="headerlink" title="数据文件 Data Files"></a>数据文件 Data Files</h4><pre><code class="SQL">--查看相关数据文件及路径select T.bytes/1024/1024/1024 &quot;TB/(GB)&quot;, T.* FROM DBA_DATA_FILES t /*where t.tablespace_name like &#39;UNDOTBS1%&#39;*/ ORDER BY 2 desc;--创建特定属性的表空间，后续也有类似的罗列CREATE TABLESPACE SHIN DATAFILE &#39;/u01/app/oracle/oradata/prod1/shin01.dbf&#39; SIZE 10M AUTOEXTEND ON NEXT 10M EXTENT MANAGEMENT LOCALUNIFORM SIZE 1M SEGMENT SPACE MANAGEMENT MANUAL;    ⬅ 默认是AUTOALTER TABLESPACE SHIN ADD DATAFILE &#39;/u01/app/oracle/oradata/prod1/shin02.dbf&#39; SIZE 10M;</code></pre><hr><h4 id="联机重做日志文件（归档日志）-redo-log"><a href="#联机重做日志文件（归档日志）-redo-log" class="headerlink" title="联机重做日志文件（归档日志） redo.log"></a>联机重做日志文件（归档日志） redo.log</h4><ul><li>修改归档日志路径</li></ul><pre><code class="SQL">mkdir /u01/app/oracle/archivelogSYS@prod1&gt; alter system set log_archive_dest_1=&#39;location=/u01/app/oracle/archivelog&#39; scope=spfile;SYS@prod1&gt; alter system switch logfile; ⬅手动切换下日志试试，看看新路径下会不会有指定格式的归档日志SYS@prod1&gt; show parameter log_archive_dest_1;NAME                     TYPE     VALUE------------------------------------ ----------- ------------------------------log_archive_dest_1             string     location=/u01/app/oracle/archiveloglog_archive_dest_10             stringlog_archive_dest_11             stringlog_archive_dest_12             stringlog_archive_dest_13             stringlog_archive_dest_14             stringlog_archive_dest_15             stringlog_archive_dest_16             stringlog_archive_dest_17             stringlog_archive_dest_18             stringlog_archive_dest_19             string--手动切换日志SYS@prod1&gt; alter system switch logfile;--进入相关日志路径，查看是否产生了新的日志ls $ORACLE_BASE/archivelog--redo log日志组的增加alter database add logfile group 4 (&#39;/u01/app/oracle/oradata/prod1/redo04_01.log&#39;,&#39;/u01/app/oracle/oradata/prod1/redo04_02.log&#39;) size 50M;--redo log组新增成员alter database add logfile member&#39;/u01/app/oracle/oradata/prod1/redo04_03.log&#39;to group 4;--redo log组删除成员，删除时当前redo log成员的状态为不为CURRENT即可--查看当前日志成员状态SYS@prod1&gt; select group#, thread#, sequence#, bytes, members, archived, status from v$log;--if status(the member that wait to be drop) = CURRENT SYS@prod1&gt; alter system switch logfile;--drop logfile memberALTER DATABASE DROP LOGFILE MEMBER &#39;/u01/app/oracle/oradata/prod1/redo04_03.log&#39;;--删除redo log日志组，删除组的条件比删除成员的条件严苛，需要状态为 INACTIVE--如何将状态切换至INACTIVE? 手动切换当前的 redo.log 日志组，并触发checkpoint同步实例SYS@prod1&gt; alter system switch logfile;SYS@prod1&gt; select group#, thread#, sequence#, bytes, members, archived, status from v$log;--触发checkpoint，同步redo.log内的数据至数据文件内SYS@prod1&gt; alter system checkpoint;--此时再执行日志组的删除SYS@prod1&gt; alter database drop logfile group 4;</code></pre><hr><h3 id="06-操作系统文件详解"><a href="#06-操作系统文件详解" class="headerlink" title="06. 操作系统文件详解"></a>06. 操作系统文件详解</h3><h4 id="初始化文件"><a href="#初始化文件" class="headerlink" title="初始化文件"></a>初始化文件</h4><ul><li><p><code>spfile&lt;sid&gt;.ora</code> </p><img src="spfile.png" srcset="/img/loading.gif" title="spfile"></li><li><p><code>init&lt;sid&gt;.ora</code></p><img src="pfile.png" srcset="/img/loading.gif" title="pfile"></li></ul><h4 id="口令文件"><a href="#口令文件" class="headerlink" title="口令文件"></a>口令文件</h4><h4 id="归档文件"><a href="#归档文件" class="headerlink" title="归档文件"></a>归档文件</h4><pre><code class="SQL">--查看归档模式，PDB数据库共享CDB归档日志，故需在CDB级别操作sqlplus / as sysdbaarchive log listalter database archivelog;--关闭归档alter database noarchivelog;--归档路径查询，这里的log_archive_dest_n为自己配置的路径，一般配置log_archive_dest_1即可SYS@prod1&gt; show parameter archiveNAME                            TYPE        VALUE-----------------------------   ----------- ------------------------------archive_lag_target                integer        0log_archive_config                stringlog_archive_dest                stringlog_archive_dest_1                string        location=/u01/app/oracle/archivelog...                             ...         ...log_archive_dest_7                stringlog_archive_dest_8                stringlog_archive_dest_9                stringlog_archive_dest_state_1        string        enablelog_archive_dest_state_10        string        enable...                             ...         ...log_archive_dest_state_17        string        enablelog_archive_dest_state_18        string        enablelog_archive_dest_state_19        string        enablelog_archive_dest_state_2        string        enable...                             ...         ...log_archive_dest_state_3        string        enablelog_archive_dest_state_9        string        enablelog_archive_duplex_dest         stringlog_archive_format                string        %t_%s_%r.dbflog_archive_max_processes        integer        4log_archive_min_succeed_dest    integer        1log_archive_start                boolean        FALSElog_archive_trace                integer        0standby_archive_dest            string        ?#/dbs/arch</code></pre><h4 id="Trace-File-amp-Alter-Log-File"><a href="#Trace-File-amp-Alter-Log-File" class="headerlink" title="Trace File &amp; Alter Log File"></a>Trace File &amp; Alter Log File</h4><pre><code class="SQL">--alert文件和.trc文件如下所示--第一部分.trc文件在如下命令显示的路径下SYS@prod1&gt; show parameter dumpNAME                                    TYPE            VALUE------------------------------------    -----------     ------------------------------background_core_dump                    string            partialbackground_dump_dest                    string            /u01/app/oracle/product/12.2.0/db_1/rdbms/logcore_dump_dest                            string            /u01/app/oracle/diag/rdbms/prod1/prod1/cdumpmax_dump_file_size                        string            unlimitedshadow_core_dump                        string            partialuser_dump_dest                            string            /u01/app/oracle/product/12.2.0/db_1/rdbms/log--user_dump_dest路径下有两组实例的.trc文件cd /u01/app/oracle/product/12.2.0/db_1/rdbms/log--alert文件和第二部分.trc文件在下面的路径下--下面这个路径下也有相关的.trc和alert文件，但是和上述的.trc文件不太类似，生成的时间周期不太一样cd /u01/app/oracle/diag/rdbms/prod1/prod1/trace</code></pre><ul><li>两组路径下有不同的.trc文件，有何区别.</li></ul><hr><h3 id="07-数据库逻辑结构"><a href="#07-数据库逻辑结构" class="headerlink" title="07. 数据库逻辑结构"></a>07. 数据库逻辑结构</h3><h4 id="行片段-块-区-段-表空间"><a href="#行片段-块-区-段-表空间" class="headerlink" title="行片段/块/区/段/表空间"></a>行片段/块/区/段/表空间</h4><p><code>行链接</code> 单行数据超过单数据库容纳时，产生行链接.<br><code>行迁移</code> 数据发生update时，产生行迁移，更新的原始行rowid不会发生变化，但整行数据会迁移至新的数据块中，旧的块内会保存指向新块的地址.</p><pre><code class="sql">CREATE TABLESPACE &lt;TBSPACE_NAME&gt; DATAFILE &#39;/u01/app/oracle/oradata/prod1/TBSPACE_NAME01.dbf&#39; size 10MAUTOEXTEND ON NEXT 10M  ⬅ 是指表空间的数据文件大小自动扩展，最大扩充至32GEXTENT MANAGEMENT LOCAL     ⬅ 区的管理是LOCALUNIFORM SIZE 1M             ⬅ 区以后是每次1MB来扩展，即表的实际大小按1MB大小递增SEGMENT SPACE MANAGEMENT MANUAL     ⬅ 默认是AUTO--TABLESPACE GROUP GROUP_TMP          ⬅ 一半临时表空间可以配置为临时表空间组，回避单临时表空间不足的问题</code></pre><hr><h4 id="表空间和数据文件"><a href="#表空间和数据文件" class="headerlink" title="表空间和数据文件"></a>表空间和数据文件</h4><p><code>SYSTEM</code> 不能脱机 offline / 不能置为只读 read only / 不能重命名 / 不能删除<br><code>TABLESPACE</code> 常规表空间，建议按需求功能配置不同表空间</p><pre><code class="SQL">CREATE TABLESPACE SHIN DATAFILE &#39;/u01/app/oracle/oradata/prod1/shin01.dbf&#39; SIZE 10M AUTOEXTEND ON NEXT 10M EXTENT MANAGEMENT LOCALUNIFORM SIZE 1M SEGMENT SPACE MANAGEMENT MANUAL;</code></pre><hr><p><code>TEMP TABLESPACE</code> 临时表空间</p><pre><code class="SQL">SYS@prod1&gt; create temporary tablespace tempts1 tempfile &#39;/home/oracle/temp1_02.dbf&#39; size 2Mtablespace group group1;SYS@prod1&gt; create temporary tablespace tempts2 tempfile &#39;/home/oracle/temp2_02.dbf&#39; size 2Mtablespace group group2;SYS@prod1&gt; select * from dba_tablespace_groups;GROUP_NAME TABLESPACE_NAME------------------------------ ------------------------------GROUP1 TEMPTS1GROUP2 TEMPTS2--将表空间从一个临时表空间组移动到另外一个临时表空间组：SYS@prod1&gt; alter tablespace tempts1 tablespace group GROUP2 ;SYS@prod1&gt; select * from dba_tablespace_groups;GROUP_NAME TABLESPACE_NAME------------------------------ ------------------------------GROUP2 TEMPTS1GROUP2 TEMPTS2</code></pre><hr><h4 id="段（表-索引-簇等）"><a href="#段（表-索引-簇等）" class="headerlink" title="段（表/索引/簇等）"></a>段（表/索引/簇等）</h4><h4 id="区"><a href="#区" class="headerlink" title="区"></a>区</h4><h4 id="块"><a href="#块" class="headerlink" title="块"></a>块</h4><hr><h4 id="高水位线（-High-Water-Mark-HWM）"><a href="#高水位线（-High-Water-Mark-HWM）" class="headerlink" title="高水位线（(High Water Mark, HWM）"></a>高水位线（(High Water Mark, HWM）</h4><p><code>HWM</code> 原则上HWM只会增大，不会缩小.</p><ul><li><p>相关影响<br>→ 全表扫描通常要读出直到 HWM 标记的所有的属于该表数据库块，即使该表中没有任何数据.<br>→ 即使 HWM 以下有空闲的数据库块，键入在插入数据时使用了 append 关键字，则在插入时使用 HWM 以上的数据块，此时 HWM 会自动增大（插入速度快）.<br>→ HWM会直接影响到相关表空间的大小，即resize表空间时会失败.<br>→ 通常需要我们去优化这些高水位线但实际数据很少的表.</p></li><li><p><code>HWM</code> 的修正</p><img src="HWM.png" srcset="/img/loading.gif" title="HWM修正" alt="HWM修正"></li></ul><pre><code class="SQL">--实际数据低于高水位线30%的表的查询SELECT TABLE_NAME,(BLOCKS*8192/1024/1024)&quot;理论大小 M&quot;,(NUM_ROWS*AVG_ROW_LEN/1024/1024/0.9)&quot;实际大小 M&quot;,round((NUM_ROWS*AVG_ROW_LEN/1024/1024/0.9)/(BLOCKS*8192/1024/1024),3)*100||&#39;%&#39; &quot; 实际使用率%&quot;FROM DBA_TABLES where blocks&gt;100 and (NUM_ROWS*AVG_ROW_LEN/1024/1024/0.9)/(BLOCKS*8192/1024/1024)&lt;0.3 order by (NUM_ROWS*AVG_ROW_LEN/1024/1024/0.9)/(BLOCKS*8192/1024/1024) desc--重建并收缩alter table te123 enable ROW MOVEMENT; --表重建，行迁移alter table te123 shrink space cascade;--alter database datafile &#39;/u01/app/oracle/oradata/prod1/te123&#39; resize 15M;</code></pre><hr><h3 id="08-数据库网络配置"><a href="#08-数据库网络配置" class="headerlink" title="08. 数据库网络配置"></a>08. 数据库网络配置</h3><h4 id="监听器（后台服务）"><a href="#监听器（后台服务）" class="headerlink" title="监听器（后台服务）"></a>监听器（后台服务）</h4><ul><li><code>监听器</code> 提供数据库对外连接的服务器信息、协议、端口.<br>  主机IP/端口号/全局数据库名/Oracle主目录/SID<br>  全局数据库名：DB_NAME.DB_DOMAIN</li></ul><pre><code class="bash"># 这部分实验不太理解，先不做解释SYS@prod1&gt; alter system set db_domain=&#39;oracle.com&#39; scope=spfile;SYS@prod1&gt; shutdown immediateSYS@prod1&gt; startup# SYS@prod1&gt; alter system set global_name=TRUE;# 网络配置文件位置cd $ORACLE_HOME/network/admin/</code></pre><pre><code class="SQL"># 监听控制su - oraclelsnrctl statuslsnrctl stoplsnrctl start</code></pre><p><code>静态监听</code> 配置文件中有 SID_LIST_LISTENER 就是静态监听.<br><code>静态注册</code> 监听程序启动时读取 listener.ora 文件内容中的实例名(SID_NAME)和服务名（GLOBAL_DBNAME）注册.<br>到监听器中<br><code>动态监听</code> 动态注册默认只注册到默认的监听器上（监听器名称是 LISTENER、端口是 1521、协议是 TCP），如果需要向使用其他协议和端口的监听器注册，则要配置 local_listener 参数动态注册发生的时间：数据库实例启动的时候、每隔一分钟动态注册的进程：PMON.<br><code>手动发起注册命令</code> <code>alter system register;</code></p><h4 id="非标准监听配置（LISTENER，port-1522）-amp-客户端网络服务名（TNS）"><a href="#非标准监听配置（LISTENER，port-1522）-amp-客户端网络服务名（TNS）" class="headerlink" title="非标准监听配置（LISTENER，port: 1522）&amp; 客户端网络服务名（TNS）"></a>非标准监听配置（LISTENER，port: 1522）&amp; 客户端网络服务名（TNS）</h4><ul><li><code>监听器</code> 提供数据库对外连接的服务器信息、协议、端口.<br>  ➡ 增加监听器 LISTENER1，暂时不增加静态监听到1522端口<br>  ➡ 增加监听tnsname别名 aaa_1（等同于别名监听实验）</li></ul><pre><code class="bash"># 增加建cd $ORACLE_HOME/network/admin/vim listener.ora# listener.oralistener.ora# listener.ora Network Configuration File: /u01/app/oracle/product/12.2.0/db_1/network/admin/listener.ora# Generated by Oracle configuration tools.LISTENER =  (DESCRIPTION_LIST =    (DESCRIPTION =      (ADDRESS = (PROTOCOL = TCP)(HOST = ocp12c)(PORT = 1521))      (ADDRESS = (PROTOCOL = IPC)(KEY = EXTPROC1521))    )  )# 新增的监听器 LISTENER1LISTENER1 =  (DESCRIPTION_LIST =    (DESCRIPTION =      (ADDRESS = (PROTOCOL = TCP)(HOST = ocp12c)(PORT = 1522))    )  )SID_LIST_LISTENER =(SID_LIST =  (SID_DESC =   (GLOBAL_DBNAME = prod1)   (ORACLE_HOME = /u01/app/oracle/product/12.2.0/db_1)   (SID_NAME = prod1)  )  (SID_DESC =   (GLOBAL_DBNAME = prodcdb)   (ORACLE_HOME = /u01/app/oracle/product/12.2.0/db_1)   (SID_NAME = prodcdb)  ))######vim tnsnames.ora# tnsnames.oraaaa_1 =  (DESCRIPTION =    (ADDRESS = (PROTOCOL = TCP)(HOST = ocp12c)(PORT = 1522))    (CONNECT_DATA =      (SERVER = DEDICATED)      (SERVICE_NAME = prod1)    )  )######</code></pre><ul><li>默认的LOCALL监听不会注册至1522端口，故需要我们修改系统参数，修改local_listener参数并手动执行动态注册操作</li></ul><pre><code class="bash">SYS@prod1&gt; alter system set local_listener=&#39;aaa_1&#39;;     ⬅ 这里的本地监听器，可以使用listener.ora中的别名.System altered.SYS@prod1&gt; alter system register;   ⬅ 这里的本地监听器，可以使用listener.ora中的别名.System altered.</code></pre><ul><li>启动新监听（port: 1522）</li></ul><pre><code class="bash">[oracle@ocp12c admin]$ lsnrctl start LISTENER1[oracle@ocp12c admin]$ lsnrctl status LISTENER1LSNRCTL for Linux: Version 12.2.0.1.0 - Production on 04-JAN-2020 11:59:41Copyright (c) 1991, 2016, Oracle.  All rights reserved.                                                                  ⬇ 这里的监听端口为1522 Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=ocp12c)(PORT=1522))) STATUS of the LISTENER------------------------Alias                     LISTENER1Version                   TNSLSNR for Linux: Version 12.2.0.1.0 - ProductionStart Date                04-JAN-2020 11:57:18Uptime                    0 days 0 hr. 2 min. 22 secTrace Level               offSecurity                  ON: Local OS AuthenticationSNMP                      OFFListener Parameter File   /u01/app/oracle/product/12.2.0/db_1/network/admin/listener.oraListener Log File         /u01/app/oracle/diag/tnslsnr/ocp12c/listener1/alert/log.xmlListening Endpoints Summary...  (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=ocp12c)(PORT=1522)))  (DESCRIPTION=(ADDRESS=(PROTOCOL=tcps)(HOST=ocp12c)(PORT=5500))(Security=(my_wallet_directory=/u01/app/oracle/admin/prod1/xdb_wallet))(Presentation=HTTP)(Session=RAW))Services Summary...Service &quot;prod1&quot; has 1 instance(s).  ⬅ 服务名  Instance &quot;prod1&quot;, status READY, has 1 handler(s) for this service...  ⬅ 实例名Service &quot;prod1XDB&quot; has 1 instance(s).  Instance &quot;prod1&quot;, status READY, has 1 handler(s) for this service...The command completed successfully</code></pre><hr><p><code>Tips</code> Oracle12c中修改了 <em>local_listener</em> 后，且未将新端口监听添加至静态监听List的情况下，1521和新增且配置了参数后的非1521端口，均可正常本地监听并完成本地连接.</p><ul><li>修改前</li></ul><img src="listener.png" srcset="/img/loading.gif" title="local_listener 修改前"><pre><code class="SQL">SYS@prod1&gt; alter system set local_listener=&#39;aaa_1&#39;;     ⬅ 这里的本地监听器，可以使用listener.ora中的别名.SYS@prod1&gt; alter system register;   ⬅ 这里的本地监听器，可以使用listener.ora中的别名.SYS@prod1&gt; show parameter localNAME                     TYPE     VALUE------------------------------------ ----------- ------------------------------local_listener                 string     aaa_1parallel_force_local             boolean     FALSE</code></pre><ul><li><p>修改后</p><img src="listener2.png" srcset="/img/loading.gif" title="local_listener 修改后，两者可并存"></li><li><p>客户端连接测试</p><img src="listener3.png" srcset="/img/loading.gif" title="local_listener 修改后，两者可并存"></li></ul><hr><ul><li>拓展：增加服务名，用于建立用户与实例的桥梁</li></ul><pre><code class="SQL">SYS@prod1&gt; show parameter service_namesNAME                     TYPE     VALUE------------------------------------ ----------- ------------------------------service_names                 string     prod1SYS@prod1&gt; alter system set service_names=&#39;prod1,aaa&#39;;SYS@prod1&gt; show parameter service_namesNAME                     TYPE     VALUE------------------------------------ ----------- ------------------------------service_names                 string     prod1,aaa</code></pre><hr><ul><li>总结：</li></ul><ol><li><code>服务名</code> 服务名的建立连接了用户进程与实例，我们可以给一组实例配置多个服务名，并且动态注册到监听中，用于非本地客户端的连接. 如<code>拓展</code>中所示，我们给实例添加了一组额外的服务名，这时候我们便可以通过这组service_name去连接实例. <code>相当于是这样</code>: 在数据库服务器和客户端之间有一监听程序（Listener），在监听程序中，会记录相应数据库对应的服务名（一个数据库可能对应有多个服务名），当客户端需要连接数据库时，只需要提供服务名，就可以建立客户端和服务器之间的连接.</li></ol><pre><code class="SQL">SYS@prod1&gt; show parameter service_namesNAME                     TYPE     VALUE------------------------------------ ----------- ------------------------------service_names                 string     prod1,aaa-- 通过新增的aaa连接prod1实例sqlplus hr/oracle@192.168.73.101:1521/aaasqlplus hr/oracle@192.168.73.101:1521/prod1    --这里的service_name = prod1，       ⬆    --碰巧和实例名prod1相同，不要混淆！！！  </code></pre><hr><ol start="2"><li>过分麻烦的IP/port/service_name/系统目录等连接信息，会让实例的连接变得繁琐，通过tnsnames.ora中的网络别名配置，有效简化了实例连接. 别名样例详见<code>客户端网络服务名（TNS）</code>.</li></ol><pre><code class="SQL">-- tnsnames.oraprod1 =     ⬅ 简单连接命名 = prod1  (DESCRIPTION =     (ADDRESS = (PROTOCOL = TCP)(HOST = ocp12c)(PORT = 1521))    (CONNECT_DATA =      (SERVER = DEDICATED)      (SERVICE_NAME = prod1)    )  )aaa_1 =     ⬅ 简单连接命名 = aaa_1  (DESCRIPTION =                                        ⬇ 这里的监听端口为非标准的1522    (ADDRESS = (PROTOCOL = TCP)(HOST = ocp12c)(PORT = 1522))    (CONNECT_DATA =      (SERVER = DEDICATED)      (SERVICE_NAME = aaa)                       ⬆ --这里监听的service_name = aaa，                         --等同于prod1，因为实例prod1我们配置了两组service_name    )  )--------通过配置tnsnames.ora，开启别名连接sqlplus hr/oracle@192.168.73.101:1521/aaasqlplus hr/oracle@192.168.73.101:1521/prod1--上述连接方式简化为sqlplus hr/oracle@aaa_1sqlplus hr/oracle@prod1</code></pre><hr><ol start="3"><li>新增监听器<strong>LISTENER1</strong>并且配置<strong>local_listener</strong>，使得非标准端口动态注册成为现实，详见<code>非标准监听配置（LISTENER，port: 1522）</code>. 这里有一个12c可能存在的新特性，配置<strong>local_listener</strong>为 <strong><em>1521</em></strong> 端口下的<strong>aaa_1</strong>（即<strong>LISTENER1</strong>）后，原始 <strong><em>1521</em></strong>端口下的默认监听器<strong>LISTENER</strong>依旧允许用户登入. 详见<code>非标准监听配置（LISTENER，port: 1522）➡ 修改后 &amp; 客户端连接测试</code></li></ol><pre><code class="SQL">sqlplus hr/oracle@192.168.73.101:1521/prod1sqlplus hr/oracle@prod1 ⬅ (prod1 ➡ service_name: prod1 ➡ instance:prod1)sqlplus hr/oracle@192.168.73.101:1522/aaasqlplus hr/oracle@aaa_1 ⬅ (aaa_1 ➡ service_name: aaa   ➡ instance:prod1)</code></pre><hr><h3 id="09-CDB-PDB"><a href="#09-CDB-PDB" class="headerlink" title="09. CDB/PDB"></a>09. CDB/PDB</h3><h4 id="CDB-VS-PDB"><a href="#CDB-VS-PDB" class="headerlink" title="CDB VS PDB"></a>CDB VS PDB</h4><ul><li>CDB中的PDB文件</li></ul><p>数据文件的共享：12c之前，PDB复用CDB的所有文件，包括redo和control复用CDB的文件，spfile/密码等文件，PDB复用CDB的文件</p><pre><code class="SQL">--PDB复用部分CDB数据文件cd /u01/app/oracle/oradata/prodcdb--spfile/密码等文件，PDB复用CDB的文件cd /u01/app/oracle/product/12.2.0/db_1/dbs</code></pre><img src="PDB复用.png" srcset="/img/loading.gif" title="PDB oradata数据文件的复用"><img src="PDB复用2.png" srcset="/img/loading.gif" title="PDB 初始化文件的复用"><img src="PDB&CDB.png" srcset="/img/loading.gif" title="多租户容器数据库体系结构"><p><code>Tips</code>12.2之后的版本，PDB默认有自己的undo表空间（local undo），若没有自己的undo，则会share CDB$ROOT 中的undo.</p><ul><li><code>小结</code><ol><li>控制文件/联机重做日志/参数文件属于CDB（根：CDB&amp;ROOT），不属于PDB，PDB复用.</li><li>位于根而不在PDB中的文件：RAC数据库所有实例的undo，Oracle提供的元数据，CDB视图，CDB资源管理计划.</li><li>位于PDB而不在根中的文件：PDB系统表空间（含sysaux），PDB临时/用户表空间.</li></ol></li></ul><p><code>疑问</code>PDB的<code>undo</code>应该也是属于各自PDB的，12.1中共享，12.2之后各自独立.（解决）</p><hr><h4 id="CDB-PDB（配置-增删）"><a href="#CDB-PDB（配置-增删）" class="headerlink" title="CDB/PDB（配置/增删）"></a>CDB/PDB（配置/增删）</h4><ul><li>PDB&amp;CDB查看/切换</li></ul><pre><code class="SQL">--查看当前是否为PDB--若有PDB$SEED，则说明当前为PRODCDBSYS@prodcdb&gt; show pdbs;CON_ID CON_NAME              OPEN MODE  RESTRICTED---------- ------------------------------ ---------- ----------2 PDB$SEED              READ ONLY  NO3 PRODPDB              READ WRITE NOSYS@prodcdb&gt; show conn_name;CON_NAME------------------------------CDB$ROOTSYS@prodcdb&gt; show con_id;CON_ID------------------------------1--切换至PDBSYS@prodcdb&gt; alter session set container=PRODPDB;Session altered.--查看，此时只有当前的PDBSYS@prodcdb&gt; show pdbs; CON_ID CON_NAME              OPEN MODE  RESTRICTED---------- ------------------------------ ---------- ----------3 PRODPDB              READ WRITE NOSYS@prodcdb&gt; show con_name;CON_NAME------------------------------PRODPDBSYS@prodcdb&gt; show con_id;CON_ID------------------------------3--切换回CDBSYS@prodcdb&gt; alter session set container=CDB$ROOT;Session altered.SYS@prodcdb&gt; show pdbs;CON_ID CON_NAME              OPEN MODE  RESTRICTED---------- ------------------------------ ---------- ----------2 PDB$SEED              READ ONLY  NO3 PRODPDB              READ WRITE NO</code></pre><hr><ul><li>新建/删除PDB的service_name</li></ul><pre><code class="SQL">--在容器数据库下可以进行全局的服务名查询SYS@prodcdb&gt; select name, pdb from cdb_services;SYS@prodcdb&gt; select name, pdb from v$services;NAME       PDB---------- ----------SYS$BACKGR CDB$ROOTOUNDSYS$USERS  CDB$ROOTprodcdbXDB CDB$ROOTprodcdb    CDB$ROOTprodpdb    PRODPDBshin       PRODPDB--尝试给CDB下的PDB实例新建一个service_name--查看创建前PDB对应的service_nameSYS@prodcdb&gt; alter session set container=PRODPDB;Session altered.SYS@prodcdb&gt; col name format a10;SYS@prodcdb&gt; col pdb format a10;SYS@prodcdb&gt; select name, pdb from cdb_services;NAME       PDB---------- ----------prodpdb    PRODPDB--创建新的PDB服务名SYS@prodcdb&gt; exec dbms_service.create_service(&#39;shin&#39;,&#39;shin&#39;);PL/SQL procedure successfully completed.SYS@prodcdb&gt;  select name,pdb from cdb_services;NAME       PDB---------- ----------prodpdb    PRODPDBshin       PRODPDB</code></pre><ul><li>但是此时去查看系统监听，并没有看到服务名shin（Instance：prodpdb）的服务被动态注册到监听中，需要启动新的服务.</li></ul><img src="新建一个PDB的service_name.png" srcset="/img/loading.gif" title="new pdb service_name"><hr><pre><code class="SQL">--启动新PDB下的服务名SYS@prodcdb&gt; exec dbms_service.START_SERVICE(&#39;shin&#39;);PL/SQL procedure successfully completed.</code></pre><img src="启动新的PDB的service.png" srcset="/img/loading.gif" title="active new pdb service_name"><img src="启动新的PDB的service2.png" srcset="/img/loading.gif" title="active new pdb service_name"><hr><p><code>Tips</code>先关闭服务，再删除服务.</p><pre><code class="SQL">SQL&gt; alter session set container=prodpdb;Session altered.SQL&gt; exec dbms_service.stop_service(&#39;shin&#39;);PL/SQL procedure successfully completed.SQL&gt; exec dbms_service.delete_service(&#39;shin&#39;);</code></pre><ul><li>CDB/PDB下连接各用户</li></ul><pre><code class="SQL">--切换实例环境，目前有该服务器上有两组数据库实例，prod1 &amp; prodcdb[oracle@ocp12c ~]$ . oraenvORACLE_SID = [prodpdb] ? prodcdbThe Oracle base remains unchanged with value /u01/app/oracle[oracle@ocp12c ~]$ sqlplus / as sysdbaSQL*Plus: Release 12.2.0.1.0 Production on Sat Jan 11 11:21:32 2020Copyright (c) 1982, 2016, Oracle.  All rights reserved.Connected to:Oracle Database 12c Enterprise Edition Release 12.2.0.1.0 - 64bit ProductionSYS@prodcdb&gt; --开始测试连接方式--服务名方式连接--conn usr/pwd@ip:port/service_nameSYS@prodcdb&gt; conn test/oracle@ocp12c:1521/prodpdb;Connected.SYS@ocp12c:1521/prodcdb&gt; conn test/oracle@ocp12c:1521/prodpdb;Connected.TEST@ocp12c:1521/prodpdb&gt; conn test/oracle@ocp12c:1521/shin;Connected.TEST@ocp12c:1521/shin&gt; conn sys/oracle@ocp12c:1521/prodcdb as sysdba;Connected.SYS@ocp12c:1521/prodcdb&gt; --TNS别名方式连接--conn usr/pwd@tnsSYS@ocp12c:1521/prodcdb&gt; conn test/oracle@prodpdb;Connected.TEST@prodpdb&gt; conn sys/oracle@prodcdb as sysdba;Connected.SYS@prodcdb&gt; --PDB PRODPDB 对应的新服务名shin 没有在tnsnames.ora中设置服务名别名，故不能用本方法连接</code></pre><hr><ul><li>OMF VS 非OMF<br><code>OMF</code>Oracle文件管理，文件名系统生成，从<code>/u01/app/oracle/oradata</code>后的路径就开始自动生成，不合并到CDB容器路径<code>/u01/app/oracle/oradata/prodcdb</code>下，而是自动生成新的路径<code>/u01/app/oracle/ORADATA/prodcdb</code>），详情看下图.<img src="OMF_PDB_create.png" srcset="/img/loading.gif"  width="300" ></li></ul><hr><ul><li>新建CDB</li></ul><p><strong><em>手工方式创建</em></strong><br>新子句 <code>SEED FILE_NAME_CONVERT</code><br>新实例参数 <code>PDB_FILE_NAME_CONVERT</code><br>OMF模式（数据实例参数） <code>DB_CREATE_FILE_DEST</code></p><pre><code class="SQL"></code></pre><p><strong><em>DBCA方式创建</em></strong></p><pre><code class="bash">dbca -silent -createDatabase -templateName General_Purpose.dbc -responseFile NO_VALUE \ -gdbname PRODCDB2 -sid PRODCDB2 \  ⬅ 数据库名 &amp; 实例名 -createAsContainerDatabase TRUE \  ⬅ 创建为CDB -sysPassword oracle -systemPassword oracle \ -datafileDestination &#39;/u01/app/oracle/oradata&#39; \ ⬅ 数据路径配置 -recoveryAreaDestination &#39;/u01/app/oracle/flash_recovery_area&#39; \ ⬅ redo.log路径配置，原始路径名为`/u01/app/oracle/fast_recovery_area` -redoLogFileSize 50 \ -storageType FS \ -characterset ZHS16GBK -nationalCharacterSet AL16UTF16 \ -sampleSchema true \ -totalMemory 1024 \ -databaseType OLTP \ -emConfiguration NONE</code></pre><p><strong><em>DBCA方式删除</em></strong></p><pre><code class="bash">dbca -silent -deleteDatabase -sourceDB PRODCDB2 -sysDBAUserName sys -sysDBAPassword oracle -forceArchiveLogDeletion</code></pre><p><strong><em>非DBCA方式创建（老师口述，暂未做实验）</em></strong></p><pre><code class="SQL">ALTER PLUGGABLE DATABASE PRODPDB OPEN RESTRICTED;drop database;</code></pre><hr><ul><li>新建PDB（从PDB$SEED生成PDB）</li></ul><p><strong><em>位置子句</em></strong>  </p><p>OMF模式<code>CREATE_FILE_DEST</code></p><pre><code class="SQL">SYS@PRODCDB2&gt; create pluggable database PRODPDB2 admin user oracle identified by oracle CREATE_FILE_DEST=&#39;/u01/app/oracle/oradata&#39;;</code></pre><img src="OMF_PDB_create3.png" srcset="/img/loading.gif"  width="300" title="PRODPDB2，位置子句下的OMF模式创建"><p>非OMF模式<code>FILE_NAME_CONVERT</code></p><pre><code class="SQL">SYS@PRODCDB2&gt; create pluggable database PRODPDB3 admin user oracle identified by oracle roles=(dba) FILE_NAME_CONVERT=(&#39;/u01/app/oracle/oradata/PRODCDB2/pdbseed&#39;,&#39;/u01/app/oracle/oradata/PRODCDB2/prodpdb3&#39;);Pluggable database created.SYS@PRODCDB2&gt; show pdbs;CON_ID CON_NAME              OPEN MODE  RESTRICTED---------- ------------------------------ ---------- ----------2 PDB$SEED              READ ONLY  NO3 PRODPDB2              MOUNTED4 PRODPDB3              MOUNTED   ⬅ </code></pre><p><strong><em>非位置子句</em></strong></p><p>OMF模式 <code>DB_CREATE_FILE_DEST</code></p><pre><code class="SQL">SYS@PRODCDB2&gt; alter session set DB_CREATE_FILE_DEST=&#39;/u01/app/oracle/oradata&#39;;Session altered.SYS@PRODCDB2&gt; create pluggable database prodpdb4 admin user oracle identified by oracle default tablespace users;Pluggable database created.SYS@PRODCDB2&gt; show pdbs;CON_ID CON_NAME              OPEN MODE  RESTRICTED---------- ------------------------------ ---------- ----------2 PDB$SEED              READ ONLY  NO3 PRODPDB2              MOUNTED4 PRODPDB3              MOUNTED5 PRODPDB4              MOUNTED   ⬅ </code></pre><img src="OMF_PDB_create2.png" srcset="/img/loading.gif"  width="300" title="PRODPDB4，非位置子句下的OMF模式创建"><p>非OMF模式 <code>PDB_FILE_NAME_CONVERT</code></p><pre><code class="SQL">SYS@prodcdb2&gt; alter session set pdb_file_name_convert=&#39;/u01/app/oracle/oradata/PRODCDB2/pdbseed&#39;,&#39;/u01/app/oracle/oradata/PRODCDB2/prodpdb5&#39;;Session altered.SYS@prodcdb2&gt; create pluggable database prodpdb5 admin user oracle identified by oracle;Pluggable database created.SYS@prodcdb2&gt; show pdbs;CON_ID CON_NAME              OPEN MODE  RESTRICTED---------- ------------------------------ ---------- ----------2 PDB$SEED              READ ONLY  NO3 PRODPDB2              MOUNTED4 PRODPDB3              MOUNTED5 PRODPDB4              MOUNTED6 PRODPDB5              MOUNTED</code></pre><hr><ul><li>克隆PDB（clone from PDB existed）</li></ul><p><strong><em>位置子句</em></strong></p><p>OMF模式：</p><pre><code class="SQL">SYS@prodcdb&gt; create pluggable database PRODPDB6 from PRODPDB CREATE_FILE_DEST = &#39;/u01/app/oracle/oradata&#39;;Pluggable database created.SYS@prodcdb&gt; show pdbs;CON_ID CON_NAME              OPEN MODE  RESTRICTED---------- ------------------------------ ---------- ----------2 PDB$SEED              READ ONLY  NO3 PRODPDB              READ WRITE NO4 PRODPDB6              MOUNTED</code></pre><p>非OMF模式：</p><pre><code class="SQL">SYS@prodcdb&gt; create pluggable database prodpdb7 from prodpdb file_name_convert=(&#39;/u01/app/oracle/prodcdb/prodpdb&#39;,&#39;/u01/app/oracle/oradata/prodcdb/prodpdb7&#39;);Pluggable database created.SYS@prodcdb&gt; show pdbs;CON_ID CON_NAME              OPEN MODE  RESTRICTED---------- ------------------------------ ---------- ----------2 PDB$SEED              READ ONLY  NO3 PRODPDB              READ WRITE NO4 PRODPDB6              READ WRITE NO7 PRODPDB7              MOUNTED</code></pre><hr><p><strong><em>非位置子句</em></strong></p><p>OMF模式：</p><pre><code class="SQL">SYS@prodcdb&gt; alter pluggable database PRODPDB7 open;     Pluggable database altered.SYS@prodcdb&gt; show pdbs;CON_ID CON_NAME              OPEN MODE  RESTRICTED---------- ------------------------------ ---------- ----------2 PDB$SEED              READ ONLY  NO3 PRODPDB              READ WRITE NO4 PRODPDB6              MOUNTED7 PRODPDB7              READ WRITE NOSYS@prodcdb&gt;  alter session set DB_CREATE_FILE_DEST =&#39;/u01/app/oracle/oradata&#39;;Session altered.SYS@prodcdb&gt;  create pluggable database prodpdb8 from prodpdb7;Pluggable database created.SYS@prodcdb&gt; show pdbs;CON_ID CON_NAME              OPEN MODE  RESTRICTED---------- ------------------------------ ---------- ----------2 PDB$SEED              READ ONLY  NO3 PRODPDB              READ WRITE NO4 PRODPDB6              MOUNTED5 PRODPDB8              MOUNTED7 PRODPDB7              READ WRITE NO</code></pre><p><code>Tips</code>一个全局PDB启动时，由OMF创建格式带来的BUG<br><img src="OMF_PDB_create_BUG.png" srcset="/img/loading.gif"  width="300" title="一个全局PDB启动时，由OMF创建格式带来的BUG."></p><p>非OMF模式：</p><pre><code class="SQL">SYS@prodcdb&gt; alter session set DB_CREATE_FILE_DEST=&#39;&#39;;Session altered.SYS@prodcdb&gt; show parameter create;NAME                     TYPE     VALUE------------------------------------ ----------- ------------------------------create_bitmap_area_size          integer     8388608create_stored_outlines             stringdb_create_file_dest             stringdb_create_online_log_dest_1         stringdb_create_online_log_dest_2         stringdb_create_online_log_dest_3         stringdb_create_online_log_dest_4         stringdb_create_online_log_dest_5         stringSYS@prodcdb&gt; alter session set pdb_file_name_convert=&#39;/u01/app/oracle/oradata/prodcdb/p/oracle/oradata/prodcdb/prodpdb9&#39;;Session altered.SYS@prodcdb&gt; show parameter pdb_file_name_convert;NAME                     TYPE     VALUE------------------------------------ ----------- ------------------------------pdb_file_name_convert             string     /u01/app/oracle/oradata/prodcdb/prodpdb7, /u01/app/oracle/oradata/prodcdb/prodpdb9SYS@prodcdb&gt; create pluggable database prodpdb9 from prodpdb7;Pluggable database created.SYS@prodcdb&gt; show pdbs  CON_ID CON_NAME              OPEN MODE  RESTRICTED---------- ------------------------------ ---------- ----------2 PDB$SEED              READ ONLY  NO3 PRODPDB              READ WRITE NO4 PRODPDB6              MOUNTED5 PRODPDB8              MOUNTED6 PRODPDB9              MOUNTED7 PRODPDB7              READ WRITE NOSYS@prodcdb&gt; show pdbs;CON_ID CON_NAME              OPEN MODE  RESTRICTED---------- ------------------------------ ---------- ----------2 PDB$SEED              READ ONLY  NO3 PRODPDB              READ WRITE NO4 PRODPDB6              MOUNTED5 PRODPDB8              MOUNTED6 PRODPDB9              MOUNTED7 PRODPDB7              READ WRITE NO8 PRODPDB10              READ WRITE NO</code></pre><hr><ul><li>删除PDB</li></ul><pre><code class="SQL">SYS@prodcdb&gt; alter pluggable database prodpdb9 close immediate;Pluggable database altered.SYS@prodcdb&gt; show pdbs;CON_ID CON_NAME              OPEN MODE  RESTRICTED---------- ------------------------------ ---------- ----------2 PDB$SEED              READ ONLY  NO3 PRODPDB              READ WRITE NO4 PRODPDB6              MOUNTED5 PRODPDB8              MOUNTED6 PRODPDB9              MOUNTED7 PRODPDB7              READ WRITE NO8 PRODPDB10              READ WRITE NOSYS@prodcdb&gt; drop pluggable database prodpdb9 including datafiles;Pluggable database dropped.SYS@prodcdb&gt; show pdbs;CON_ID CON_NAME              OPEN MODE  RESTRICTED---------- ------------------------------ ---------- ----------2 PDB$SEED              READ ONLY  NO3 PRODPDB              READ WRITE NO4 PRODPDB6              MOUNTED5 PRODPDB8              MOUNTED7 PRODPDB7              READ WRITE NO8 PRODPDB10              READ WRITE NO</code></pre><ul><li>12.2热克隆<br>上述克隆方式均为read write模式下的热克隆.</li></ul><hr><h4 id="多租户CDB-PDB的相关管理（配置-增删）"><a href="#多租户CDB-PDB的相关管理（配置-增删）" class="headerlink" title="多租户CDB/PDB的相关管理（配置/增删）"></a>多租户CDB/PDB的相关管理（配置/增删）</h4><ul><li><p>PDB相关拔插实验还未完成</p></li><li><p>CDB关闭与开启（类似于单实例操作）</p></li><li><p>PDB关闭与自启</p></li></ul><p><code>SAVE STATE</code>打开<strong>指定PDB</strong>，使用<strong>SAVE STATE</strong>子句保存PDB的最后打开状态.</p><pre><code class="SQL">ALTER PLUGGABLE DATABASE PRODPDB OPEN;ALTER PLUGGABLE DATABASE PRODPDB SAVE STATE;--所有PDB均自启ALTER PLUGGABLE DATABASE all OPEN;ALTER PLUGGABLE DATABASE all SAVE STATE;--还原，不再保存PDB的最后打开状态ALTER PLUGGABLE DATABASE all DISCARD STATE;--立即关闭alter pluggable database prodpdb close immediate;--除去某个PDB外均关闭alter pluggable database all except prodpdb close;--正常全关闭alter pluggable database all close; --断电（PDB不同于CDB，断电后需要恢复介质！！！）alter pluggable database prodpdb close abort;alter pluggable database prodpdb open;recover database --recover datafile--或在 rman 里执行：recover pluggable database prodpdb;</code></pre><ul><li>PDB开启模式</li></ul><pre><code class="SQL">--受限模式打开（sys/system用户）ALTER PLUGGABLE DATABASE PRODPDB OPEN RESTRICTED;--只读ALTER PLUGGABLE DATABASE PRODPDB OPEN READ ONLY;--读写ALTER PLUGGABLE DATABASE PRODPDB OPEN;--PDB_SPFILE$</code></pre><ul><li>临时还原（不太用）<pre><code class="SQL">ALTER SYSTEM SET TEMP_UNDO_ENABLE=TRUE;</code></pre></li></ul><hr><h3 id="10-安全"><a href="#10-安全" class="headerlink" title="10. 安全"></a>10. 安全</h3><h4 id="Schema-profile"><a href="#Schema-profile" class="headerlink" title="Schema profile"></a>Schema profile</h4><ul><li>常用的两组profile参数修改</li></ul><pre><code class="SQL">alter PROFILE default LIMIT PASSWORD_LIFE_TIME unlimited;alter PROFILE default LIMIT FAILED_LOGIN_ATTEMPTS 100;</code></pre><ul><li><p>密码限制</p><img src="schema_profile.png" srcset="/img/loading.gif"  width="300" title="密码限制."></li><li><p>资源限制</p><img src="schema_profile2.png" srcset="/img/loading.gif"  width="300" title="资源限制."></li></ul><h4 id="权限"><a href="#权限" class="headerlink" title="权限"></a>权限</h4><p><code>权限分类</code>系统权限/对象权限</p><pre><code class="SQL">grant connect,resource,create view to test</code></pre><ul><li>系统权限</li></ul><p><code>with admin option</code>系统权限转授，即被赋予某权限的用户拥有赋予其它schema相同权限的能力.<br><code>Tips</code>系统权限被收回时，后续级联的权限<strong>不会</strong>被回收！！！</p><pre><code class="SQL">create user test1 identified by oracle;create user test2 identified by oracle;grant connect,resource,create view to test1 with admin option;sqlplus test1/oracle@prod1;grant connect,resource,create view to test2;</code></pre><ul><li>对象权限</li></ul><p><code>with grant option</code>对象权限转授，即被赋予某权限的用户拥有赋予其它schema相同权限的能力.<br><code>Tips</code>对象权限被收回时，后续级联的权限<strong>会</strong>被回收！！！</p><h4 id="角色"><a href="#角色" class="headerlink" title="角色"></a>角色</h4><p><code>角色</code>即不同权限的集合体.<br><img src="预定义角色.png" srcset="/img/loading.gif"  width="300" title="预定义角色."></p><h4 id="CDB内的公共用户（C-）"><a href="#CDB内的公共用户（C-）" class="headerlink" title="CDB内的公共用户（C##）"></a>CDB内的公共用户（C##）</h4><ul><li>公共角色</li></ul><img src="公共用户.png" srcset="/img/loading.gif"  width="300" title="公共用户."><ul><li>公共权限</li></ul><pre><code class="SQL">3.Create the following users:i.  Create a user,USER1 with the following specifications:(1)    USER1 should have the same identity in PDBPROD1,PDBPROD2,PDBPROD3,PDBPROD4,and PDBPROD5.(2)    USER1 should also have the same identity in any future PDB in PRODCDB..oraenvPRODCDBsqlplus / as sysdbacreate user c##user1 identified by oracle container=ALL;ii. Create user USER2 that is defined only in the PDBPROD1 database.sqlplus sys/oracle@pdbprod1 as sysdbacreate user user2 identified by oracle;4.Create the following roles:i.  Create a role named ROLE1 with the following specifications:(1)    It should be granted to users in all pluggable databases in the  PRODCDB container database.(2)    ROLE1 should also exist in any future PDB in PRODCDB.. oraenvPRODCDBsqlplus / as sysdbacreate role C##ROLE1 container=ALL;ii. Create a role named ROLE2 that can be granted to users only in the PDBPROD1 pluggable database in the PRODCDB container database.sqlplus sys/oracle@pdbprod1 as sysdbacreate role role2;5.Grant privileges and roles to users and roles.Use the following specifications:i.  Grant the USER1 user the privilege to connect to all current and future containers in the PRODCDB container database.Grant no other privileges.. oraenvPRODCDBsqlplus / as sysdbagrant create session to c##user1 container=ALL;revoke unlimited tablespace from c##user1 container=ALL;  --默认没有无限表空间权限ii. Grant the USER2 user the privilege to connect only to the PDBPROD1 container in the PRODCDB container database.Grant no other privileges.sqlplus sys/oracle@pdbprod1 as sysdbagrant create session to user2 container=current;iii.  Grant the ROLE1 role the privilege to create procedures so that the privilege can be granted to users in all pluggable databases in the PRODCDB container database.sqlplus / as sysdbagrant create procedure to c##role1 container=ALL;iiii. Grant the USER2 user the privilege to create triggers such that the privilege can be used by USER2 only in the PDBPROD1 pluggable database in the PRODCDB container database.sqlplus sys/oracle@pdbprod1 as sysdbagrant create trigger to user2 container=current;</code></pre><hr><h3 id="11-数据泵-闪回-RMAN"><a href="#11-数据泵-闪回-RMAN" class="headerlink" title="11. 数据泵/闪回/RMAN"></a>11. 数据泵/闪回/RMAN</h3><h4 id="EXPDP-IMPDP"><a href="#EXPDP-IMPDP" class="headerlink" title="EXPDP/IMPDP"></a>EXPDP/IMPDP</h4><ul><li>设置bakdump目录</li></ul><pre><code class="sql">--查询目录名称和对应的路径select * from dba_directories;--用于设置导入导出文件所在或存放的位置create directory dump_scott as &#39;/home/oracle/dump/scott&#39;;--可以通过 dba_directories 来查看系统中已创建的目录select * from dba_directories;--对于创建的目录必须授予用户读写目录的权限grant read,write on directory dump_scott to scott;</code></pre><hr><ul><li>EXPDP</li></ul><p><code>EXPDP</code>配合众多参数，完成多种维度的数据快速导出.<br><code>DUMPFILE</code><br><code>SCHEMAS</code><br><code>TABLES</code><br><code>TABLESOACES</code><br><code>CONTENT</code><br><code>ESTIMATE</code>expdp dp/dp estimate_only=y full=y logfile=n<br><code>LOGFILE</code><br><code>PARALLEL</code><br><code>VERSION</code>主要用在高版本至低版本时迁移时使用，例：从12c导出时，指定目标数据库的版本（11g），才可顺利导入.</p><pre><code class="sql">--按照schema导出expdp system/oracle@prod1 directionary=DATA_PUMP_DIR dumpfile=SCHEMAS_HR.dat schemas=hr</code></pre><hr><ul><li>IMPDP</li></ul><p><code>IMPDP</code>配合众多参数，完成多种维度的数据快速导入.<br><code>TABLE_EXISTS_ACTION</code><br><code>PARALLEL</code><br><code>TRANSFORM</code>impdp dp/dp directory=dp_dir dumpfile=hrc.dmp transform=disable_archive_logging:Y<br><code>REMAP_SCHEMA</code></p><pre><code class="sql">--只把全库中的 hr 这个 schema 下的对象导入到 hr2 这个 schema 下，hr2 这个用户会在导入过程中自动建立好，密码和权限和 hr 一样impdp system/oracle directory=DATA_PUMP_DIR dumpfile=SCHEMAS_HR.dat logfile=SCHEMAS_HR_IMPDP.log SCHEMAS=hr remap_schmas=hr:hr2 </code></pre><ul><li>Ex：</li></ul><pre><code class="sql">sqlplus / as sysdbaconn sys/oracle@prodpdb as sysdbacol OWNER format a15col DIRECTORY_NAME format a25col DIRECTORY_PATH format a60create or replace directory bakdump as &#39;/home/oracle/oracle_bakup/bak_dump&#39;;select * from dba_directories;grant read, write on directory bakdump to system;expdp system/oracle@prodpdb directory=DUMP_SYSTEM dumpfile=20200314_prodpdb_schema_test_expdp.dmp logfile=20200314_prodpdb_schema_test_expdp.log parallel=2 schemas=test impdp system/oracle@prodpdb directory=DUMP_SYSTEM dumpfile=20200314_prodpdb_schema_test_expdp.dmp logfile=20200314_prodpdb_schema_test_impdp.log parallel=2 table_exists_action=replace remap_schema=test:shin transform=disable_archive_logging:Y</code></pre><ul><li>EXPDP快速迁移表空间</li></ul><p><code>Tips</code>expdp的使用可以有效回避sysdba用户的权限索取.</p><pre><code class="SQL">expdp system/oralce@prodpdb directory=DUMP_SYSTEM dumpfile=20200314_prodpdb_tbspace_test_exp.dmp logfile=20200314_prodpdb_tbspace_test_exp.log parallel=2 tablespaces=test--后续data文件迁移...</code></pre><h4 id="闪回（暂未更新）"><a href="#闪回（暂未更新）" class="headerlink" title="闪回（暂未更新）"></a>闪回（暂未更新）</h4><ul><li>SCN详解</li></ul><p><code>SCN</code></p><pre><code class="SQL">--查询当前SCN号select current_scn from v$database;</code></pre><h4 id="RMAN"><a href="#RMAN" class="headerlink" title="RMAN"></a>RMAN</h4><ul><li>RMAN查看/备份</li></ul><p><code>Tips</code>记得记录下RMAN CLI打印的数据库实例DBID，测试环境下：DBID=3022297241.</p><pre><code class="SQL">--打印RMAN配置SHOW ALL;using target database control file instead of recovery catalogRMAN configuration parameters for database with db_unique_name PROD1 are:CONFIGURE RETENTION POLICY TO REDUNDANCY 1; # defaultCONFIGURE BACKUP OPTIMIZATION OFF; # defaultCONFIGURE DEFAULT DEVICE TYPE TO DISK; # defaultCONFIGURE CONTROLFILE AUTOBACKUP ON; # defaultCONFIGURE CONTROLFILE AUTOBACKUP FORMAT FOR DEVICE TYPE DISK TO &#39;%F&#39;; # defaultCONFIGURE DEVICE TYPE DISK PARALLELISM 1 BACKUP TYPE TO BACKUPSET; # defaultCONFIGURE DATAFILE BACKUP COPIES FOR DEVICE TYPE DISK TO 1; # defaultCONFIGURE ARCHIVELOG BACKUP COPIES FOR DEVICE TYPE DISK TO 1; # defaultCONFIGURE MAXSETSIZE TO UNLIMITED; # defaultCONFIGURE ENCRYPTION FOR DATABASE OFF; # defaultCONFIGURE ENCRYPTION ALGORITHM &#39;AES128&#39;; # defaultCONFIGURE COMPRESSION ALGORITHM &#39;BASIC&#39; AS OF RELEASE &#39;DEFAULT&#39; OPTIMIZE FOR LOAD TRUE ; # defaultCONFIGURE RMAN OUTPUT TO KEEP FOR 7 DAYS; # defaultCONFIGURE ARCHIVELOG DELETION POLICY TO NONE; # defaultCONFIGURE SNAPSHOT CONTROLFILE NAME TO &#39;/u01/app/oracle/product/12.2.0/db_1/dbs/snapcf_prod1.f&#39;; # default--修改并行度--指定在以后的备份与恢复中，将采用并行度为2，同时开启2个通道进行备份与恢复，当然也可以在run中指定通道来决定备份与恢复的并行程度. 默认情况下，自动分配通道的并行度为1，如果你通过设置PARALLELISM设置了并行通道为2，那么在run块中，如果你没有单独通过ALLOCATE CHANNEL 命令指定通道，它会默认使用2条并行通道，如果你在run 命令块中指定了数个ALLOCATE CHANNEL，那么rman在执行备份命令时会以你设置的channel为准，而忽略configure中的配置.CONFIGURE DEVICE TYPE DISK PARALLELISM 2;--开启备份优化CONFIGURE BACKUP OPTIMIZATION ON;--修改FORMATCONFIGURE CHANNEL DEVICE TYPE DISK FORMAT &#39;/home/oracle/oracle_bakup/rman_bakup/data/%U&#39;;--备份策略CONFIGURE RETENTION POLICY TO REDUNDANCY 1; -- 基于冗余数量CONFIGURE RETENTION POLICY TO RECOVERY WINDOW OF n DAYS; --基于时间，n天前--开始备份（含归档日志，不开归档的数据库也可以在RMAN备份时单独归档）BACKUP DATABASE PLUS ARCHIVELOG;BACKUP DATABASE PLUS ARCHIVELOG DELETE ALL INPUT;-- BACKUP DATABASE FORMAT &#39;/home/oracle/oracle_bakup/rman_bakup/data/%U&#39; PLUS ARCHIVELOG FORMAT &#39;/home/oracle/oracle_bakup/rman_bakup/data/arch%U&#39; DELETE ALL INPUT;---备份打印的日志Starting backup at 21-MAR-20current log archivedallocated channel: ORA_DISK_1channel ORA_DISK_1: SID=282 device type=DISKchannel ORA_DISK_1: starting archived log backup setchannel ORA_DISK_1: specifying archived log(s) in backup setinput archived log thread=1 sequence=1 RECID=4 STAMP=1028214027input archived log thread=1 sequence=2 RECID=5 STAMP=1028215012input archived log thread=1 sequence=3 RECID=6 STAMP=1028215025input archived log thread=1 sequence=4 RECID=7 STAMP=1028215044input archived log thread=1 sequence=5 RECID=8 STAMP=1029412585input archived log thread=1 sequence=6 RECID=9 STAMP=1030013412input archived log thread=1 sequence=7 RECID=10 STAMP=1034028291input archived log thread=1 sequence=8 RECID=11 STAMP=1035639753channel ORA_DISK_1: starting piece 1 at 21-MAR-20channel ORA_DISK_1: finished piece 1 at 21-MAR-20piece handle=/home/oracle/oracle_bakup/rman_bakup/data/05url6ua_1_1 tag=TAG20200321T134234 comment=NONEchannel ORA_DISK_1: backup set complete, elapsed time: 00:00:36channel ORA_DISK_1: starting archived log backup setchannel ORA_DISK_1: specifying archived log(s) in backup setinput archived log thread=1 sequence=26 RECID=2 STAMP=1028213737input archived log thread=1 sequence=27 RECID=3 STAMP=1028213738input archived log thread=1 sequence=28 RECID=1 STAMP=1028213728channel ORA_DISK_1: starting piece 1 at 21-MAR-20channel ORA_DISK_1: finished piece 1 at 21-MAR-20piece handle=/home/oracle/oracle_bakup/rman_bakup/data/06url6ve_1_1 tag=TAG20200321T134234 comment=NONEchannel ORA_DISK_1: backup set complete, elapsed time: 00:00:15Finished backup at 21-MAR-20Starting backup at 21-MAR-20using channel ORA_DISK_1channel ORA_DISK_1: starting full datafile backup setchannel ORA_DISK_1: specifying datafile(s) in backup setinput datafile file number=00003 name=/u01/app/oracle/oradata/prod1/undotbs01.dbfinput datafile file number=00001 name=/u01/app/oracle/oradata/prod1/system01.dbfinput datafile file number=00002 name=/u01/app/oracle/oradata/prod1/sysaux01.dbfinput datafile file number=00005 name=/u01/app/oracle/oradata/prod1/shin01.dbfinput datafile file number=00006 name=/u01/app/oracle/oradata/prod1/shin03.dbfinput datafile file number=00004 name=/u01/app/oracle/oradata/prod1/users01.dbfchannel ORA_DISK_1: starting piece 1 at 21-MAR-20channel ORA_DISK_1: finished piece 1 at 21-MAR-20piece handle=/home/oracle/oracle_bakup/rman_bakup/data/07url6vu_1_1 tag=TAG20200321T134326 comment=NONEchannel ORA_DISK_1: backup set complete, elapsed time: 00:01:36Finished backup at 21-MAR-20Starting backup at 21-MAR-20current log archivedusing channel ORA_DISK_1channel ORA_DISK_1: starting archived log backup setchannel ORA_DISK_1: specifying archived log(s) in backup setinput archived log thread=1 sequence=9 RECID=12 STAMP=1035639903channel ORA_DISK_1: starting piece 1 at 21-MAR-20channel ORA_DISK_1: finished piece 1 at 21-MAR-20piece handle=/home/oracle/oracle_bakup/rman_bakup/data/08url730_1_1 tag=TAG20200321T134504 comment=NONEchannel ORA_DISK_1: backup set complete, elapsed time: 00:00:01Finished backup at 21-MAR-20Starting Control File and SPFILE Autobackup at 21-MAR-20piece handle=/u01/app/oracle/fast_recovery_area/prod1/PROD1/autobackup/2020_03_21/o1_mf_s_1035639905_h7cbh2vn_.bkp comment=NONEFinished Control File and SPFILE Autobackup at 21-MAR-20--查看备份集LIST BACKUPSET;List of Backup Sets===================BS Key  Type LV Size       Device Type Elapsed Time Completion Time------- ---- -- ---------- ----------- ------------ ---------------1       Full    10.50M     DISK        00:00:01     28-DEC-19              BP Key: 1   Status: AVAILABLE  Compressed: NO  Tag: TAG20191228T145606        Piece Name: /u01/app/oracle/fast_recovery_area/prod1/PROD1/autobackup/2019_12_28/o1_mf_s_1028213766_h0fz46yf_.bkp  SPFILE Included: Modification time: 28-DEC-19  SPFILE db_unique_name: PROD1  Control File Included: Ckp SCN: 1015657      Ckp time: 28-DEC-19BS Key  Type LV Size       Device Type Elapsed Time Completion Time------- ---- -- ---------- ----------- ------------ ---------------2       Full    10.50M     DISK        00:00:00     28-DEC-19              BP Key: 2   Status: AVAILABLE  Compressed: NO  Tag: TAG20191228T150553        Piece Name: /u01/app/oracle/fast_recovery_area/prod1/PROD1/autobackup/2019_12_28/o1_mf_s_1028214353_h0fzpkq5_.bkp  SPFILE Included: Modification time: 28-DEC-19  SPFILE db_unique_name: PROD1  Control File Included: Ckp SCN: 1016724      Ckp time: 28-DEC-19BS Key  Type LV Size       Device Type Elapsed Time Completion Time------- ---- -- ---------- ----------- ------------ ---------------3       Full    10.50M     DISK        00:00:00     28-DEC-19              BP Key: 3   Status: AVAILABLE  Compressed: NO  Tag: TAG20191228T153057        Piece Name: /u01/app/oracle/fast_recovery_area/prod1/PROD1/autobackup/2019_12_28/o1_mf_s_1028215857_h0g15k7x_.bkp  SPFILE Included: Modification time: 28-DEC-19  SPFILE db_unique_name: PROD1  Control File Included: Ckp SCN: 1017948      Ckp time: 28-DEC-19BS Key  Type LV Size       Device Type Elapsed Time Completion Time------- ---- -- ---------- ----------- ------------ ---------------4       Full    10.50M     DISK        00:00:01     28-DEC-19              BP Key: 4   Status: AVAILABLE  Compressed: NO  Tag: TAG20191228T180801        Piece Name: /u01/app/oracle/fast_recovery_area/prod1/PROD1/autobackup/2019_12_28/o1_mf_s_1028225281_h0gbd244_.bkp  SPFILE Included: Modification time: 28-DEC-19  SPFILE db_unique_name: PROD1  Control File Included: Ckp SCN: 1019243      Ckp time: 28-DEC-19BS Key  Size       Device Type Elapsed Time Completion Time------- ---------- ----------- ------------ ---------------5       665.06M    DISK        00:00:32     21-MAR-20              BP Key: 5   Status: AVAILABLE  Compressed: NO  Tag: TAG20200321T134234        Piece Name: /home/oracle/oracle_bakup/rman_bakup/data/05url6ua_1_1  List of Archived Logs in backup set 5  Thrd Seq     Low SCN    Low Time  Next SCN   Next Time  ---- ------- ---------- --------- ---------- ---------  1    1       1015561    28-DEC-19 1016395    28-DEC-19  1    2       1016395    28-DEC-19 1017178    28-DEC-19  1    3       1017178    28-DEC-19 1017186    28-DEC-19  1    4       1017186    28-DEC-19 1017195    28-DEC-19  1    5       1017195    28-DEC-19 1064037    11-JAN-20  1    6       1064037    11-JAN-20 1102736    18-JAN-20  1    7       1102736    18-JAN-20 1126791    02-MAR-20  1    8       1126791    02-MAR-20 1162833    21-MAR-20BS Key  Type LV Size       Device Type Elapsed Time Completion Time------- ---- -- ---------- ----------- ------------ ---------------7       Full    1.35G      DISK        00:01:26     21-MAR-20              BP Key: 7   Status: AVAILABLE  Compressed: NO  Tag: TAG20200321T134326        Piece Name: /home/oracle/oracle_bakup/rman_bakup/data/07url6vu_1_1  List of Datafiles in backup set 7  File LV Type Ckp SCN    Ckp Time  Abs Fuz SCN Sparse Name  ---- -- ---- ---------- --------- ----------- ------ ----  1       Full 1162864    21-MAR-20              NO    /u01/app/oracle/oradata/prod1/system01.dbf  2       Full 1162864    21-MAR-20              NO    /u01/app/oracle/oradata/prod1/sysaux01.dbf  3       Full 1162864    21-MAR-20              NO    /u01/app/oracle/oradata/prod1/undotbs01.dbf  4       Full 1162864    21-MAR-20              NO    /u01/app/oracle/oradata/prod1/users01.dbf  5       Full 1162864    21-MAR-20              NO    /u01/app/oracle/oradata/prod1/shin01.dbf  6       Full 1162864    21-MAR-20              NO    /u01/app/oracle/oradata/prod1/shin03.dbfBS Key  Size       Device Type Elapsed Time Completion Time------- ---------- ----------- ------------ ---------------8       18.00K     DISK        00:00:00     21-MAR-20              BP Key: 8   Status: AVAILABLE  Compressed: NO  Tag: TAG20200321T134504        Piece Name: /home/oracle/oracle_bakup/rman_bakup/data/08url730_1_1  List of Archived Logs in backup set 8  Thrd Seq     Low SCN    Low Time  Next SCN   Next Time  ---- ------- ---------- --------- ---------- ---------  1    9       1162833    21-MAR-20 1162910    21-MAR-20BS Key  Type LV Size       Device Type Elapsed Time Completion Time------- ---- -- ---------- ----------- ------------ ---------------9       Full    10.50M     DISK        00:00:01     21-MAR-20              BP Key: 9   Status: AVAILABLE  Compressed: NO  Tag: TAG20200321T134505        Piece Name: /u01/app/oracle/fast_recovery_area/prod1/PROD1/autobackup/2020_03_21/o1_mf_s_1035639905_h7cbh2vn_.bkp  SPFILE Included: Modification time: 14-MAR-20  SPFILE db_unique_name: PROD1  Control File Included: Ckp SCN: 1162919      Ckp time: 21-MAR-20--查看并删除冗余备份集REPORT OBSOLETE;DELETE OBSOLETE;--删除失效或损坏的备份集DELETE EXPIRED BACKUP--交叉校验备份集CROSSCHECK BACKUP;--删除特定备份集DELETE BACKUPSET n;--删除所有备份DELETE BACKUP;</code></pre><ul><li>非归档模式下的RMAN备份与恢复（被备份的源库实例需启动，可以mount不可open）</li></ul><pre><code class="SQL">--非归档模式下的BACKUP备份与恢复RMAN&gt; SHUTDOWN IMMEDIATERMAN&gt; STARTUP MOUNTEDRESTORE DATABASE;RECOVER DATABASE;ALTER DATABASE OPEN RESETLOGS;</code></pre><ul><li>归档模式下的RMAN备份与恢复（被备份的源库实例需启动，可以mount或open）</li></ul><pre><code class="SQL">--非归档模式下的BACKUP备份与恢复RESTORE DATABASE;RECOVER DATABASE; ALTER DATABASE OPEN RESETLOGS;</code></pre><ul><li><p>RMAN备份控制文件和SPFILE</p><pre><code class="SQL">BACKUP CURRENT CONTROLFILE;</code></pre></li><li><p>RMAN恢复控制文件和SPFILE</p><img src="RMAN_CONTROL_SPFILE.png" srcset="/img/loading.gif"  width="300" title="RMAN恢复控制文件和SPFILE"></li></ul><hr><h3 id="12-静默安装（本次测试使用的是11-2-0，12c以上需另寻测试环境）"><a href="#12-静默安装（本次测试使用的是11-2-0，12c以上需另寻测试环境）" class="headerlink" title="12. 静默安装（本次测试使用的是11.2.0，12c以上需另寻测试环境）"></a>12. 静默安装（本次测试使用的是11.2.0，12c以上需另寻测试环境）</h3><h4 id="YUM源依赖-用户-路径-环境变量"><a href="#YUM源依赖-用户-路径-环境变量" class="headerlink" title="YUM源依赖/用户/路径/环境变量"></a>YUM源依赖/用户/路径/环境变量</h4><pre><code class="bash">yum install -y gcc libaio glibc compat-libstdc++-33 elfutils-libelf-devel gcc-c++ libaio-devel libgcc libstdc++ libstdc++-devel unixODBC unixODBC-develoracle: vim ~/.bash_profile</code></pre><h4 id="compat-libstdc-33-此包2003镜像缺失，手动安装"><a href="#compat-libstdc-33-此包2003镜像缺失，手动安装" class="headerlink" title="compat-libstdc++-33 此包2003镜像缺失，手动安装"></a>compat-libstdc++-33 此包2003镜像缺失，手动安装</h4><h4 id="注意大磁盘挂载点-home，database-lt-—-安装包解压缩后的文件夹"><a href="#注意大磁盘挂载点-home，database-lt-—-安装包解压缩后的文件夹" class="headerlink" title="注意大磁盘挂载点 /home，database &lt;— 安装包解压缩后的文件夹"></a>注意大磁盘挂载点 /home，database &lt;— 安装包解压缩后的文件夹</h4><h4 id="默认应答配置"><a href="#默认应答配置" class="headerlink" title="默认应答配置"></a>默认应答配置</h4><pre><code class="bash"># 备份cp /home/u01/database/response/db_install.rsp /home/u01/database/response/db_install.rsp.bak# 修改数据库软件应答（无库仅软件）/home/u01/database/response/db_install.rsp# 配置response文件/liangren/database/response/db_install.rsp这个文件需要更改的地方有：oracle.install.option=INSTALL_DB_SWONLYORACLE_HOSTNAME=主机名UNIX_GROUP_NAME=oinstallINVENTORY_LOCATION=/home/u01/app/oraInventorySELECTED_LANGUAGES=en,zh_CNORACLE_HOME=/home/u01/app/oracle/product/11.2.0/db_1ORACLE_BASE=/home/u01/app/oracleoracle.install.db.InstallEdition=EEoracle.install.db.DBA_GROUP=dbaoracle.install.db.OPER_GROUP=dbaDECLINE_SECURITY_UPDATES=true# 静默安装软体./runInstaller -silent  –ignoreSysPrereq -responseFile /home/u01/database/response/db_install.rsp# 执行俩root脚本# 监听netca /silent /responsefile /home/u01/database/response/netca.rsp# 修改静默建库应答vim /home/u01/database/response/dbca.rsp  # 主要修改全局库名/实例名/密码等[CREATEDATABASE]gdbName = &quot;SHGRNOP&quot; # 修改！！！sid = &quot;SHGRNOP&quot;     # 修改！！！templateName = &quot;General_Purpose.dbc&quot;   # 等会修改基本库模板characterSet = &quot;ZHS16GBK&quot; memoryPercentage = &quot;60&quot;emConfiguration = &quot;LOCAL&quot;sysPassword = &quot;oracle&quot;systemPassword = &quot;oracle&quot;dbsnmpPassword = &quot;oracle&quot;sysmanPassword = &quot;oracle&quot;# 修改基本库模板 General_Purpose.dbc# 备份cp $ORACLE_HOME/assistants/dbca/templates/General_Purpose.dbc $ORACLE_HOME/assistants/dbca/templates/General_Purpose.dbc.bak$ORACLE_HOME/assistants/dbca/templates/General_Purpose.dbc    # 我仅修改了processes连接数至600# 静默建库dbca -silent -responseFile /home/u01/database/response/dbca.rsplsnrctl status</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>Oracle12c</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20191219_1545_ZooKeeper-Kafka</title>
    <link href="/2020/03/19/20191219-1545-ZooKeeper-Kafka/"/>
    <url>/2020/03/19/20191219-1545-ZooKeeper-Kafka/</url>
    
    <content type="html"><![CDATA[<h2 id="ZooKeeper-amp-Kafka-配置"><a href="#ZooKeeper-amp-Kafka-配置" class="headerlink" title="ZooKeeper &amp; Kafka 配置"></a>ZooKeeper &amp; Kafka 配置</h2><h3 id="01-ZooKeeper-amp-Kafka"><a href="#01-ZooKeeper-amp-Kafka" class="headerlink" title="01. ZooKeeper &amp; Kafka"></a>01. ZooKeeper &amp; Kafka</h3><ul><li>Repo Point<br><a href="http://mirrors.tuna.tsinghua.edu.cn/apache/kafka/0.10.2.1/kafka_2.11-0.10.2.1.tgz" target="_blank" rel="noopener">kafka_2.11-0.10.2.1.tgz</a><br><a href="http://archive.apache.org/dist/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz" target="_blank" rel="noopener">ZooKeeper</a></li></ul><h3 id="02-环境配置"><a href="#02-环境配置" class="headerlink" title="02. 环境配置"></a>02. 环境配置</h3><h4 id="1-环境包-rz-至相关路径，分流至其它节点："><a href="#1-环境包-rz-至相关路径，分流至其它节点：" class="headerlink" title="1. 环境包 rz 至相关路径，分流至其它节点："></a>1. 环境包 rz 至相关路径，分流至其它节点：</h4><h5 id="源码分发"><a href="#源码分发" class="headerlink" title="源码分发"></a>源码分发</h5><pre><code class="bash">scp -r /usr/local/src/spark-2.0.2-bin-hadoop2.6 root@slave1:/usr/local/src/scp -r /usr/local/src/spark-2.0.2-bin-hadoop2.6 root@slave2:/usr/local/src/</code></pre><h5 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h5><ul><li>后续补充</li></ul><h5 id="启动Zookeeper"><a href="#启动Zookeeper" class="headerlink" title="启动Zookeeper"></a>启动Zookeeper</h5><pre><code class="bash">zkServer.sh startzkServer.sh stopzkServer.sh status</code></pre><p><code>Tips</code>Zookeeper启停需在所有节点执行，可以自定义</p><pre><code class="bash">(base) [root@master bin]# cat zkServer_start_all.sh# zkServer_start_all.shzkServer.sh startssh slave1 &lt;&lt;EOF    zkServer.sh start    exitEOFssh slave2 &lt;&lt;EOF        zkServer.sh start    exitEOFsleep 3szkServer.sh status######(base) [root@master bin]# cat zkServer_stop_all.sh # zkServer_stop_all.shzkServer.sh stopssh slave1 &lt;&lt;EOF    zkServer.sh stop    exitEOFssh slave2 &lt;&lt;EOF        zkServer.sh stop    exitEOF######</code></pre><h5 id="启动Kafka"><a href="#启动Kafka" class="headerlink" title="启动Kafka"></a>启动Kafka</h5><pre><code class="bash">kafka-server-start.sh -daemon /usr/local/src/kafka_2.11-0.10.2.1/config/server.propertieskafka-server-stop.shjps</code></pre><p><code>Tips</code>Kafka启停需在所有节点执行，可以自定义</p><pre><code class="bash"># 首先是节点Kafka启动脚本# start-kafka.sh/usr/local/src/kafka_2.11-0.10.2.1/bin/kafka-server-start.sh -daemon /usr/local/src/kafka_2.11-0.10.2.1/config/server.properties######(base) [root@master bin]# cat kafka_start-all.sh # kafka_start-all.shstart-kafka.shssh slave1 &lt;&lt;EOF    start-kafka.sh    exitEOFssh slave2 &lt;&lt;EOF        start-kafka.sh        exitEOFsleep 2sjps######(base) [root@master bin]# cat kafka_stop-all.sh # kafka_stop-all.sh kafka-server-stop.shssh slave1 &lt;&lt;EOF    kafka-server-stop.sh    exitEOFssh slave2 &lt;&lt;EOF    kafka-server-stop.sh        exitEOFsleep 2sjps######</code></pre><h4 id="2-Topic"><a href="#2-Topic" class="headerlink" title="2. Topic"></a>2. Topic</h4><h5 id="命令行操作"><a href="#命令行操作" class="headerlink" title="命令行操作"></a>命令行操作</h5><ul><li>创建第一个Topic，并查看</li></ul><pre><code class="bash">kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 2 --topic PREDATA# --replication-factor 2 // 复制两份# --partitions 1 // 创建1个分区# --topic // 主题为my-topic# --zookeeper // 此处为为zookeeper监听的地址kafka-topics.sh --list --zookeeper localhost:2181# 查看Topic状态kafka-topics.sh --describe --zookeeper localhost:2181 --topic notify_topic # PREDATA</code></pre><img src="Topic.png" srcset="/img/loading.gif"  width="300" title="Topic test状态."><ul><li>删除第一个Topic</li></ul><pre><code class="bash">kafka-topics.sh --zookeeper localhost:2181 --delete --topic test</code></pre><ul><li>创建第一个Consumer</li></ul><pre><code class="bash">kafka-console-consumer.sh --zookeeper master:2181 --topic test --from-beginning# Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].# --zookeeper参数在后续版本会被取消，这里使用下面的命令替代kafka-console-consumer.sh --bootstrap-server 10.72.8.46:9092 10.72.8.30:9092 10.72.8.38:9092 --topic notify_topic --from-beginning# 从最新的offset开始消费kafka-console-consumer.sh --bootstrap-server 10.72.8.46:9092 10.72.8.30:9092 10.72.8.38:9092 --topic PREDATA# 从指定的offset开始消费，且指定偏移量时，需指定partitionkafka-console-consumer.sh --bootstrap-server 10.72.8.46:9092 10.72.8.38:9092 10.72.8.30:9092 --topic PREDATA --offset 25087628 --partition PREDATA</code></pre><ul><li>创建第一个Producer</li></ul><pre><code class="bash">kafka-console-producer.sh --broker-list 10.72.8.46:9092 10.72.8.38:9092 10.72.8.30:9092 --topic TESTTOPIC</code></pre><ul><li>查看日志</li></ul><pre><code class="bash"># Kafka日志需要利用其自带的工具才能查看../../bin/kafka-run-class.sh kafka.tools.DumpLogSegments --files xxx.index  --print-data-log</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>ZooKeeper Kafka</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20200226_1638_Hive（持续更新）</title>
    <link href="/2020/02/26/20191220-1940-Hive/"/>
    <url>/2020/02/26/20191220-1940-Hive/</url>
    
    <content type="html"><![CDATA[<h2 id="移花接木-Hive"><a href="#移花接木-Hive" class="headerlink" title="移花接木-Hive"></a>移花接木-Hive</h2><h3 id="01-Hive-1-x"><a href="#01-Hive-1-x" class="headerlink" title="01. Hive 1.x"></a>01. Hive 1.x</h3><ul><li>Repo Point<br><a href="http://mirror.bit.edu.cn/apache/hive/hive-1.2.2/apache-hive-1.2.2-bin.tar.gz" target="_blank" rel="noopener">apache-hive-1.2.2-bin.tar.gz</a><br><a href="https://downloads.mysql.com/archives/get/file/mysql-connector-java-5.1.44.tar.gz" target="_blank" rel="noopener">mysql-connector-java-5.1.44.tar</a><br><a href="http://archive.apache.org/dist/hive/hive-1.2.2/apache-hive-1.2.2-src.tar.gz" target="_blank" rel="noopener">apache-hive-1.2.2-src.tar.gz</a></li></ul><hr><h3 id="02-Beeline连接Hive（Review）"><a href="#02-Beeline连接Hive（Review）" class="headerlink" title="02. Beeline连接Hive（Review）"></a>02. Beeline连接Hive（Review）</h3><ul><li><p>这里使用的是<code>Hive Metastore ➡ hiveserver2 ➡ beeline</code>的连接方式（启动方式见下方）.</p></li><li><p>也可以使用<code>Hive Metastore ➡ hive（CLI）</code>的连接方式</p></li><li><p>后缀为重定向，需百度解决（已解决，详见 <a href="http://localhost:4000/2020/02/14/20191219-2254-Hive-1-x/" target="_blank" rel="noopener">20191224_1414_Hive-1.x配置</a>）</p></li></ul><pre><code class="bash"># hive-site.xml中配置了hive.metastore.uris后，无论是HS2或是Hive CLI开启前，都需开启Metastorehive --service metastore  1&gt;/dev/null  2&gt;&amp;1  &amp;# beeline方式连接Hive，默认端口为10000hive --service hiveserver2 &amp;#beeline#!connect jdbc:hive2://master:10000 root 123456beeline -u jdbc:hive2://master:10000 -n root -p 123456 --color=true</code></pre><p><code>Tips</code> 仅退出当前beeline: <strong>!close</strong>     彻底退出: <strong>!q</strong>     查询表: <strong>!tables</strong></p><hr><h3 id="03-Hive-元素"><a href="#03-Hive-元素" class="headerlink" title="03. Hive 元素"></a>03. Hive 元素</h3><h4 id="3-1-元数据-amp-数据"><a href="#3-1-元数据-amp-数据" class="headerlink" title="3-1 元数据 &amp; 数据"></a>3-1 元数据 &amp; 数据</h4><ul><li>Hive中的表是纯逻辑表，即<code>元数据</code>；</li><li>数据存储在<code>HDFS</code>上，元数据与数据存储分离；</li><li>数据计算依赖分布式计算框架 <strong>MapReduce</strong>；</li><li>Hive<code>读多写少</code>，从HDFS中读，MR计算，并写回HDFS；<code>不支持数据改写&amp;删除</code>；</li><li>用户需指定<code>三个属性</code>用来定义数据格式：<br>列分隔符：空格 ‘,’  ‘\t’<br>行分隔符：’\n’<br>读取数据的方法</li></ul><h4 id="3-2-Hive-连接方式"><a href="#3-2-Hive-连接方式" class="headerlink" title="3-2 Hive 连接方式"></a>3-2 Hive 连接方式</h4><p><code>CLI</code> <strong><em>Hive Metastore</em></strong> → hiveserver2(HS2) → <strong><em>Beeline</em></strong><br><code>JDBC</code> <strong><em>Metastore</em></strong> → Java/Python/C++<br><code>WUI</code> apache-hive-0.13.0-src.tar.gz → <strong><em>hive –service hwi</em></strong></p><h4 id="3-3-Hive-HDFS"><a href="#3-3-Hive-HDFS" class="headerlink" title="3-3 Hive HDFS"></a>3-3 Hive HDFS</h4><ul><li>HDFS部分主要是贮存离线数据，主要就是数据目录或数据的相关操作（详见 <a href="https://shinnosuke1028.github.io/2020/01/15/20191219-2347-Hadoop-2-6-1/" target="_blank" rel="noopener">20201219_2347_Hadoop-2.6.1（持续更新</a>）.</li></ul><hr><h3 id="04-数据入库"><a href="#04-数据入库" class="headerlink" title="04. 数据入库"></a>04. 数据入库</h3><h4 id="4-0-内置函数"><a href="#4-0-内置函数" class="headerlink" title="4-0 内置函数"></a>4-0 内置函数</h4><pre><code class="SQL">--时间戳select current_date;    --2020-02-13select current_timestamp;   --2020-02-13 18:09:40.666--转内核时间格式select to_unix_timestamp(&#39;2020-02-13 12:12:12&#39;);    --1581567132--内核转当前可读select from_unixtime(unix_timestamp()); --2020-02-13 18:09:12select from_unixtime(unix_timestamp(), &#39;yyyy-MM-dd HH:mm:ss&#39;);--字符串转日期select to_date(&#39;2020-02-13 13:34:12&#39;);--时间戳间隔天数select datediff(&#39;2020-02-13&#39;,&#39;2020-02-15&#39;);select date_sub(&#39;2020-02-13&#39;,4);select date_add(&#39;2015-04-09&#39;,4);    --select date_sub(&#39;2020-02-13&#39;,-4);year/month/day/hour/minute/second</code></pre><h4 id="4-1-DML"><a href="#4-1-DML" class="headerlink" title="4-1 DML"></a>4-1 DML</h4><ul><li>表的属性<ol><li>一张表 是内部表或者外部表， 这两个是二选一的</li><li>一张表可以是分区表</li><li>一张表可以是分桶表</li><li>一张表可以是分区表也可以是分桶表</li><li>第一条和剩余三条可以搭配（内部【外部】分区表 / 内部【外部】分桶表 / 内部【外部】分区分桶表）</li></ol></li></ul><p><code>内部表</code></p><pre><code class="SQL">-- create table hive_student_mng(id int, name string, sex string, age int, department string) -- row format delimited -- fields terminated by &quot;,&quot; lines terminated by &quot;\n&quot;;--建表常用参数create table if not exists init_partition(uuid string, viewnt INT comment &#39;view cnt&#39;)comment &#39;This is table init&#39;tblproperties(&#39;creator&#39;=&#39;Shin&#39;, &#39;created_at&#39;=&#39;2020-04-29 15:17:00&#39;, ...)partitioned by (dt sting, country string)   --分区字段不可出现在元数据内!!!--clustered by(uuid) sorted by(viewcnt) into XX buckets --分桶row format    格式化关键字    delimited         fields terminated by &quot;,&quot;    --字段与字段之间的分隔符：列        collection items terminated by &quot;,&quot;  --集合元素之间的分隔符：array(&#39;a&#39;,&#39;b&#39;)        map keys terminated by &#39;:&#39;  --map键值对之间分隔符：map(&#39;dt&#39;:&#39;2020-04-29&#39;, &#39;cnt&#39;:)        lines terminated by &quot;\n&quot;    --记录与记录之间的分隔符：行    serde   --这个参数忘记用途了            ...location &#39;/usr/local/src/apache-hive-1.2.2-bin/warehouse/init_partition&#39;;--查询表属性show tblproperties init_partition;describe init_partition;describe extended init_partition;--可读性更强、内容更详细的表属性describe formatted init_partition;--修改表属性alter table init_partition set TBLPROPERTIES(&#39;creator&#39;=&#39;Shin2&#39;);--从HDFS内导入load data inpath &#39;/data/hive_hdfs/student.txt&#39; into table hive_student_mng;--从本地导入load data local inpath &#39;/data/hive/student.txt&#39; into table hive_student_mng;</code></pre><p><code>Tips</code>从HDFS导入数据相当于mv，从local导入数据相当于cp </p><ul><li>管理表，即内部表，在数据共享方面表现的不够优异，即数据与元数据进行了捆绑，这其实是一种<strong>所有权</strong>的表现，为了削弱这种耦合性，我们可以使用<strong>外部表</strong>，即建立元数据，再指定数据所在位置，但并不”拥有”数据. 这样，Hive可以在这份数据上执行一些查询聚合，且数据的<strong>所有权</strong>不归于Hive所有.</li></ul><hr><p><code>外部表</code></p><pre><code class="SQL">create external table hive_student_ext(id int, name string, sex string, age int, department string) row format delimited fields terminated by &quot;,&quot; lines terminated by &quot;\n&quot; location &#39;/usr/local/src/apache-hive-1.2.2-bin/warehouse/hive_student_ext&#39;;--从HDFS内导入load data inpath &#39;/data/hive_hdfs/student.txt&#39; into table hive_student_ext;--从本地导入--load data local inpath &#39;/data/hive/student.txt&#39; into table hive_student_mng;</code></pre><p><code>内外转换</code></p><pre><code class="SQL">--内部转外部alter table hive_student_ext set TBLPROPERTIES(&#39;EXTERNAL&#39;=&#39;false&#39;);--外部转内部alter table hive_student_ext set TBLPROPERTIES(&#39;EXTERNAL&#39;=&#39;true&#39;);</code></pre><ul><li>一个由内外表转换引发的数据错乱问题（<strong>在默认未指定location的情况下</strong>，外部表在修改表名后，HDFS内的location名也不会随之变更，所以后续建立的同名同结构外部表，会重复使用这份数据，但是后续建立的如果是同名不同结构的外部表，则会报错）</li></ul><pre><code class="SQL">--外部表创建，且不指定外部表的路径，采用默认warehouse默认路径create external table tb1(id int, name string);insert into tb1 values(1, &#39;2&#39;);--此时，我们切换外部表表名alter table tb1 rename to tb2;select * from tb2;  --此时是可以获取数据的--再次建立同名同结构外部表create external table tb1(id int, name string);select * from tb1;  --这里一样可以获取，出现了数据复用，但是这种模式会有一些&quot;BUG&quot;--出现了数据共用的状态，</code></pre><p><code>明明首次建表却报错</code>如果重新建立的tb1结构发生变化，切未指定新的外部表路径，就会出现这种报错.</p><ul><li>回避这种问题的方法是，在重命名之前，先切换表的属性<br>```SQL<br>create external table tb1(id int, name string);<br>insert into tb1 values(1, ‘2’);</li></ul><p>–此时，我们切换外部表表名<br>alter table tb1 set TBLPROPERTIES(‘EXTERNAL’=’false’)<br>alter table tb1 rename to tb1_bak;<br>–这里的数据路径是否发生下下列变化，有待确认???<br>–/usr/local/src/apache-hive-1.2.2-bin/warehouse/tb1 -&gt; /usr/local/src/apache-hive-1.2.2-bin/warehouse/tb1_bak<br>select * from tb1_bak;  –这里是可以获取数据的！！！</p><p>–再次建立同名同结构外部表，一样，不指定location，使用默认的warehouse路径<br>create external table tb1(id int, name string);<br>select * from tb1;  –这里已经获取不到原先的数据了，因为原先的tb1再更名前，已转换为内部表，对应数据的路径名也随之更改</p><pre><code>---`CTAS````SQL--类Oraclecreate table hive_student_mng_2 as select * from hive_student_mng;</code></pre><hr><p><code>分区表</code></p><pre><code class="SQL">create table hive_student_ptn(id int, name string, sex string) partitioned by (age int, department string) row format delimited fields terminated by &quot;,&quot;;--本地数据至分区表时，需指定分区-- load data local inpath &quot;/home/hadoop/student.txt&quot; into table student_ptn partition(age=17,department=&quot;MA&quot;);--多重模式插入数据至分区表，同样需要指定分区，但是只需遍历一次源表from hive_student_mnginsert into table hive_student_ptn partition(age=17,department=&quot;MA&quot;) select id, name, sex where age = 17 and department=&quot;MA&quot;insert into table hive_student_ptn partition(age=18,department=&quot;CS&quot;) select id, name, sex where age = 18 and department=&quot;CS&quot;insert into table hive_student_ptn partition(age=18,department=&quot;IS&quot;) select id, name, sex where age = 18 and department=&quot;IS&quot;;</code></pre><p><code>静态分区表</code></p><pre><code class="SQL">create table hive_student_ptn_static(id int, name string, sex string) partitioned by (dt date) row format delimited fields terminated by &quot;,&quot;;--日期直接写程&quot;-&quot;分割的string即可，Hive引擎会自动试别并转换alter table hive_student_ptn_static add partition(dt=&#39;2020-02-14&#39;) partition(dt=&#39;2020-02-15&#39;);load data local inpath &quot;xx&quot; into table xxx (column=&quot;xx&quot;) partition(dt=&#39;2020-02-14&#39;)</code></pre><p><code>动态分区表</code></p><ul><li>数据根据分区字段，自动进行分区的创建和数据导入，<code>作为分区的字段在查询中需要按顺序放在最后</code>！</li><li>两个重要参数：<ol><li>set hive.exec.dynamic.partition=true;</li><li>set hive.exec.dynamic.partition.mode=nonstrict;  ⬅非严格动态分区模式，严格动态分区模式需要给分区表至少配置一个静态分区（避免用户误操作，导致无数个partition生成）</li><li>动态分区的一些限制：<br> set hive.exec.max.dynamic.partitions.pernode=100;   ⬅每个节点生成动态分区最大个数<br> set hive.exec.max.dynamic.partitions=1000;  ⬅生成动态分区最大个数，如果自动分区数大于这个参数，将会报错</li></ol></li></ul><pre><code class="SQL">--动态分区表插入数据create table hive_student_ptn_2(id int, name string, sex string) partitioned by (age int, department string) row format delimited fields terminated by &quot;,&quot;;insert into table hive_student_ptn_2 partition(department=&#39;xxx&#39;, age) select id, name, sex,  age from hive_student_mng; </code></pre><p><code>分桶表</code>（有疑问）</p><pre><code class="SQL">create table hive_student_bucket (id int, name string, sex string, age int, department string) clustered by (age) sort by (age desc, id asc) into 2 buckets row format delimited fields terminated by &quot;,&quot;;--虽然创建分桶表的时候指定了分桶字段和排序字段--但是数据到底有没有进行分桶和排序是根据插入数据的HQL决定的（这边还是不太理解）set mapreduce.job.reduces=3;insert into table hive_student_bucket select * from hive_student_mng distribute by age sort by age desc, id asc;--distribute by --方法1：导出，将分桶表导出，不设置mapreduce.job.reduces，默认为2个reduces，导出两个文件insert overwrite local directory &quot;/data/hive/hive_student_bucket_2/&quot; select * from hive_student_bucket;--放法2：导出，在源表的基础上使用分桶排序关键字做查询并导出，3个reduces，三个文件set mapreduce.job.reduces=3;insert overwrite local directory &quot;/data/hive/hive_student_bucket/&quot; select * from hive_student_mng distribute by age sort by age desc, id asc;--方法3：分桶表的基础上再做分桶，和方法2有点冗余，结果相同set mapreduce.job.reduces=3;insert overwrite local directory &quot;/data/hive/hive_student_bucket_4/&quot; select * from hive_student_bucket distribute by age sort by age desc, id asc;--cluster by distribute by age sort by age == cluster by age</code></pre><p><code>Tips</code>虽然cluster有简化分桶操作的概念，但是日常操作时还是建议使用<em>distribute by + sort by</em>.<br><code>Tips</code>导出分桶数据有两种方式，但是导出的数据文件状态不同：其一，利用方法1导出，表本身分为3桶（0/1/2），即便设置reduce数量，但仅能导出2个文件，3桶中的0和2被放置在一个文件中；其二，按照正常分桶逻辑，3桶导出3个文件（0/1/2）；其三，对分桶表做分桶导出，和方法2一样，也是导出3文件，但显得很啰嗦.<br><code>Tips</code>导出的文件含有隐藏分隔符^A（\x01，<strong>ctrl+V+A</strong>），需要用tab替换下，便于后续入库：<br>    sed -e ‘s/^A/\t/g’  ⬅查看<br>    sed -i ‘s/^A/\t/g’  ⬅修改</p><hr><h4 id="4-2-DQL"><a href="#4-2-DQL" class="headerlink" title="4-2 DQL"></a>4-2 DQL</h4><h5 id="支持-VS-不支持"><a href="#支持-VS-不支持" class="headerlink" title="支持 VS 不支持"></a>支持 VS 不支持</h5><ul><li>支持union all/ join/like/where/having/各种聚合/json解析</li><li>支持UDF/UDAF/UDTF</li><li>支持in/exists，但是Hive推荐使用semi join（半连接）</li><li>支持case when</li><li>支持truncate，和Oracle类似，只清数据，不删分区</li><li>不支持update/delete</li><li>不支持or条件</li><li>不支持非等值连接</li></ul><pre><code class="SQL">--半连接select a.* from a semi join b on a.id = b.id;</code></pre><img src="semi.png" srcset="/img/loading.gif" title="semi join."><hr><ul><li>下面是个字段切割的例子：Local数据导入，数据位于Master节点：/data/mr_wc/The_man_of_property.txt</li></ul><pre><code class="SQL">--databases listshow databases;OK+----------------+--+| database_name  |+----------------+--+| badou          || default        |+----------------+--+--查询库路径desc database badou;OK+----------+----------+----------------------------------------------------------------------------+-------------+-------------+-------------+--+| db_name  | comment  |                                  location                                  | owner_name  | owner_type  | parameters  |+----------+----------+----------------------------------------------------------------------------+-------------+-------------+-------------+--+| badou    |          | hdfs://master:9000/usr/local/src/apache-hive-1.2.2-bin/warehouse/badou.db  | root        | USER        |             |+----------+----------+----------------------------------------------------------------------------+-------------+-------------+-------------+--+1 row selected (0.225 seconds)use badou;OK--元数据建立create table article(sentence string) row format delimited fields terminated by &#39;\n&#39;; -- location /data/xxx --每一行的formatdesc article; --查询表结构--数据导入前，表为空select article.sentence from article limit 2; --本地数据load（Local Load）,注意这里是Linux服务器本地数据导入load data local inpath &#39;/data/mr_wc/The_man_of_property.txt&#39; into table article;--导入数据：--load data inpath &#39;/data/The_man_of_property.txt&#39; into table article; --导入 HDFS 数据：--load data local inpath &#39;/data/mr_wc/The_man_of_property.txt&#39; into table article; --导入本地数据：--再次查询select article.sentence from article limit 1;OK+-------------------+--+| article.sentence  |+-------------------+--+| Preface           |+-------------------+--+1 row selected (0.185 seconds)</code></pre><ul><li>HDFS数据目录<ol><li>入库的article.txt数据load进了HDFS对应目录</li><li>一句对应sentence列中的一行，现在要做Word Count，相当于将每行sentence切分为一个个单词，并转换进同一列（一个单词一行，所有单词并入同一列）</li></ol></li></ul><pre><code class="SQL">[root@master sbin]# hadoop fs -ls /usr/local/src/apache-hive-1.2.2-bin/warehouse/badou.db/article/Found 1 items-rwx-wx-wx   1 root supergroup     632207 2019-12-24 14:34 /usr/local/src/apache-hive-1.2.2-bin/warehouse/badou.db/article/operty.txt--单词按照空格切分，结果为数组格式，以下是样例演示--Hive里很特别，语句不限于SQL模式，可以将HQL的结果按照数据下标提取出来select &#39;the heroic and that there&#39; as origin, split(&#39;the heroic and that there&#39;,&#39; &#39;) as origin_list, split(&#39;the heroic and that there&#39;,&#39; &#39;)[0] as word1, split(&#39;the heroic and that there&#39;,&#39; &#39;)[1] as word2;+----------------------------+----------------------------------------+--------+---------+--+|           origin           |              origin_list               | word1  |  word2  |+----------------------------+----------------------------------------+--------+---------+--+| the heroic and that there  | [&quot;the&quot;,&quot;heroic&quot;,&quot;and&quot;,&quot;that&quot;,&quot;there&quot;]  | the    | heroic  |+----------------------------+----------------------------------------+--------+---------+--+--单行转多行select explode(split(&#39;the heroic and that there&#39;,&#39; &#39;)) origin_list_row;OK+------------------+--+| origin_list_row  |+------------------+--+| the              || heroic           || and              || that             || there            |+------------------+--+--wcselect origin_list_row, count(1) cn from(    select explode(split(&#39;the heroic and that there&#39;,&#39; &#39;)) origin_list_row)w --记得这里要加上子查询的别名，不然会出现以下错误group by origin_list_rowlimit 100;</code></pre><p>iii. 这里涉及了group by，会跑一组MR<br><img src="WC_MR.png" srcset="/img/loading.gif" alt="WC_MR." title="WC_MR."> </p><p><code>Tips</code>HQL内子查询的外侧要起别名，遗漏时会报错<br><img src="子查询别名.png" srcset="/img/loading.gif" alt="HQL子查询别名遗漏报错." title="HQL子查询别名遗漏报错."> </p><h5 id="正则过滤"><a href="#正则过滤" class="headerlink" title="正则过滤"></a>正则过滤</h5><pre><code class="SQL">--HQLselect regexp_extract(&#39;(mentioned_13^%sa&#39;, &#39;[[\\w]]+&#39;, 0);select regexp_extract(&#39;(mentioned_13^%sa&#39;, &#39;[[0-9a-zA-Z]]+&#39;, 0);--括号数量和最后一个入口参数idx要匹配，4个括号，最多4个idx，对应各个切分位select regexp_extract(&#39;12321&amp;22222&amp;33333&amp;44444&#39;, &#39;(.*?)(&amp;)(.*?)(&amp;)&#39;, 0);select regexp_extract(&#39;SHCMCC_SAS_2020022517062233_XXXXX&#39;, &#39;.*?_.*?_(.*?)(_)&#39;, 1);select regexp_extract(&#39;SHCMCC_SAS_2020022517062233_XXXXX&#39;, &#39;[\\d]+&#39;, 0);hive&gt; select regexp_extract(&#39;中国abc123!&#39;,&#39;[\\u4e00-\\u9fa5]+&#39;,0) from dual; //实用：只匹配中文hive&gt; select regexp_replace(&#39;中国abc123&#39;,&#39;[\\u4e00-\\u9fa5]+&#39;,&#39;&#39;) from dual; //实用：去掉中文--regexp_extractselect * from(    select regexp_extract(origin_list_row, &#39;[[0-9a-zA-Z]]+&#39;, 0) word, count(1) cn from    -- select origin_list_row, count(1) cn from    (        select explode(split(sentence,&#39; &#39;)) origin_list_row from article    )a --记得这里要加上子查询的别名，不然会出现以下错误    group by regexp_extract(origin_list_row, &#39;[[0-9a-zA-Z]]+&#39;, 0) --Jobs:1  Map: 1  Reduce: 1)bwhere length(word) &gt; 0 --去除None值order by cn desclimit 100; --加上排序后，Jobs:2  Map: 2  Reduce: 2--部分样例对比结果如下：⬇ RE过滤                       ⬇ 原始单词+-----------+-----+--+         +-----------------------+-----+--+|   word    | cn  |            |    origin_list_row    | cn  |+-----------+-----+--+         +-----------------------+-----+--+|           | 35  |            |                       | 35  || Baynes    | 1   |            | (Baynes               | 1   || Dartie    | 1   |            | (Dartie               | 1   || Dartie    | 1   |            | (Dartie’s             | 1   || Down      | 2   |            | (Down-by-the-starn)   | 2   || Down      | 1   |            | (Down-by-the-starn),  | 1   || He        | 1   |            | (He                   | 1   || I         | 1   |            | (I                    | 1   || James     | 1   |            | (James)               | 1   || L500      | 1   |            | (L500)                | 1   || Louisa    | 1   |            | (Louisa               | 1   || Mrs       | 1   |            | (Mrs.                 | 1   || Roger     | 1   |            | (Roger                | 1   || Roger     | 1   |            | (Roger’s              | 1   || Soames    | 1   |            | (Soames               | 1   || Soames    | 1   |            | (Soames)              | 1   || The       | 1   |            | (The                  | 1   || a         | 5   |            | (a                    | 5   || also      | 1   |            | (also                 | 1   || although  | 1   |            | (although             | 1   |+-----------+-----+--+         +-----------------------+-----+--+--where条件的添加，去除空值+-----------+-------+--+       +-----------+-------+--+|  b.word   | b.cn  |          |  b.word   | b.cn  |   +-----------+-------+--+       +-----------+-------+--+| the       | 5168  |          | the       | 5168  |   | of        | 3425  |          | of        | 3425  |   | to        | 2822  |          | to        | 2822  |   | and       | 2686  |          | and       | 2686  |   | a         | 2564  |          | a         | 2564  |   | he        | 2251  |          | he        | 2251  |   | his       | 1929  |          | his       | 1929  |   | in        | 1753  |          | in        | 1753  |   | was       | 1745  |          | was       | 1745  |   | had       | 1534  |          | had       | 1534  |   | that      | 1387  |          | that      | 1387  |   |           | 1309  |          | her       | 1200  |   | her       | 1200  |          | with      | 1037  |   | with      | 1037  |          | it        | 976   |   | it        | 976   |          | at        | 822   |   | at        | 822   |          | for       | 798   |   +-----------+-------+--+       +-----------+-------+--+</code></pre><hr><h4 id="4-3-分区-分桶（4-1中有建立过程，具体分析见0x）"><a href="#4-3-分区-分桶（4-1中有建立过程，具体分析见0x）" class="headerlink" title="4-3 分区/分桶（4.1中有建立过程，具体分析见0x）"></a>4-3 分区/分桶（4.1中有建立过程，具体分析见0x）</h4><hr><h3 id="05-数据类型"><a href="#05-数据类型" class="headerlink" title="05. 数据类型"></a>05. 数据类型</h3><h4 id="array"><a href="#array" class="headerlink" title="array"></a>array</h4><pre><code class="sql">create table hive_array(name string, citys array&lt;string&gt;)row format delimited fields terminated by &#39;\t&#39; collection items terminated by &#39;,&#39;;--数据样式：huangbo beijing,shanghai,tianjin,hangzhouload data local inpath &quot;/data/hive/array0217.txt&quot; into table hive_array;--查询select citys, citys[0] from hive_array;</code></pre><h4 id="map（dict）"><a href="#map（dict）" class="headerlink" title="map（dict）"></a>map（dict）</h4><pre><code class="sql">create table hive_map(name string, score map&lt;string, int&gt;)row format delimited fields terminated by &#39;\t&#39; collection items terminated by &#39;,&#39;map keys terminated by &#39;:&#39;;--数据样式：huangbo yuwen:80,shuxue:89,yingyu:95--实际工程中可以将map拆分成多个字段，便于使用--查询select citys, citys[&#39;yingyu&#39;] from hive_map;</code></pre><h4 id="struct"><a href="#struct" class="headerlink" title="struct"></a>struct</h4><pre><code class="sql">create table hive_struct(id int, score struct&lt;course:string, value:int&gt;)row format delimited fields terminated by &#39;\t&#39;collection items terminated by &#39;,&#39;;--数据样式：1 english,90--查询select t.score.course, t.score.value from hive_struct t;</code></pre><h4 id="视图"><a href="#视图" class="headerlink" title="视图"></a>视图</h4><pre><code class="sql">create view hive_student_view select department, count(1) from student group by department;</code></pre><hr><h3 id="06-多字节分隔符（分组正则表达式）"><a href="#06-多字节分隔符（分组正则表达式）" class="headerlink" title="06. 多字节分隔符（分组正则表达式）"></a>06. 多字节分隔符（分组正则表达式）</h3><ul><li>默认的SERDE只能支持单字节的分隔符，替换默认的SERDE，可以使其支持多字节分隔符.</li><li>RegexSerde正则分隔符解析，利用正则的规则来进行分组，每一组就是一个字段.</li></ul><pre><code class="bash">input.regex=&#39;(.*),,,(.*),,,(.*)&#39;output.format.string=&#39;%1$s %7$s %15$s ... %n$s&#39;</code></pre><pre><code class="sql">--查询desc formatted hive_ms;--万能替换01,,,huangbo02,,,xuzheng03,,,wangbaoqiangcreate table hive_ms(id string,name string)row format serde &#39;org.apache.hadoop.hive.serde2.RegexSerDe&#39;with serdeproperties(&#39;input.regex&#39;=&#39;(.*),,,(.*),,,(.*)&#39;,&#39;output.format.string&#39;=&#39;%1$s %2$s %3$s&#39;)stored as textfile;</code></pre><hr><h3 id="07-分析函数"><a href="#07-分析函数" class="headerlink" title="07. 分析函数"></a>07. 分析函数</h3><ul><li>样例数据：年月日+温度</li></ul><pre><code class="bash">20140101142014010216...20150106492015010722...20200109992020011023</code></pre><h4 id="row-number-over-partition-by-…-order-by-…"><a href="#row-number-over-partition-by-…-order-by-…" class="headerlink" title="row_number() over(partition by … order by …)"></a>row_number() over(partition by … order by …)</h4><p><code>row_number()</code>按照一定的字段，划分每组的rank（按顺序编号，不留空位），并求出每组的TopN</p><pre><code class="sql">--求出每一年的最高温度是那一天（日期， 最高温度）select * from (    select year, dt, temp,     row_number()over(partition by year order by temp) as seq     from hive_temp_year) t1 --按照年份划分，比如19年一组，20年一组，每组各自排序where t1.seq &lt;=3; --取出TopN</code></pre><h4 id="dense-rank-over-partition-by-…-order-by-…"><a href="#dense-rank-over-partition-by-…-order-by-…" class="headerlink" title="dense_rank() over(partition by … order by …)"></a>dense_rank() over(partition by … order by …)</h4><p><code>dense_rank()</code>按顺序编号，相同的值同编号，不留空位</p><pre><code class="sql">select * from (    select year, dt, temp,     dense_rank()over(partition by year order by temp) as seq     from hive_temp_year) t1 --按照年份划分，比如19年一组，20年一组，每组各自排序where t1.seq &lt;=3; --取出TopN</code></pre><h4 id="rank-over-partition-by-…-order-by-…"><a href="#rank-over-partition-by-…-order-by-…" class="headerlink" title="rank() over(partition by … order by …)"></a>rank() over(partition by … order by …)</h4><p><code>rank()</code>按顺序编号，相同的值同编号，留出空位</p><pre><code class="sql">select * from (    select year, dt, temp,     rank()over(partition by year order by temp) as seq     from hive_temp_year) t1 --按照年份划分，比如19年一组，20年一组，每组各自排序where t1.seq &lt;=3; --取出TopN</code></pre><hr><h3 id="08-窗口函数"><a href="#08-窗口函数" class="headerlink" title="08. 窗口函数"></a>08. 窗口函数</h3><h4 id="sum-avg-max-min-over-partition-by-…-rows-between-…"><a href="#sum-avg-max-min-over-partition-by-…-rows-between-…" class="headerlink" title="sum/avg/max/min() over(partition by … rows between …)"></a>sum/avg/max/min() over(partition by … rows between …)</h4><ul><li>用户/时间/点击次数</li></ul><pre><code class="bash">cookie1,2015-04-10,1cookie1,2015-04-11,5cookie1,2015-04-12,7cookie1,2015-04-13,3cookie1,2015-04-14,2cookie1,2015-04-15,4cookie1,2015-04-16,4</code></pre><ul><li>窗口函数上下边界<ol><li>默认没有给范围，那就是该partition内的第一条到当前这条：rows between unbounded preceding and current row</li><li>也可以给范围：rows between A and B<br> unbounded preceding  ⬅ 窗口最开始<br> 3 preceding     ⬅ 往前3行<br> 1 following     ⬅ 往后1行<br> unbounded following  ⬅ 到最后一行</li></ol></li></ul><pre><code class="sql">select cookieid,createtime,pv,    sum(pv) over(partition by cookieid order by createtime) as pv1, -- 默认为从起点到当前行    sum(pv) over(partition by cookieid order by createtime rows between unbounded preceding and current row) as pv2,    --从起点到当前行，结果同pv1    sum(pv) over(partition by cookieid) as pv3, --分组内所有行    sum(pv) over(partition by cookieid order by createtime rows between 3 preceding and current row) as pv4,    --当前行+往前3行    sum(pv) over(partition by cookieid order by createtime rows between 3 preceding and 1 following) as pv5,    --当前行+往前3行+往后1行    sum(pv) over(partition by cookieid order by createtime rows between current row and unbounded following) as pv6 --当前行+往后所有行from cookie;</code></pre><hr><h3 id="09-explode-amp-LATERAL-VIEW"><a href="#09-explode-amp-LATERAL-VIEW" class="headerlink" title="09. explode &amp; LATERAL VIEW"></a>09. explode &amp; LATERAL VIEW</h3><h4 id="爆炸函数"><a href="#爆炸函数" class="headerlink" title="爆炸函数"></a>爆炸函数</h4><p><code>explode</code>将数组转成多行一列：”a-b-c”；将字典转成多行多列：{name:ghr,score:85}<br><code>lateral view</code>虚拟表，主要配合<strong>explode</strong>和<strong>split</strong>，将单列的数组或是字段拆分多行，并能和其余字段关联.</p><ul><li>字典的炸裂和数组炸裂的写法，重命名部分有点不太一样</li><li>数组炸裂</li></ul><pre><code class="sql">--原始数据select * from hive_explode;+------------------+--------------------+-------------------+----------------------+--+| hive_explode.id  | hive_explode.name  | hive_explode.age  | hive_explode.favors  |+------------------+--------------------+-------------------+----------------------+--+| 5                | huge               | 39                | e-f-d                || 6                | liuyifei           | 35                | a-d-e                |+------------------+--------------------+-------------------+----------------------+--+--爆炸拼接select id, name, favors_new from hive_explode lateral view explode(split(favors,&#39;-&#39;)) e as favors_new; --这里和字典部分的炸裂处理略有不同+-----+----------------+-------------+--+| id  |      name      | favors_new  |+-----+----------------+-------------+--+| 5   | huge           | e           || 5   | huge           | f           || 5   | huge           | d           || 6   | liuyifei       | a           || 6   | liuyifei       | d           || 6   | liuyifei       | e           |+-----+----------------+-------------+--+20 rows selected (0.174 seconds)</code></pre><img src="数组炸裂.png" srcset="/img/loading.gif" title="数组炸裂."><ul><li>字典炸裂<pre><code class="sql">select dd.key as object, dd.value as ratio, round(1-dd.value, 2) as anti_ratio from(  select class_id,  map(&#39;math&#39;, round(avg(case when math_score&gt;=60 then 1 else 0 end),2),  &#39;chinese&#39;, round(avg(case when chinese_score&gt;=60 then 1 else 0 end),2)  ) as object  from class_tmp group by class_id)t lateral view explode(object) dd; --这里和数组部分的炸裂处理略有不同</code></pre><img src="字典炸裂.png" srcset="/img/loading.gif" title="字典炸裂."><img src="字典炸裂2.png" srcset="/img/loading.gif" title="字典炸裂2."></li></ul><hr><h3 id="0x-分桶"><a href="#0x-分桶" class="headerlink" title="0x. 分桶"></a>0x. 分桶</h3><h4 id="有点绕的概念（处理大表和大表的关联时很有用）"><a href="#有点绕的概念（处理大表和大表的关联时很有用）" class="headerlink" title="有点绕的概念（处理大表和大表的关联时很有用）"></a>有点绕的概念（处理大表和大表的关联时很有用）</h4><p><code>语法</code> tablesample(bucket x out of y on id)</p><pre><code class="SQL">TABLESAMPLE (BUCKET x OUT OF y [ON colname])</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>Hive</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20200225_1220_Python3-Django（一个简单的数据操作应用）</title>
    <link href="/2020/02/25/20200225-1220-Python3-Django%EF%BC%88%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C%E5%BA%94%E7%94%A8%EF%BC%89/"/>
    <url>/2020/02/25/20200225-1220-Python3-Django%EF%BC%88%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C%E5%BA%94%E7%94%A8%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h2 id="Python3-Django-2-0-6"><a href="#Python3-Django-2-0-6" class="headerlink" title="Python3 Django-2.0.6"></a>Python3 Django-2.0.6</h2><ul><li>Repo Point<br><a href="">Python 3.5.2</a><br><a href="">Django 2.0.6</a></li></ul><hr><h3 id="01-视图"><a href="#01-视图" class="headerlink" title="01. 视图"></a>01. 视图</h3><ul><li>这里仅搭建和实验最小化的Django应用，测试专用.</li></ul><pre><code class="bash">pip install Django==2.0.6cd $Django_HOME/code/django-admin --help     ⬅ 帮助查询django-admin startproject mysitepython manage.py help     ⬅ 帮助查询# 服务启动python manage.py runserver# 创建应用cd /home/shiyanlou/Code/mysitepython3 manage.py startapp lib# 编辑视图vim /home/shiyanlou/Code/mysite/lib/views.py    ```python    from django.shortcuts import render    from django.http import HttpResponse    def index(request):        return HttpResponse(&quot;Hello, world!&quot;)    ```# 下面这一步我们将应用lib里视图函数与URL映射到了一起.vim /home/shiyanlou/Code/mysite/lib/urls.py    ```python    from django.urls import path    from . import views    urlpatterns = [        path(route=&#39;&#39;, view=views.index, name=&#39;index&#39;),    ]    ```# 在上面的代码中，route为空意味着我们可以直接用链接http://localhost:8000/lib/访问该视图函数，view=view.index是调用了view.py中的index视图，name为index代表我们可以在模板中用index来引用返回的变量.vim /home/shiyanlou/Code/mysite/mysite/urls.py    ```python    from django.contrib import admin    from django.urls import include, path    urlpatterns = [        path(&#39;lib/&#39;, include(&#39;lib.urls&#39;)),        path(&#39;admin/&#39;, admin.site.urls),    ]    ```# 接下来，我们需要向mysite/mysite/urls.py告知使用应用lib的视图.实现的原理就是使用了include()函数，它允许应用其他的URLconfs.# 比如使用这个地址进行请求时：http://localhost:8000/lib/，首先在mysite/mysite/urls.py中会截断与此项匹配的 URL 部分，也就是lib/，然后将剩余的字符串发送到 URLconf 以供进一步处理.# 到这里，我们就把index视图添加进了URLconf。</code></pre><img src="mysite_tree.png" srcset="/img/loading.gif" title="Django工程结构"><h3 id="02-模型"><a href="#02-模型" class="headerlink" title="02.模型"></a>02.模型</h3><ul><li>实验分为4步，配置数据、创建模型、激活模型和使用API.</li><li>这里的数据模型感觉像是编辑数据库内的元数据创建语句，激活便是生效语句，最后使用Django API执行查询操作.</li></ul><h4 id="创建模型"><a href="#创建模型" class="headerlink" title="创建模型"></a>创建模型</h4><pre><code class="bash">vim /home/shiyanlou/Code/mysite/mysite/settings.pyTIME_ZONE = &#39;Asia/Shanghai&#39;vim /home/shiyanlou/Code/mysite/lib/models.py    ```python    from django.db import models    class Book(models.Model):        name = models.CharField(max_length=200)        author = models.CharField(max_length=100)        pub_house = models.CharField(max_length=200)        pub_date = models.DateTimeField(&#39;date published&#39;)    ```# 从代码可以看出，模型是django.db.models.Model类的子类。每个模型有一些类变量，它们都表示模型里的一个数据库字段.# 每个字段都是Field类的实例。比如字符字段是CharField，日期字段被表示为DateTImeField。这将告诉Django每个字段要处理的数据类型.</code></pre><h4 id="激活模型"><a href="#激活模型" class="headerlink" title="激活模型"></a>激活模型</h4><ul><li>通过运行<code>makemigrations</code>命令，Django会检测你对模型文件的修改，并且把修改的部分储存为一次<code>迁移</code>.</li></ul><pre><code class="bash">vim /home/shiyanlou/Code/mysite/lib/apps.py    ```python    INSTALLED_APPS = [        &#39;lib.apps.LibConfig&#39;,   ⬅ 追加LibConfig类        &#39;django.contrib.admin&#39;,        ...    ]    ```# 现在项目中会包含lib应用，运行以下命令python manage.py makemigrations lib     ⬅ 创建迁移语句# 运行迁移命令，看会执行哪些SQLpython manage.py sqlmigrate lib 0001# 现在运行migrate命令，在数据库里创建新定义的模型的数据表python manage.py migrate # 使用APIpython manage.py shell# 类似于Jupyter的shell界面&gt;&gt;&gt;from lib.models import Book&gt;&gt;&gt;Book.objects.all()   #获取Book所有对象&lt;QuerySet []&gt;&gt;&gt;&gt;from django.utils import timezone&gt;&gt;&gt;b = Book(name=&#39;Business&#39;, author=&#39;Tom&#39;, pub_house=&#39;First Press&#39;, pub_date=timezone.now())    #创建&gt;&gt;&gt;b.save() #保存&gt;&gt;&gt;b.id1&gt;&gt;&gt;b.name&#39;Business&#39;&gt;&gt;&gt;b.pub_datedatetime.datetime(2018, 7, 4, 2, 29, 7, 578323, tzinfo=&lt;UTC&gt;)# 使用这个命令而不是简单的使用 &quot;Python&quot; 是因为 manage.py 会设置 DJANGO_SETTINGS_MODULE 环境变量，这个变量会让 Django 根据 mysite/settings.py 文件来设置 Python 包的导入路径.</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>Django</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20200123_2103_Python3-Bokeh（0.13.0）</title>
    <link href="/2020/01/23/20200113-1424-Bokeh/"/>
    <url>/2020/01/23/20200113-1424-Bokeh/</url>
    
    <content type="html"><![CDATA[<h2 id="Python3-visualization-bokeh"><a href="#Python3-visualization-bokeh" class="headerlink" title="Python3 visualization_bokeh"></a>Python3 visualization_bokeh</h2><h3 id="服务器数据清单可视化"><a href="#服务器数据清单可视化" class="headerlink" title="服务器数据清单可视化"></a>服务器数据清单可视化</h3><p><code>Tips</code>这里的bokeh模块版本为0.13.0，最新的1.4.0始终过不了cmd调试.</p><ul><li><code>Repo Point</code><br>  pandas, shutil, sys,<br>  bokeh.plotting.output_file<br>  bokeh.plotting.figure<br>  bokeh.plotting.show<br>  bokeh.models.ColumnDataSource</li></ul><ul><li><p><code>Self Repo</code><br>  func_demo.func_f.date_f<br>  src.conf.bas_mail_conf<br>  src.conf.bokeh_conf</p></li><li><p><code>更新日志</code><br>  修改方法：visualization_bokeh.df_html_bokeh，可进行图例隐藏</p></li></ul><hr><h4 id="Tree"><a href="#Tree" class="headerlink" title="Tree"></a>Tree</h4><pre><code class="bash">src|-- __init__.py|-- conf|   |-- bas_insert_conf.py|   |-- bas_mail_conf.py|   |-- bokeh_conf.py|   `-- sql_conf.py|-- data|   |-- 20191212_GATHER.csv|   `-- 20200217_GATHER.csv|-- data_output|   |-- 20200112_log_lines.html|   `-- 20200116_log_lines.html|-- func_demo|   |-- __init__.py|   |-- ftp_wc.py|   `-- func_f.py|-- visualization_bokeh.py`-- main.py</code></pre><hr><h4 id="main-py"><a href="#main-py" class="headerlink" title="main.py"></a>main.py</h4><ul><li>可视化工具入口：数据准备 &amp; 数据可视化</li></ul><pre><code class="python"># -*- coding:utf-8 -*-# @Author: Shin# @Date: 20/1/16 18:05# @File: main.py# @Usage: Mainimport os# import MySQLdb# Self Repofrom visualization_bokeh import *from func_demo.ftp_wc import re_matchif __name__ == &#39;__main__&#39;:    # print(f&#39;***FILE COPY***&#39;)    # data_path = None    # for path in os.walk(&#39;./data/&#39;):    #     print(path[2])    #     data_path = path[2]    # re_results = re_match(data_path, date_f()[0])    # if re_results:    #     print(f&#39;Status: File {re_results} already existed...&#39;)    # else:    #     print(f&#39;Status: Target file [{date_f()[0]}_GATHER.csv] copy from path [{&quot;../../Python_service_daily/src/data_output/&quot;}]\n&#39;)    #     file_copy_f(f&#39;../../Python_service_daily/src/data_output/{date_f()[0]}_GATHER.csv&#39;, f&#39;./data/&#39;)    df_list_1 = df_preparation(data_input_path=bokeh_conf.data_source,                               cols=bokeh_conf.cols,                               cols_rename_dic=bokeh_conf.cols_rename_dic,                               # html_output_path=bokeh_conf.html_output_path,                               info_filter=bokeh_conf.info_filter,                               index_col=&#39;HOUR&#39;)    # &lt;class &#39;bokeh.core.property.containers.PropertyValueColumnData&#39;&gt;    # df_preparation 目前输出的输出格式仅包含：x,y,flag    # print(f&#39;df_list_1: {df_list_1}&#39;)    # print(f&#39;\ndf_list_1[0].data: {df_list_1[0].data}\n&#39;)    tools = [        (&quot;FILE_SIZE&quot;, &quot;@{y}{0.2f}MB&quot;),        (&quot;HOUR&quot;, &quot;@x&quot;),        (&quot;FILE_STYLE&quot;, &quot;@flag&quot;)    ]    kwargs_axis = {&#39;x_axis_label&#39;: &#39;HOUR&#39;,                   &#39;y_axis_label&#39;: &#39;FILE_SIZE&#39;,                   &#39;color&#39;: bokeh_conf.info_filter[&#39;3&#39;],                   }    print(f&#39;Current data source: {bokeh_conf.data_source}\n&#39;)    print(f&#39;Total legend number: {len(df_list_1)}\nProcessing Begin...&#39;)    df_html_bokeh(html_output_path=bokeh_conf.html_output_path, title=&#39;Daily Check&#39;,                  tooltips=tools, source=df_list_1, **kwargs_axis)    # final_fig = None    # for rs in df_list_1:    #     final_fig = df_html_bokeh(title=&#39;Daily Check&#39;,tooltips=tools, source=rs)    ######</code></pre><hr><h4 id="visualization-bokeh-py"><a href="#visualization-bokeh-py" class="headerlink" title="visualization_bokeh.py"></a>visualization_bokeh.py</h4><ul><li>bokeh方法封装<br>  <code>visualization_bokeh.df_2_list</code> DataFrame转list，便于后续数据呈现前的装载.<br>  <code>visualization_bokeh.source_dict</code> DataFrame单列转dict？？？<br>  <code>visualization_bokeh.df_preparation</code> 数据源/过滤条件等内容加载.<br>  <code>visualization_bokeh.df_html_bokeh</code> HTML趋势图配置 </li></ul><pre><code class="python"># -*- coding:utf-8 -*-# @Author: Shin# @Date: 19/11/26 10:44# @File: visualization_bokeh.pyfrom bokeh.plotting import output_file, figure, showfrom bokeh.models import ColumnDataSourceimport pandas as pdfrom sys import exc_info# from pandas import DataFrame# import numpy as np# from sys import exc_info# from shutil import copy# Self Repo# from src.func_demo.ftp_wc import ftp_connectfrom func_demo.func_f import date_f, file_copy_f    # , file_rm_single# from src.conf import bas_mail_conffrom conf import bokeh_confdef df_2_list(data_df, column, value=None):    &quot;&quot;&quot;    单列    :param data_df: DataFrame &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;    :param column:  string  过滤所用字段    :param value:   tuple   过滤条件    :return: &lt;class list&gt; 列表内每一个对象都是一个DataFrame    &quot;&quot;&quot;    df_list = []    try:        if value is not None:            for r1 in value:                # 按条件将DF分组，并存入list                df_temp = data_df[data_df[column] == r1]   # df_temp = data(data[&#39;STYLE&#39;] == &#39;NSN_4G_PM&#39;)                # print(df_temp)                df_list.append(df_temp)        else:            # 这里需要切分            series = data_df.groupby([column], as_index=False).count()            df_list = [data_df[data_df[column] == r2] for r2 in series[column]]            # df_list.append(data_df)        return df_list    except Exception as e:        print(&#39;Status: File write error!&#39;)        print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)        print(&#39;------------------&#39; * 2)        return 1def source_dict(*args):    data_dict = dict(        x=args[0],        y=args[1],        flag=args[2]    )    return data_dictdef df_preparation(data_input_path, cols, cols_rename_dic, info_filter, index_col=None):    &quot;&quot;&quot;    :param data_input_path: 数据路径    :param cols: 数据字段        Ex: [&#39;DATA_STYLE&#39;, &#39;HOUR&#39;, &#39;NORMAL_FILE_SIZE/MB&#39;, &#39;NOW_SIZE/MB&#39;]    :param cols_rename_dic: 字段重命名        Ex: {&#39;DATA_STYLE&#39;: &#39;STYLE&#39;, &#39;NORMAL_FILE_SIZE/MB&#39;: &#39;NORMAL&#39;, &#39;NOW_SIZE/MB&#39;: &#39;NOW&#39;}    :param info_filter: 数据切片配置字典，按照示例排列字典 k,v        Ex: {   # 1 筛选字段                &#39;column_flag&#39;: &#39;STYLE&#39;,                # 2 筛选值                &#39;owner_flag&#39;: ( &#39;NSN_4G_PM&#39;, &#39;HW_4G_PM&lt;OMC1&gt;&#39;, &#39;HW_4G_PM&lt;OMC2&gt;&#39;,                                &#39;NSN_4G_CM&#39;, &#39;HW_4G_CM&lt;OMC1&gt;&#39;, &#39;HW_4G_CM&lt;OMC2&gt;&#39;),                # 3                &#39;owner_color&#39;: (&#39;RED&#39;, &#39;ORANGE&#39;, &#39;SKYBLUE&#39;, &#39;GREEN&#39;, &#39;CHOCOLATE&#39;, &#39;BLUEVIOLET&#39;),                # 4 根据自己需要展示的Y轴字段名来配置                &#39;y&#39;: &#39;NOW&#39;}    :param index_col: 数据索引字段        Ex: [&#39;HOUR&#39;]    :return: DataFrame for HTML Bokeh   &lt;class &#39;bokeh.models.sources.ColumnDataSource&#39;&gt;    &quot;&quot;&quot;    print(f&#39;***HTML DATA PRE***&#39;)    # Initialization    df_owner = []    # Data load    try:        data_df = pd.read_csv(data_input_path, usecols=cols, index_col=index_col, encoding=&#39;GBK&#39;)        if cols_rename_dic is not None:            # 这部分的写法其实就是键值对的方式，入参 cols_rename_dic 为修改前后的键值对字典            data_df_rename = data_df.rename(columns=cols_rename_dic)        else:            data_df_rename = data_df        del data_df        print(&#39;Status: Data load Successfully.&#39;, &#39;\n&#39;)        # print(f&#39;data_df_rename: {data_df_rename}&#39;)    except IOError as e:        # print(&#39;Status: Data load failure: %s&#39; % e)        print(&#39;Status: Data load failure...&#39;)        print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)        print(&#39;------------------&#39; * 2)        exit(1)    else:        # Data Filter        # bokeh_conf.info_filter 序列加载        k = []        dict_range = len(info_filter)   # Ex: len(info_filter) = 4        for i in range(dict_range):            k.append(sorted(info_filter.keys())[i]) # 这里添加排序是为了标签 load 方便            # Ex:            # k1 = sorted(info_filter.keys())[0]            # k2 = sorted(info_filter.keys())[1]            # ...        # print(k)        # # 定义筛选列        # Ex:        # column_flag = &#39;STYLE&#39;        # # 定义筛选值        # owner_flag = (&#39;NSN_4G_PM&#39;, &#39;HW_4G_PM&lt;OMC1&gt;&#39;, &#39;HW_4G_PM&lt;OMC2&gt;&#39;,        #               &#39;NSN_4G_CM&#39;, &#39;HW_4G_CM&lt;OMC1&gt;&#39;, &#39;HW_4G_CM&lt;OMC2&gt;&#39;)        # owner_color = (&#39;RED&#39;, &#39;ORANGE&#39;, &#39;SKYBLUE&#39;, &#39;GREEN&#39;, &#39;CHOCOLATE&#39;, &#39;BLUEVIOLET&#39;)        # # 数据筛选        # df_filter_1 = df_data_frame(data_df=df_rename, column=&#39;STYLE&#39;, loc_flag=owner_flag)     # &lt;class &#39;list&#39;&gt;        # print(info_filter[k[0]], info_filter[k[1]])        # 过滤        data_df_filter = df_2_list(data_df=data_df_rename,      # 字段重命名后的Dataframe                                   column=info_filter[k[0]],    # info_filter[&#39;1&#39;]                                   value=info_filter[k[1]])     # info_filter[&#39;2&#39;]        # print(data_df_filter[0])        source_dict_flag = info_filter[k[0]]    # Ex: &#39;STYLE&#39;        source_dict_y = info_filter[k[-1] ]  # Ex: &#39;NOW&#39;        # 列表封装各线图源数据        for rs in data_df_filter:            df_tmp = ColumnDataSource(data=source_dict(rs.index, rs[source_dict_y], rs[source_dict_flag]))            # &lt;class &#39;bokeh.models.sources.ColumnDataSource&#39;&gt; == dict            # Ex: df_tmp = ColumnDataSource(data=source_dict(rs.index, rs[&#39;NOW&#39;], rs[&#39;STYLE&#39;]))            # str_flag = rs[source_dict_flag].loc[0]   # 仅取一行作为这一类的标签，不然后续判断时会报错            # Ex: str_flag = rs[&#39;STYLE&#39;].loc[0]            # print(rs[&#39;STYLE&#39;].loc[[0]]) # 等价于 print(rs[&#39;STYLE&#39;].[0:1]) 含索引列，返回的是一个            df_owner.append(df_tmp)        # print(df_owner[1].data)    return df_ownerdef df_html_bokeh(html_output_path, title, tooltips, source, plot_width=1300, plot_height=674, **kwargs):    &quot;&quot;&quot;    :param html_output_path: HTML文件生成路径    :param title: HTML标题    :param tooltips: Bokeh工具配置    :param source: pd.read_csv ---&gt; type: &lt;class &#39;bokeh.models.sources.ColumnDataSource&#39;&gt;    :param plot_width: 图表宽度    :param plot_height: 图表高度    :param kwargs: 其余图形配置参数    :return: 0 or 1    &quot;&quot;&quot;    print(f&#39;***HTML OUTPUT***&#39;)    try:        # Output Configuration        output_file(html_output_path)        # Ex:        # tools = [        #     (&quot;FILE_SIZE&quot;, &quot;@{y}{0.2f}MB&quot;),        #     (&quot;HOUR&quot;, &quot;@x&quot;),        #     (&quot;FILE_STYLE&quot;, &quot;@flag&quot;)        # ]        # HTML Figures Display Configuration        p = figure(            tooltips=tooltips,  # [(&quot;x&quot;, &quot;$x&quot;), (&quot;y&quot;, &quot;$y&quot;)],            plot_width=plot_width,            plot_height=plot_height,            title=title,    # &#39;Gather Glyphs&#39;,            x_axis_label=kwargs[&#39;x_axis_label&#39;],    # &#39;HOUR&#39;,            y_axis_label=kwargs[&#39;y_axis_label&#39;],    # &#39;FILE_SIZE&#39;,        )        p.x_range.range_padding = p.y_range.range_padding        i = 0   # 计数器        # print(type(source)) # &lt;class &#39;list&#39;&gt;        for rs in source:            # Ex:            # loc:  按索引标签选取            # iloc: 按索引位置选取（iloc做法）            # 未指定索引字段时，索引编号恰巧和数据索引标签相同(0,1,2,...)，故两者的效果类似，但实质不同            # print(f&#39;rs.date[&quot;flag&quot;]: {rs.data[&quot;flag&quot;]}\n&#39;            #       f&#39;rs.date[&quot;flag&quot;],loc[0]: {rs.data[&quot;flag&quot;].loc[0]}\n&#39;            #       f&#39;rs.date[&quot;flag&quot;],iloc[0]: {rs.data[&quot;flag&quot;].iloc[0]}\n&#39;)            # for key, value in kwargs_axis.items():            #     print(&#39;key:{}, value:{}&#39;.format(key, value))            #     print(f&#39;key:{key}, value:{value}&#39;)            if &#39;color&#39; not in kwargs.keys():                print(&#39;No color in configuration.&#39;, &#39;\n&#39;)                p.line(x=&#39;x&#39;, y=&#39;y&#39;,legend=rs.data[&#39;flag&#39;].loc[0], source=rs)                # p.line(x=kwargs[&#39;x&#39;], y=kwargs[&#39;y&#39;], legend=kwargs[&#39;legend&#39;], line_width=kwargs[&#39;line_width&#39;], source=source)                p.circle(x=&#39;x&#39;, y=&#39;y&#39;,legend=rs.data[&#39;flag&#39;].loc[0], size=10, source=rs)                p.legend.click_policy = &quot;mute&quot;                # p.circle(x=kwargs[&#39;x&#39;], y=kwargs[&#39;y&#39;], legend=kwargs[&#39;legend&#39;], size=10, source=source)            else:                print(f&#39;Bokeh legend-{i} processing...&#39;)                p.line(x=&#39;x&#39;, y=&#39;y&#39;, legend=rs.data[&#39;flag&#39;].loc[0], line_width=3,   # kwargs[&#39;line_width&#39;],                       color=kwargs[&#39;color&#39;][i], source=rs,                       muted_color=kwargs[&#39;color&#39;][i], muted_alpha=0.2)                p.circle(x=&#39;x&#39;, y=&#39;y&#39;, legend=rs.data[&#39;flag&#39;].loc[0],                         size=10,   # kwargs[&#39;size&#39;],                         fill_color=kwargs[&#39;color&#39;][i], line_color=kwargs[&#39;color&#39;][i], source=rs,                         muted_color=kwargs[&#39;color&#39;][i], muted_alpha=0.2)                # 图例交互                # p.legend.location = &quot;top_left&quot;                p.legend.click_policy = &quot;mute&quot;            i += 1        # HTML结果自动呈现        show(p)        return 0    except Exception as e:        print(&#39;Status: Failed to processed HTML file!&#39;)        print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)        print(&#39;------------------&#39; * 2)        return 1# Demo# if __name__ == &#39;__main__&#39;:#     # # sys.path.append(&quot;.&quot;)#     # print(sys.path)##     # 数据帧测试#     # source = pd.read_csv(bas_mail_conf.data_source, usecols=bokeh_conf.cols, index_col=&#39;HOUR&#39;, encoding=&#39;GBK&#39;)#     # print(source) # &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;#     # print(type(source))#     ##     # frame = DataFrame(source)#     # print(frame)#     # print(type(frame)) # &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;###     # file_copy_f(f&#39;../../Python_service_daily/src/data_output/{date_f()[0]}_GATHER.csv&#39;, f&#39;./data/&#39;)#     # file_copy_f(f&#39;../../Python_service_daily/src/data_output/{date_f()[0]}_GATHER.csv&#39;, r&#39;D:\FTP\Mail&#39;)##     #######     # info_filter = {&#39;1&#39;: &#39;STYLE&#39;,    # 1 筛选所用字段 column_flag#     #                &#39;2&#39;: (&#39;NSN_4G_PM&#39;, &#39;HW_4G_PM&lt;OMC1&gt;&#39;, &#39;HW_4G_PM&lt;OMC2&gt;&#39;, &#39;NSN_4G_CM&#39;,#     #                      &#39;HW_4G_CM&lt;OMC1&gt;&#39;, &#39;HW_4G_CM&lt;OMC2&gt;&#39;),   # 2 过滤条件 owner_flag#     #                &#39;3&#39;: (&#39;RED&#39;, &#39;ORANGE&#39;, &#39;SKYBLUE&#39;, &#39;GREEN&#39;, &#39;CHOCOLATE&#39;, &#39;BLUEVIOLET&#39;),   # 3 owner_color#     #                &#39;4&#39;: &#39;NOW&#39;}  # 4 根据自己需要展示的Y轴字段名来配置 y轴值##     df_list_1 = df_preparation(data_input_path=bokeh_conf.data_source,#                                cols=bokeh_conf.cols,#                                cols_rename_dic=bokeh_conf.cols_rename_dic,#                                # html_output_path=bokeh_conf.html_output_path,#                                info_filter=bokeh_conf.info_filter,#                                index_col=&#39;HOUR&#39;)#     # &lt;class &#39;bokeh.core.property.containers.PropertyValueColumnData&#39;&gt;#     # df_preparation 目前输出的输出格式仅包含：x,y,flag#     # print(df_list_1, &#39;\n&#39;, df_list_1[0].data)#     # print(df_list_1[0].data)##     tools = [#         (&quot;FILE_SIZE&quot;, &quot;@{y}{0.2f}MB&quot;),#         (&quot;HOUR&quot;, &quot;@x&quot;),#         (&quot;FILE_STYLE&quot;, &quot;@flag&quot;)#     ]##     kwargs_axis = {&#39;x_axis_label&#39;: &#39;HOUR&#39;,#                    &#39;y_axis_label&#39;: &#39;FILE_SIZE&#39;,#                    &#39;color&#39;: bokeh_conf.info_filter[&#39;3&#39;],#                    }##     df_html_bokeh(html_output_path=bokeh_conf.html_output_path, title=&#39;Daily Check&#39;,#                   tooltips=tools, source=df_list_1, **kwargs_axis)##     # final_fig = None#     # for rs in df_list_1:#     #     final_fig = df_html_bokeh(title=&#39;Daily Check&#39;,tooltips=tools, source=rs)##     ######</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>Python3</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20200115_1339_Python3-FTP-list（Ver.2）</title>
    <link href="/2020/01/15/20200115-1339-Python3-FTP-list/"/>
    <url>/2020/01/15/20200115-1339-Python3-FTP-list/</url>
    
    <content type="html"><![CDATA[<h2 id="Python3-ftp-monitor"><a href="#Python3-ftp-monitor" class="headerlink" title="Python3 ftp_monitor"></a>Python3 ftp_monitor</h2><h3 id="服务器数据清单采集（暂时不含可视化部分）"><a href="#服务器数据清单采集（暂时不含可视化部分）" class="headerlink" title="服务器数据清单采集（暂时不含可视化部分）"></a>服务器数据清单采集（暂时不含可视化部分）</h3><ul><li><p><code>Repo Point</code><br>  ftplib, socket, os, re</p></li><li><p><code>Self Repo</code><br>  Oracle2File.FileWR（class_email_daily3.py）<br>  func_demo.func_f.date_f<br>  ftp_conf_bak</p></li></ul><hr><h4 id="Tree"><a href="#Tree" class="headerlink" title="Tree"></a>Tree</h4><pre><code class="bash"># tree ftp_monitor/src -L 2$ tree src -L 3src|-- __init__.py|-- conf|   |-- __init__.py|   |-- ftp_conf.py|   `-- ftp_conf_bak.py|-- data_output|-- ftp_wc.py|-- func_demo|   |-- Oracle2File.py|   |-- __init__.py|   -- func_f.py|   `-- os_f.py|-- logs|-- main.py|-- main_re.py`-- start-all.sh ⬅ 脚本启动入口</code></pre><hr><h4 id="main-py"><a href="#main-py" class="headerlink" title="main.py"></a>main.py</h4><ul><li>清单采集程序入口程序<br>  <code>ftp_job</code> FTP任务启动/清单文件生成，任务以采集各厂家服务器清单为主，可根据需求修改清单输出格式.  </li></ul><pre><code class="python"># -*- coding:utf-8 -*-# @Author: Shin# @Date: 2019/12/5 11:48# @File: main.py# @Usage: Main# Self Repofrom src.ftp_wc import *# from src.func_demo.os_f import file_create# from concurrent.futures import ThreadPoolExecutor, as_completeddef ftp_job(flag, local_path, file_flag, file_title ):    &quot;&quot;&quot;    @ FTP Application 定义，可根据功能自定义扩展    :param flag:        采集标签，标签用来区分清单采集的厂家和内容    :param local_path:  清单输出路径    :param file_flag:   hint to distinguish between different flag_lists    :param file_title:  file title    Ex:        (main_job(&#39;HW_CM&#39;), local_path=file_nlst_path, file_flag=fileDict[&#39;HW_CM&#39;][&#39;flag&#39;], file_title=fileDict[&#39;HW_CM&#39;][&#39;title&#39;])        # 清单标签,和采集清单一一对应        fileDict = {            &#39;LOCAL&#39;:{                &#39;title&#39;: (&#39;flag&#39;, &#39;LOCAL&#39;),                &#39;flag&#39;: &#39;FileList&#39;            },            &#39;HW_CM&#39;:{                &#39;title&#39;: (&#39;flag&#39;, &#39;85_HW_CM&#39;),                &#39;flag&#39;: &#39;85_HW_CM&#39;            },            &#39;HW_CM_NSA&#39;: {                &#39;title&#39;: (&#39;flag&#39;, &#39;85_HW_CM_NSA&#39;),                &#39;flag&#39;: &#39;85_HW_CM&#39;            },            &#39;NSN_CM&#39;: {                &#39;title&#39;: (&#39;flag&#39;, &#39;86_NSN_CM&#39;),# &#39;86_NSN_CM&#39;,                &#39;flag&#39;: &#39;86_NSN_CM&#39;            },        }    :return:    counter: 0 or 1    &quot;&quot;&quot;    res_list = main_job(flag)    print(f&#39;res_list: {res_list}&#39;)    ftp_nlst_write(res_list[&quot;ftp_findall&quot;], local_path=local_path, file_flag=file_flag, file_title=file_title)    # return res_list[&quot;error_list&quot;]if __name__ == &#39;__main__&#39;:    # 测试用文件&amp;文件夹生成，生产环境无需部署以下两步    # file_create(d_path, 0, d_name)    # file_create(f_path, 1, *file_name_list)    print(&#39;------------------&#39;*2)    ######    # 方法4    from multiprocessing.dummy import Pool as ThreadPool    list_host = [rs[&#39;host&#39;] for rs in ftp_ip_dict[&#39;LOCAL&#39;]]    print(list_host)    list_usr = [rs[&#39;usr&#39;] for rs in ftp_ip_dict[&#39;LOCAL&#39;]]    print(list_usr)    list_passwd = [rs[&#39;passwd&#39;] for rs in ftp_ip_dict[&#39;LOCAL&#39;]]    print(list_passwd)    list_port = [rs[&#39;port&#39;] for rs in ftp_ip_dict[&#39;LOCAL&#39;]]    print(list_port)    list_timeout = [15 for rs in ftp_ip_dict[&#39;LOCAL&#39;]]    print(list_timeout, &#39;\n&#39;)    pool = ThreadPool(4)    # results = pool.starmap(ftp_connect, [(&#39;192.168.73.1&#39;, &#39;xxx@qq.com&#39;, &#39;xxx&#39;, 21, 15),(&#39;192.168.73.2&#39;, &#39;xxx@qq.com&#39;, &#39;xxx&#39;, 21, 15)])    # print(results)    # 下面的配置为.py配置文件，在 ftp_conf_bak.py 中    pool.starmap(ftp_job,                 [                     # (&#39;LOCAL&#39;, &#39;./data_output/&#39;, &#39;LOCAL&#39;, (&#39;flag&#39;, &#39;LOCAL&#39;)),                     # (&#39;LOCAL2&#39;, &#39;./data_output/&#39;, &#39;LOCAL2&#39;, (&#39;flag&#39;, &#39;LOCAL2&#39;)),                     (&#39;NSN_CM&#39;, &#39;./data_output/&#39;, fileDict[&#39;NSN_CM&#39;][&#39;flag&#39;], fileDict[&#39;NSN_CM&#39;][&#39;title&#39;]),                     # (&#39;HW_PM_3G&#39;, &#39;./data_output/&#39;, &#39;HW_PM_3G&#39;, (&#39;flag&#39;, &#39;HW_PM_3G&#39;)),                     # (&#39;HW_CM&#39;, &#39;./data_output/&#39;, fileDict[&#39;HW_CM&#39;][&#39;flag&#39;], fileDict[&#39;HW_CM&#39;][&#39;title&#39;]),                     (&#39;HW_CM_NSA&#39;, &#39;./data_output/&#39;, fileDict[&#39;HW_CM_NSA&#39;][&#39;flag&#39;], fileDict[&#39;HW_CM_NSA&#39;][&#39;title&#39;]),                 ]                 )    ######    # 方法1 传统线程池方法    # threads = []    # # t= None    # # 线程初始化    # for rs in fileDict.keys():    #     # print(rs)    #     # print(&#39;%s配置:&#39; % rs, sqlConf[rs])    #     # print(&#39;%sTITLE配置:&#39; % rs, fileTitleJob[rs])    #     if rs == &#39;LOCAL&#39;:   # &#39;NSN_CM&#39;:    #         t = MyThread(func=ftp_job, args=(rs, file_nlst_path, fileDict[rs][&#39;flag&#39;], fileDict[rs][&#39;title&#39;]))    #         # print(t.getName(), &#39;\n&#39;)    #         threads.append(t)    #    # # 线程批量启动    # for rt in threads:    #     rt.start()    #     # print(f&#39;Final List Status: \n{rt.get_result()}&#39;)    #    # for rt in threads:    #     rt.join()    #    # # 清单采集    # # 这里可以引入多线程，同时统计多个服务器上的采集清单    # # ftp_result = main_job(&#39;HW_CM&#39;)    # # ftp_nlst_write(ftp_result, local_path=file_nlst_path, file_flag=fileDict[&#39;HW_CM&#39;][&#39;flag&#39;], file_title=fileDict[&#39;HW_CM&#39;][&#39;title&#39;])    # #    # # ftp_result = main_job(&#39;HW_CM_NSA&#39;)    # # ftp_nlst_write(ftp_result, local_path=file_nlst_path, file_flag=fileDict[&#39;HW_CM_NSA&#39;][&#39;flag&#39;], file_title=fileDict[&#39;HW_CM_NSA&#39;][&#39;title&#39;])    # #    # # ftp_result = main_job(&#39;NSN_CM&#39;)    # # ftp_nlst_write(ftp_result, local_path=file_nlst_path, file_flag=fileDict[&#39;NSN_CM&#39;][&#39;flag&#39;], file_title=fileDict[&#39;NSN_CM&#39;][&#39;title&#39;])    #    # # 清单处理    #    # # 暂停    # # input_word = input(&#39;Input any key to quit: &#39;)    # ######    # all_task = []    # # 方法3 ThreadPoolExecutor    # # 需要打印时替换方法2    # executor = ThreadPoolExecutor(max_workers=5)    # for rs in fileDict.keys():    #     # if rs in [&#39;LOCAL&#39;, &#39;HW_CM_NSA&#39;]:    #     if rs in [&#39;LOCAL&#39;]:    #         future = executor.submit(ftp_job, rs, file_nlst_path, fileDict[rs][&#39;flag&#39;], fileDict[rs][&#39;title&#39;])    #         all_task.append(future)    #         print(f&#39;Future: {future}\n&#39;)    #    # ######    # # # 方法2 ThreadPoolExecutor    # # executor = ThreadPoolExecutor(max_workers=5)    # # all_task = [executor.submit(ftp_job, rs, file_nlst_path, fileDict[rs][&#39;flag&#39;], fileDict[rs][&#39;title&#39;])    # #             for rs in fileDict.keys() if rs in [&#39;LOCAL&#39;, &#39;HW_CM_NSA&#39;] ]    #    # A = None    # for future in as_completed(all_task):    #     if not future.result():    #         print(f&#39;Future: {future}, OK&#39;)    #     else:    #         print(f&#39;Future: {future}, Error: {future.result()}&#39;)    # ######</code></pre><hr><h4 id="ftp-wc-py"><a href="#ftp-wc-py" class="headerlink" title="ftp_wc.py"></a>ftp_wc.py</h4><ul><li>FTP方法封装<br>  <code>ftp_wc.ftp_connect</code> FTP任务启动/清单文件生成<br>  <code>ftp_wc.re_match</code> 正则匹配<br>  <code>ftp_wc.main_job</code> 主任务，主要用于相关方法的组合<br>  <code>ftp_wc.ftp_nlst</code> FTP清单遍历<br>  <code>ftp_wc.ftp_nlst_write</code> FTP清单文件生成<br>  <code>ftp_wc.ftp_download</code> FTP Download</li></ul><pre><code class="Python"># -*- coding:utf-8 -*-# @Author: Shin# @Date: 2019/12/5 11:13# @File: ftp_wc.py# @Usage: Generation for Remote FTP Listimport ftplibimport osimport socketimport re# import sys# import csv# Self Repo# from src.func_demo.func_f import date_ffrom src.conf.ftp_conf_bak import *from src.func_demo.Oracle2File import *# from src.ftp_wc import *from src.func_demo.os_f import file_createdef ftp_connect(host, usr=None, passwd=None, port=21, timeout=5):    &quot;&quot;&quot;    @ 连接    :param host: remote FTP address    :param usr: username for FTP    :param passwd: password    :param port: port &lt;int&gt;    :param timeout: the timeout to set against the ftp socket(s)    :return: &lt;class &#39;ftplib.FTP&#39;&gt; or 1&lt;num&gt;    Ex: ftp = ftp_connect(host=rs[&#39;host&#39;], port=rs[&#39;port&#39;], usr=rs[&#39;usr&#39;], passwd=rs[&#39;passwd&#39;])    &quot;&quot;&quot;    try:        print(f&#39;Current Connection Info: {host}:{port}/{usr}&#39;)        ftp = ftplib.FTP()        # ftp.encoding = &#39;utf-8&#39;        # print(type(ftp))        ftp.connect(host, port, timeout)    except socket.timeout as e:        print(f&#39;Status: Host&lt;{host}&gt; timed out during connection.&#39;)        print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)        print(&#39;------------------&#39; * 2)        return 1        # raise OSError(&#39;FTP connect timed out!&#39;)    except ConnectionRefusedError as e:        print(&#39;Status: Login failed. Please check whether the remote address is normal.&#39;)        print(&#39;------------------&#39;*2, f&#39;\nError Details:\n{e}&#39;)        print(&#39;------------------&#39;*2)        return 1    else:        ftp.set_debuglevel(0)  # 打开调试级别2，显示详细信息        try:            ftp.login(usr, passwd)            # print(f&#39;***Welcome Infomation: {ftp.welcome}***&#39;)  # 打印出欢迎信息            print(f&#39;Status: FTP User &lt;{usr}&gt; has connected to &lt;{host} {port}&gt;.&#39;)            return ftp        except ftplib.error_perm as e:            print(&#39;Status: Login failed. Please check whether the login information is correct.&#39;)            print(&#39;------------------&#39;*2, f&#39;\nError Details:\n{e}&#39;)            print(&#39;------------------&#39;*2)            return 1        except socket.timeout as e:            print(&#39;Status: Time out during login.&#39;)            print(&#39;------------------&#39;*2, f&#39;\nError Details:\n{str(e).title()}&#39;)            print(&#39;------------------&#39;*2)            return 1def ftp_nlst(ftp, remote_path, re_pattern):    &quot;&quot;&quot;    @ 匹配并获取清单    :param ftp:             &lt;class &#39;ftplib.FTP&#39;&gt;    :param remote_path:     FTP file path    :param re_pattern:      RE: Regular expression    Ex: ftp_reply = ftp_nlst(ftp, remote_path=remote_path, re_pattern=f&#39;.*.{y}{m}{d}.*.csv$&#39;)    :return: &lt;class &#39;list&#39;&gt; or Error 1    &quot;&quot;&quot;    # ftp = ftplib.FTP()    print(f&#39;***NLST***&#39;)    # Local Path    print(f&#39;--Local script path: {os.getcwd()}&#39;)    # Remote Path    print(f&#39;--Remote path: {remote_path}&#39;)    try:        n_lst_decode = []        ftp.cwd(remote_path)        # ftp.dir() # &lt;class &#39;NoneType&#39;&gt;        # print(f&#39;ftp_dir: {type(ftp_dir)}&#39;)        cur = ftp.pwd()        print(f&#39;Status: Successfully change dirName into {cur}.&#39;)        try:            n_lst = ftp.nlst()  # &lt;class &#39;list&#39;&gt;            for rs in n_lst:                n_lst_decode.append(rs.encode(&#39;iso-8859-1&#39;).decode(&#39;gbk&#39;))  # 解决Python3中文乱码  latin-1 ---&gt; gbk/gb2312            # print(f&#39;n_lst_decode: {n_lst_decode}&#39;)            # ftp.retrlines(cmd=&#39;LIST&#39;, &lt;function&gt;)            match_result = re_match(list_input=n_lst_decode, re_pattern=re_pattern)            print(f&#39;match_result: {match_result}&#39;)            ftp.close()            return match_result        except Exception as e:            print(&#39;Status: Failed to obtain the file lists!&#39;)            print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)            print(&#39;------------------&#39; * 2)            ftp.close()            return 1    except ftplib.all_errors as e:        print(&#39;Status: Path switching failed!&#39;)        print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)        print(&#39;------------------&#39; * 2)        ftp.close()        return 2    finally:        ftp.close()def re_match(list_input, re_pattern):    &quot;&quot;&quot;    @ 正则匹配工具    :param list_input:  &lt;class: list&gt;    :param re_pattern:  RE    :return:    返回正则匹配结果集 re.compile(re_pattern).findall() &lt;class: list&gt;    &quot;&quot;&quot;    print(f&#39;***RE***&#39;)    try:        # # 1        #     match_result = [rs for rs in list_input if re.match(re_pattern, rs)]        # # 2        #     match_result = []        #     for rs in list_input:        #         if re.match(re_pattern, rs):        #             match_result.append(rs)        #             print(f&#39;Status: File match successfully. Filename: {rs}&#39;)        #         else:        #             # print(f&#39;Match failed. Filename: {rs}&#39;)        #             print(f&#39;Match failed.&#39;)        #     print(match_result)        #     return match_result        # 3        #     match_result = []        #     p = re.compile(re_pattern)        #     for rs in list_input:        #         if p.findall(rs):        #             match_result.append(rs)        #             print(rs)        p = re.compile(re_pattern)        match_result = [rs for rs in list_input if p.findall(rs)]  # 别忘了，findall返回的是 &lt;class &#39;list&#39;&gt;        # print(match_result)        return match_result    except Exception as e:        print(&#39;Status: RE failed!&#39;)        print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)        print(&#39;------------------&#39; * 2)        return 1def main_job(key):    &quot;&quot;&quot;    @ 完整的连接/采集/清洗过程，不含写清单    :param key: 采集标签，标签用来区分清单采集的厂家和内容    :return: &lt;class: dict&gt;  [ftp_findall, error_counter, error_list]    Ex:    &quot;&quot;&quot;    ftp_result = []    ftp_findall = []    error_list = []    # error_counter = 0    # try:    if key in ftp_ip_dict.keys():        print(f&#39;***MAIN***&#39;)        for rn_dict in ftp_ip_dict.keys():            # 测试Limit            if rn_dict == key:  # &#39;NSN_CM&#39;:                # pprint.pprint(ftp_ip_dict[rs])                # print(f&#39;{rn_dict}: {ftp_ip_dict[rn_dict]}&#39;)                for rs in ftp_ip_dict[rn_dict]:                    # print(rs)  # &lt;class &#39;dict&#39;&gt;                    # 这里后续可以引入多线程，同时统计当前采集服务器上的对应的所有IP                    ftp = ftp_connect(host=rs[&#39;host&#39;], port=rs[&#39;port&#39;], usr=rs[&#39;usr&#39;], passwd=rs[&#39;passwd&#39;])                    if ftp == 1:                        # print(f&#39;FinishStatus: User {rs[&quot;usr&quot;]} Exception!&#39;, &#39;\n&#39;*2)                        print(f&#39;FinishStatus: {key}_{rs[&quot;host&quot;]} Connect Exception!&#39;, &#39;\n&#39; * 2)                        # error_counter += 1                        error_list.append(rs[&#39;host&#39;])                        continue                    else:                        remote_path = rs[&#39;remotePath&#39;]                        # print(f&#39;已配置的远程路径: {remote_path}&#39;)                        ftp_reply = ftp_nlst(ftp, remote_path=remote_path, re_pattern=rs[&#39;re_pattern&#39;]) # &lt;class &#39;list&#39;&gt;                        if ftp_reply == 1:                            print(f&#39;FinishStatus: {key}_{rs[&quot;host&quot;]} NLST Exception!&#39;, &#39;\n&#39; * 2)                            # error_counter += 1                            error_list.append(rs[&#39;host&#39;])                            continue                        else:                            print(f&#39;ftp_current: {ftp_reply}&#39;)                            ftp_result.extend(ftp_reply)                            del ftp_reply                        print(f&#39;ftp_result: {ftp_result}&#39;)                        print(f&#39;FinishStatus: {key}_{rs[&quot;host&quot;]} Succeed!\n&#39;)                # Final Result Gather                print(&#39;***FINAL***&#39;)                for rf in ftp_result:                    ftp_findall.append([key, rf])                for err in error_list:                    ftp_findall.append([key, err])                print(f&#39;ftp_findall: {ftp_findall}&#39;)        # Results filled in dict        data_dict = dict(ftp_findall=ftp_findall, error_list=error_list)        return data_dict    # elif not ftp_findall:    #     return error_counter    else:        print(f&#39;{key}_{rs[&quot;host&quot;]} Error FTP Key in ftp_conf...&#39;)def ftp_nlst_write(message_date, local_path, file_flag, file_title):    &quot;&quot;&quot;    @ 写清单        :param message_date: list input    :param local_path:  local file path for out-put    :param file_flag:   hint to distinguish between different flag_lists    :param file_title:  file title    :return 0(file output) or 1    &quot;&quot;&quot;    print(f&#39;***NLST_WRITE***&#39;)    # 清单    # local_path = file_nlst_path    # file_title = &#39;LOCAL&#39;    # fileDict[&#39;LOCAL&#39;][&#39;title&#39;]    # file_flag = fileDict[&#39;HW_CM&#39;][&#39;flag&#39;]    try:        file = FileWR(local_file_path=local_path, title=file_title)        flag = file.file_write_f(message_date=message_date, job_flag=file_flag)        return flag    # 方法2:    # try:    #     with open(output_file, &#39;w&#39;, newline=&#39;&#39;, encoding=&#39;UTF-8&#39;) as file_1:    #         writer_csv = csv.writer(file_1)    #         writer_csv.writerow([file_title])    #         for row in n_lst:    #             writer_csv.writerow([row])  # csv提供的写入方法可以按行写入list，无需按照对象一个个写入，效率更高    #         # writer_csv.writerows([n_lst])    #     return 0    # except Exception as e:    #     print(&#39;Status: 文件写入失败!&#39;)    #     print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)    #     print(&#39;------------------&#39; * 2)    #     return 1    except (IOError, OSError, Exception) as e:        print(&#39;Status: File write error!&#39;)        print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)        print(&#39;------------------&#39; * 2)        return 1def ftp_download(ftp, remote_path, local_path, re_pattern):    &quot;&quot;&quot;    @ 下载    :param ftp:  FTP连接池    :param remote_path: 远程文件路径（含文件名）    :param local_path:  本地保存路径（含文件名）    :return:  0 or 1    &quot;&quot;&quot;    # ftp = ftplib.FTP()    try:        # ftp.set_debuglevel(0)        ftp.cwd(remote_path)        buf_size = 1024        fd = open(local_path, &#39;wb&#39;)        re_list = ftp_nlst(ftp, remote_path=remote_path, re_pattern=re_pattern)  # &lt;class &#39;list&#39;&gt;        for rs in re_list:            ftp.retrbinary(f&#39;RETR {remote_path}&#39;, fd.write, buf_size)        fd.close()        return 0    except Exception as e:        print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)        print(&#39;------------------&#39; * 2)        return 1# if __name__ == &#39;__main__&#39;:#     pass</code></pre><hr><h4 id="ftp-conf-py"><a href="#ftp-conf-py" class="headerlink" title="ftp_conf.py"></a>ftp_conf.py</h4><pre><code class="Python"># -*- coding:utf-8 -*-# @Author: Shin# @Date: 2019/12/5 11:34# @File: ftp_conf.py# @Usage: Configuration&quot;&quot;&quot;@ FTP Application Configuration&quot;&quot;&quot;import os.pathimport pprintimport pandas as pd# Self Repofrom src.func_demo.func_f import date_f# 时间戳准备y = date_f()[3][&#39;year&#39;]     # 2020m = date_f()[3][&#39;month&#39;]    # 01d = date_f()[3][&#39;day&#39;]      # 15h = date_f()[3][&#39;hour&#39;]     # 14# FTP登录信息配置ftp_ip_dict = {                # LOCAL                &#39;LOCAL&#39;: (                    {                        &#39;host&#39;: &#39;192.168.73.1&#39;,                        &#39;port&#39;: 21,                        &#39;usr&#39;: &#39;xxx@qq.com&#39;,                        &#39;passwd&#39;: &#39;xxx&#39;,                        &#39;remotePath&#39;: f&#39;/test/{y}{m}{d}&#39;,                        &#39;re_pattern&#39;: f&#39;.*{y}{m}{d}.*.gz$&#39;      # Ex: *.20191210.csv                    }, {                        &#39;host&#39;: &#39;192.168.73.1&#39;,                        &#39;port&#39;: 21,                        &#39;usr&#39;: &#39;xxx@qq.com&#39;,                        &#39;passwd&#39;: &#39;xxx&#39;,                        &#39;remotePath&#39;: f&#39;/test/{y}{m}{d}&#39;,                        &#39;re_pattern&#39;: f&#39;.*{y}{m}{d}{h}.*.csv$&#39; }    # Ex: *.20191210.csv                ),                &#39;LOCAL2&#39;: (                    {                        &#39;host&#39;: &#39;192.168.73.1&#39;,                        &#39;port&#39;: 21,                        &#39;usr&#39;: &#39;xxx@qq.com&#39;,                        &#39;passwd&#39;: &#39;xxx&#39;,                        &#39;remotePath&#39;: f&#39;/test/{y}{m}{d}&#39;,                        &#39;re_pattern&#39;: f&#39;.*?{y}{m}{d}.*.gz$&#39;      # Ex: *.20191210.csv                    },  {                        &#39;host&#39;: &#39;192.168.73.1&#39;,                        &#39;port&#39;: 21,                        &#39;usr&#39;: &#39;xxx@qq.com&#39;,                        &#39;passwd&#39;: &#39;xxx&#39;,                        &#39;remotePath&#39;: f&#39;/test/{y}{m}{d}&#39;,                        &#39;re_pattern&#39;: f&#39;.*{y}{m}{d}.*.gz$&#39;      # Ex: *.20191210.csv                    }                ),}# 清单相关配置# 生成路径file_nlst_path = &#39;./data_output/&#39;# 清单标签,和采集清单一一对应fileDict = {    &#39;LOCAL&#39;: {        &#39;title&#39;: (&#39;flag&#39;, &#39;LOCAL&#39;),        &#39;flag&#39;: &#39;FileList&#39;    },    &#39;LOCAL2&#39;: {        &#39;title&#39;: (&#39;flag&#39;, &#39;LOCAL2&#39;),        &#39;flag&#39;: &#39;FileList2&#39;    },    &#39;HW_CM&#39;: {        &#39;title&#39;: (&#39;flag&#39;, &#39;85_HW_CM&#39;),        &#39;flag&#39;: &#39;85_HW_CM&#39;    },    &#39;HW_CM_NSA&#39;: {        &#39;title&#39;: (&#39;flag&#39;, &#39;85_HW_CM_NSA&#39;),        &#39;flag&#39;: &#39;85_HW_CM_NSA&#39;    },    &#39;NSN_CM&#39;: {        &#39;title&#39;: (&#39;flag&#39;, &#39;86_NSN_CM&#39;),# &#39;86_NSN_CM&#39;,        &#39;flag&#39;: &#39;86_NSN_CM&#39;    },    &#39;HW_PM_3G&#39;: {        &#39;title&#39;: (&#39;flag&#39;, &#39;73_HW_PM_3G&#39;),  # &#39;86_NSN_CM&#39;,        &#39;flag&#39;: &#39;73_NSN_CM&#39;    }}# 清单标题titleDict = {    &#39;DEFAULT&#39;: (&#39;flag&#39;, &#39;86_NSN_CM&#39;)}for key, value in fileDict.items():    titleDict[key] = (&#39;flag&#39;, fileDict[key][&#39;flag&#39;])# 测试用文件&amp;文件夹生成# 文件名file_name_1 = f&#39;34G_output_{date_f()[1]}_tmp.csv&#39;file_name_2 = f&#39;5G_output_{date_f()[0]}.xml.gz&#39;file_name_3 = f&#39;5G_output_{date_f()[1]}.csv&#39;# 文件名列表file_name_list = [file_name_1, file_name_2, file_name_3]# 文件路径# f_path = rf&#39;../../../../../FTP/test/{date_f()[0]}/&#39;    # D:\Hadoop\PyFloder\ftp_monitor\src\func_demo\f_path = os.path.abspath(f&#39;../../../../../FTP/test/{date_f()[0]}&#39;) + &#39;\\&#39;  # 表示当前所处项目的绝对路径：D:\Hadoop\PyFloder\bokeh_test# 文件夹名d_name = f&#39;{date_f()[0]}&#39;# 文件夹路径d_path = os.path.abspath(f&#39;../../../../../FTP/test&#39;) + &#39;\\&#39;</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>Python3</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20200110_2042_MySQL2</title>
    <link href="/2020/01/10/20200110-2042-MySQL2/"/>
    <url>/2020/01/10/20200110-2042-MySQL2/</url>
    
    <content type="html"><![CDATA[<h2 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h2><h3 id="01-MySQL-5-7"><a href="#01-MySQL-5-7" class="headerlink" title="01. MySQL 5.7"></a>01. MySQL 5.7</h3><ul><li>Repo Point<br><a href="https://www.g.cn" target="_blank" rel="noopener">MySQL</a></li></ul><h3 id="02-数据库设计（分而治之）"><a href="#02-数据库设计（分而治之）" class="headerlink" title="02. 数据库设计（分而治之）"></a>02. 数据库设计（分而治之）</h3><h4 id="三大范式（规范）"><a href="#三大范式（规范）" class="headerlink" title="三大范式（规范）"></a>三大范式（规范）</h4><ul><li><p>1NF: 所有的域是原子性的</p><blockquote><p>表中的每一列字段应该是不可拆分的最小单元，最低要求.</p></blockquote></li><li><p>2NF: 所有的非主键字段必须与主键相关,不能与部分主键相关(联合主键)</p><blockquote><p>每张表中描述和表示一类数据，多种不同的数据需要拆分到多张表中.</p></blockquote></li><li><p>3NF: 所有非主键字段必须与主键直接相关,而不能依赖于其他非主键字段</p><blockquote><p>数据不能具有传递依赖，每个字段与主键的关系是直接的而非间接的.</p></blockquote></li></ul><h4 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h4><ul><li>常用存储引擎 <ul><li>InnoDB是事务型数据库的首选，执行安全性数据库，行锁定和外键，mysql5.5之后默认使用.</li><li>MyISAM插入速度和查询效率较高，但不支持事务.</li><li>MEMORY将表中的数据存储在内存中，速度较快.</li></ul></li></ul><pre><code class="SQL"># mysql默认开启自动事务提交,将每个dml操作当做一个事务。# 如果需要将多个dml操作放在同一事务,需要关闭自动事务提交set Autocommit=0;//开启事务start TRANSACTION;#一组dml操作update user set money = money -100 where userid = 1;update user set money = money + 100 where userid = 2;...commit;//提交事务(数据持久化到底层文件中)rollback;//回滚事务(数据恢复到开启事务之前的状态)</code></pre><ul><li>存储过程</li></ul><pre><code class="SQL">delimiter //; --设置分隔符drop procedure if exists func_name;create procedure func_name(eno int, out ename varchar(20))begin        select * from MURA.emp e where e.empno = eno;        select e.ename into ename from MURA.emp e where e.empno = eno;end //;--调用，返回值用@修饰，并单独查询call func_name(7028, @x);select @x;--查看PROCEDURE状态show procedure status like &#39;func_name&#39;</code></pre><hr><h4 id="某东3C产品数据分析"><a href="#某东3C产品数据分析" class="headerlink" title="某东3C产品数据分析"></a>某东3C产品数据分析</h4><h5 id="设计实体及其关系"><a href="#设计实体及其关系" class="headerlink" title="设计实体及其关系"></a>设计实体及其关系</h5><ul><li><p>order_detail<br>  product/order_platform/payment_pattern/users</p></li><li><p>product<br>  product_category/repertory</p></li><li><p>china<br>  repertory/users</p></li></ul><hr><ul><li>统计各个城市销售额的前十名<br><code>pid</code> <code>p_name</code> <code>c_name</code> <code>sales</code> <code>sales_e</code> <code>rank</code></li></ul><!-- | 项目        | 价格   |  数量  || --------   | -----:  | :----:  || 计算机     | $1600 |   5     || 手机        |   \$12   |   12   || 管线        |    \$1    |  234  | --><hr>]]></content>
    
    
    
    <tags>
      
      <tag>MySQL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20200109_2055_MySQL</title>
    <link href="/2020/01/09/20200103-2355-MySQL/"/>
    <url>/2020/01/09/20200103-2355-MySQL/</url>
    
    <content type="html"><![CDATA[<h2 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h2><h3 id="01-MySQL-5-7"><a href="#01-MySQL-5-7" class="headerlink" title="01. MySQL 5.7"></a>01. MySQL 5.7</h3><ul><li>Repo Point<br><a href="https://www.g.cn" target="_blank" rel="noopener">MySQL</a></li></ul><h3 id="02-环境配置"><a href="#02-环境配置" class="headerlink" title="02. 环境配置"></a>02. 环境配置</h3><ul><li>基本环境配置</li></ul><p><code>Tips</code>MySQL中的<code>utf8</code>并不是常规认知里的<code>UTF-8</code>，而<code>utf8mb4</code>才是真正的<code>UTF-8</code>，切记.</p><pre><code class="bash"># 1 查看并卸载自带的MySQLrpm -qa|grep mysql# 2 发现的话就卸载掉rpm -e --nodeps mysql...# 3 删除老版本MySQL的开头文件和库rm -rf /usr/lib/mysqlrm -rf /usr/include/mysqlrm -rf /etc/my.cnfrm -rf /var/lib/mysql# 4 mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar与视频不太一样，需要按顺序安装一些依赖：rpm -ivh mysql-community-common-5.7.28-1.el7.x86_64.rpmrpm -ivh mysql-community-libs-5.7.28-1.el7.x86_64.rpmrpm -ivh mysql-community-client-5.7.28-1.el7.x86_64.rpmrpm -ivh mysql-community-server-5.7.28-1.el7.x86_64.rpm# 5 检查安装结果rpm -qa|grep mysql# 6-1 初始密码的位置/root/.mysql_secret# 打印瞧瞧：cat /root/.mysql_secret# 这个好像5.7不太一样，需要按如下方式获取：grep &#39;temporary password&#39; /var/log/mysqld.log# 6-2 修改密码策略&amp;重启服务：vim /etc/my.cnf# my.cnf[mysqld]# 复杂密码策略关闭validate_password=off# 设置默认编码集character-set-server=utf8mb4# 回避ERROR 1055 BUGsql_mode=&#39;&#39;####### 重启服务# service mysqld restartsystemctl restart mysqld# 7 set password=password(&#39;root&#39;);mysql -uroot -p&#39;root&#39; --auto-rehash# exit下并重新进入。exit# 8 查看下端口，可以判断MySQL是否起来了netstat -nltptcp6       0      0 :::3306     :::*      LISTEN      978/mysqld# 9 增加远程登录权限show grants for root;select user,host from mysql.user;# 查询指定IP对应的连接权限，即@&#39;%&#39;GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39; WITH GRANT OPTION;GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;master&#39; IDENTIFIED BY &#39;123456&#39; WITH GRANT OPTION;FLUSH PRIVILEGES;# JDK 配置vim ~/.bashrc# SET JAVA_PATHexport JAVA_HOME=/usr/local/src/jdk1.8.0_201export JRE_HOME=${JAVA_HOME}/jreexport CLASSPATH=.:${JAVA_HOME}/lib:${JAVA_HOME}/libexport PATH=${JAVA_HOME}/bin:$PATH</code></pre><hr><ul><li>MySQL 5.7 ERROR 1055 BUG<br>  MySQL5.7中默认会开启 <code>only_full_group_by</code> 这个模式，可以在配置文件中把它去掉</li></ul><pre><code class="bash"># Linux为my.cnf，Windows为my.inivim /etc/my.cnf</code></pre><img src="ERROR-1055.png" srcset="/img/loading.gif" title="ERROR 1055" alt="ERROR 1055"><ul><li>查询相关配置</li></ul><pre><code class="SQL">mysql&gt; show variables;mysql&gt; show variables like &#39;%char%&#39; \G--临时修改参数mysql&gt; set xxx=&#39;xxx&#39;;--永久修改--修改上述的my.cnf文件--my.cnf</code></pre><pre><code class="shell">[client]port=3306default-character-set=utf8mb4[mysqld]character-set-server=utf8mb4collation-server=utf8mb4_general_civalidate_password=offdatadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.sock# Disabling symbolic-links is recommended to prevent assorted security riskssymbolic-links=0log-error=/var/log/mysqld.logpid-file=/var/run/mysqld/mysqld.pidsql_mode=&#39;&#39;# autocommit=OFF</code></pre><h3 id="03-命令行自动化"><a href="#03-命令行自动化" class="headerlink" title="03. 命令行自动化"></a>03. 命令行自动化</h3><ul><li>最简单的执行方式</li></ul><pre><code class="SQL">mysql -u&lt;user&gt; -p&lt;password&gt;  -D&lt;db_name&gt; -e &quot;show tables; drop table if exists student_bak; show tables;&quot; --auto-rehash</code></pre><h3 id="04-Common-Operation（常规DDL-DML-DQL-DCL）"><a href="#04-Common-Operation（常规DDL-DML-DQL-DCL）" class="headerlink" title="04. Common Operation（常规DDL/DML/DQL/DCL）"></a>04. Common Operation（常规DDL/DML/DQL/DCL）</h3><h4 id="DDL"><a href="#DDL" class="headerlink" title="DDL"></a>DDL</h4><pre><code class="SQL">create database dbname;drop database dbname;use dbname;show tables;</code></pre><ul><li>元数据配置，与Oracle不同，新增字段可以指定插入位置</li></ul><pre><code class="SQL">--指定位置插入新字段alter table student add card char(8) after first;alter table student add card char(8) after sname;--修改数据类型alter table tname change colname colname colTypealter table tname modify colname colType--修改字段位置alter table tname modify colname colType after colname2;</code></pre><ul><li>字符集</li></ul><pre><code class="SQL">--两个更改表维度字符集的模式好像没什么区别，第一个是影响表，第二个是影响行.ALTER TABLE tname DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci;ALTER TABLE tname CONVERT TO CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci;ALTER TABLE student CHANGE sex sex VARCHAR(20) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci;</code></pre><ul><li>主键约束/唯一约束（单表可配置多个）</li></ul><p><code>Tips</code>字符集，表内若有中文数据，需指定表的字符集.</p><pre><code class="SQL">create table `student`(    sid int(11) primary key,    ...);create table `student`(    sid int(11),    ...,    primary key(sid, xxx)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;create table `student` (  `sid` int(11) not null auto_increment,  ⬅ int类型的单位为字节：-2147483648 ~ 2147483647  `cid` int(11) default null,  `telephone` char(11) default null,  `sname` varchar(20) default null,  `card` char(8) default null,  `sex` varchar(5) default null,  `birthday` date default null,  `email` varchar(20) default null,  primary key (`sid`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;--主键约束的增加与删除alter table student add constraint PK_SID primary key(sid, xxx, xxx);alter table student drop primary key;--唯一约束alter table student add constraint UN_CARD unique(card, xxx, xxx);drop unique index UN_CARD on student;--主键自增create table `student`(    sid int(11) primary key auto_increment,    id_card char(18) unique);--后期修改alter table student change sid sid int(11) not null AUTO_INCREMENT;--新增加表时：CREATE TABLE TABLE_1 (    ID INT UNSIGNED NOT NULL PRIMARY KEY AUTO_INCREMENT, --ID列为无符号整型，该列值不可以为空，并不可以重复，而且自增    NAME VARCHAR(5) NOT NULL ) AUTO_INCREMENT = 100;--（ID列从100开始自增）--创建表格后，设置：alter table users AUTO_INCREMENT=10000;</code></pre><ul><li>主键-域完整性（字段准确）：类型约束/非空约束/默认值</li></ul><pre><code class="SQL">create table `person`(    sid int(11) primary key auto_increment,    pname varchar(20) not null,    sex bit(1) default 1);</code></pre><ul><li>外键-引用完整性（参照准确）</li></ul><pre><code class="SQL">create table `classroom`(    cid int primary key auto_increment,    cname varchar(20) not null);create table `stu`(    sid int primary key auto_increment,    sname varchar(20) not null,    cid int);alter table stu add constraint FK_CID foreign key(cid) references classroom(cid)</code></pre><!-- * 运算```SQLselect 3 / 2;select 3 div 2;   --1select 3 / 0;   --NULLselect 5 % 2;   --1select 3 > 2;   --1select 3 <> 2;  --1``` --><h4 id="DML"><a href="#DML" class="headerlink" title="DML"></a>DML</h4><ul><li>增删改查</li></ul><pre><code class="SQL">--与Oracle相同，新表不继承约束和索引信息create table student_bak as select * from student where 1=2;--update student_bak set sname = &#39;dd&#39; where sid = 8;</code></pre><!-- insert into MURA.student(cid,telephone,sname,card,sex,birthday,email)values(801790, 13921446055, 'GHR', 'cardguo1', '男', '1992/10/28', 'shinnosuke1028@foxmail.com'),(801390, 13066951004, 'LL', 'cardlll1', '女', '1992/12/10', 'lili1004@foxmail.com'); --><pre><code class="SQL">--MySQL tablecreate table MURA.`t_pm_alarm_tst` (`res_type` int(11) not null,`res_key` varchar(20) default null,`kpi_id` BIGINT not null AUTO_INCREMENT,`time` date default null,`value` int(5) default null,`vendor` varchar(5) default null,`city` varchar(5) default null,primary key (`kpi_id`))AUTO_INCREMENT = 5000200010001 ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;insert into MURA.t_pm_alarm_tst (res_type, res_key, time, value, vendor, city)values(&#39;1&#39;, &#39;10001&#39;, now() + 0, &#39;60&#39;, &#39;华为&#39;, &#39;531&#39;),(&#39;1&#39;, &#39;10001&#39;, now() + 0, &#39;60&#39;, &#39;华为&#39;, &#39;531&#39;),(&#39;1&#39;, &#39;10001&#39;, now() + 0, &#39;61&#39;, &#39;诺基亚&#39;, &#39;531&#39;),(&#39;2&#39;, &#39;10001&#39;, now() + 100000, &#39;62&#39;, &#39;华为&#39;, &#39;531&#39;);</code></pre><h4 id="MySQL函数"><a href="#MySQL函数" class="headerlink" title="MySQL函数"></a>MySQL函数</h4><pre><code class="SQL">--随机数select rand();select round(13.45 -1); --10</code></pre><ul><li>部分时间函数</li></ul><pre><code class="SQL">--部分时间函数--后一个时间-前一个时间，返回天/月/年SELECT TIMESTAMPDIFF(DAY,&#39;2012-10-01&#39;,&#39;2013-01-13&#39;);SELECT TIMESTAMPDIFF(MONTH,&#39;2012-10-01&#39;,&#39;2013-01-13&#39;);SELECT TIMESTAMPDIFF(YEAR,&#39;2012-10-01&#39;,&#39;2013-01-13&#39;);-- 第一个时间-第二个时间，返回天SELECT DATEDIFF(&#39;2013-01-13&#39;,&#39;2012-10-01&#39;);--时间迁移select date_sub(now(),interval 1 day); 获取一天前的今天的日期select date_sub(now(),interval 1 week); 获取一年前的今天的日期select date_sub(now(),interval 1 month); 获取一年前的今天的日期--时间格式化select date_format(date_sub(last_day(now()), interval 2 day), &#39;%Y-%m-%d&#39;));--返回一个天数，从0年开始的天数select to_days(&#39;2020-01-08&#39;); --xxxxaaselect FROM_DAYS(xxxxaa); </code></pre><ul><li>部分陌生字符串函数</li></ul><pre><code class="SQL">--连接select concat(&#39;ghr&#39;, &#39;921028&#39;);--截取select SUBSTR(&#39;ghr921028&#39;, 1, 3);</code></pre><ul><li>加密函数</li></ul><pre><code class="SQL">--加密select md5(&#39;root&#39;);select SHA(&#39;root&#39;);select PASSWORD(&#39;ghr921028&#39;);</code></pre><ul><li>流程&amp;连接函数</li></ul><pre><code class="SQL">select if(2&gt;3, 2, 3);select ifnull(1028, &#39;xxx&#39;);--和Oracle一样的case when--连接--只有连接的表之间的字段命名相同，且是等值连接时，才可以使用；不等连接不好使用usingselect * from dept d [inner] join emp e on e.deptno = d.deptno;select * from dept d [inner] join emp using(deptno);select * from dept d left [outer] join emp e on e.deptno = d.deptno;select * from dept d left [outer] join emp using(deptno);</code></pre><ul><li>提取<pre><code class="SQL">select </code></pre></li></ul><hr><h4 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h4><pre><code class="SQL">CREATE TABLE `student_bak`(    `sdate` date,    `sid` int(11) NOT NULL AUTO_INCREMENT,    `cid` int(11) DEFAULT NULL,    `telephone` char(11) DEFAULT NULL,    `sname` varchar(20) DEFAULT NULL,    `card` char(8) DEFAULT NULL,    `sex` varchar(5) DEFAULT NULL,    `birthday` date DEFAULT NULL,    `email` varchar(50) DEFAULT NULL,    PRIMARY KEY (`sid`,`sdate`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4partition by range (to_days(sdate))(    partition p20200108 values less than (TO_DAYS(&#39;20200109&#39;)),    partition p20200109 values less than (TO_DAYS(&#39;20200110&#39;)))</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>MySQL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20200105_1338_Python3-Monitor（Up to date）</title>
    <link href="/2020/01/05/20191218-1300-Python3-Monitor/"/>
    <url>/2020/01/05/20191218-1300-Python3-Monitor/</url>
    
    <content type="html"><![CDATA[<h2 id="Python3-Oracle2CSV-Monitor"><a href="#Python3-Oracle2CSV-Monitor" class="headerlink" title="Python3 Oracle2CSV_Monitor"></a>Python3 Oracle2CSV_Monitor</h2><h3 id="01-数据库数据采集-amp-监控"><a href="#01-数据库数据采集-amp-监控" class="headerlink" title="01. 数据库数据采集&amp;监控"></a>01. 数据库数据采集&amp;监控</h3><ul><li><p>Repo Point<br>  cx_Oracle, csv, smtplib, email<br>  threading</p></li><li><p>class_email_daily3.py</p><pre><code class="python"># -*- coding:utf-8 -*-# @Author: Shin# @Date: 2019/11/20 15:14# @File: class_email_daily3.py</code></pre></li></ul><p>import sys<br>import os, re</p><h1 id="import-math"><a href="#import-math" class="headerlink" title="import math"></a>import math</h1><h1 id="from-os-path-import-abspath-join-dirname"><a href="#from-os-path-import-abspath-join-dirname" class="headerlink" title="from os.path import abspath, join, dirname"></a>from os.path import abspath, join, dirname</h1><h1 id="import-pprint"><a href="#import-pprint" class="headerlink" title="import pprint"></a>import pprint</h1><p>import cx_Oracle<br>import csv<br>import smtplib</p><h1 id="import-numpy-as-np"><a href="#import-numpy-as-np" class="headerlink" title="import numpy as np"></a>import numpy as np</h1><p>import threading</p><h1 id="import-copy"><a href="#import-copy" class="headerlink" title="import copy"></a>import copy</h1><p>from email.mime.text import MIMEText<br>from email.header import Header<br>from email.utils import formataddr<br>from email.mime.multipart import MIMEMultipart</p><p>from time import sleep, ctime, time<br>from tqdm import tqdm</p><h1 id="CMD模式运行配置"><a href="#CMD模式运行配置" class="headerlink" title="CMD模式运行配置"></a>CMD模式运行配置</h1><p>from func_test.func_f import date_f</p><h1 id="from-conf-import-bas-insert-conf"><a href="#from-conf-import-bas-insert-conf" class="headerlink" title="from conf import bas_insert_conf"></a>from conf import bas_insert_conf</h1><p>from conf import bas_mail_conf<br>from conf import sql_conf</p><h1 id="非CMD模式运行配置"><a href="#非CMD模式运行配置" class="headerlink" title="非CMD模式运行配置"></a>非CMD模式运行配置</h1><h1 id="from-src-conf-import-bas-mail-conf"><a href="#from-src-conf-import-bas-mail-conf" class="headerlink" title="from src.conf import bas_mail_conf"></a>from src.conf import bas_mail_conf</h1><h1 id="from-src-conf-import-sql-conf"><a href="#from-src-conf-import-sql-conf" class="headerlink" title="from src.conf import sql_conf"></a>from src.conf import sql_conf</h1><h1 id="from-src-conf-import-bas-mail-conf-1"><a href="#from-src-conf-import-bas-mail-conf-1" class="headerlink" title="from src.conf import bas_mail_conf"></a>from src.conf import bas_mail_conf</h1><h1 id="from-src-func-test-func-f-import-date-f"><a href="#from-src-func-test-func-f-import-date-f" class="headerlink" title="from src.func_test.func_f import date_f"></a>from src.func_test.func_f import date_f</h1><h1 id="sys-path-insert-0-join-abspath-dirname-file-‘-func-test-‘"><a href="#sys-path-insert-0-join-abspath-dirname-file-‘-func-test-‘" class="headerlink" title="sys.path.insert(0, join(abspath(dirname(file)), ‘../func_test/‘))"></a>sys.path.insert(0, join(abspath(dirname(<strong>file</strong>)), ‘../func_test/‘))</h1><p>sys.path.append(‘./func_test’)</p><p>class OracleExecution(object):<br>    # def <strong>init</strong>(self, sql=None, connect=None, check_style=None):<br>    def <strong>init</strong>(self):<br>        self.<strong>connect = None<br>        self.</strong>sql = None<br>        self.__check_style = None<br>        # self.message_str = ‘’<br>        self.rs = []<br>        self.message = ‘’<br>        self.message_data = []<br>        self.db = None</p><pre><code># # 装饰器用法1：# @property# def conf_f(self):#     return self.__connect, self.__sql, self.__check_style## @conf_f.setter# def conf_f(self, value):#     self.__connect = value[0]#     self.__sql = value[1]#     self.__check_style = value[2]# 装饰器用法2：&lt;1等价于2&gt;def get_conf_f(self):    return self.__connect, self.__sql, self.__check_styledef set_conf_f(self, value):    self.__connect = value[0]    self.__sql = value[1]    self.__check_style = value[2]conf_f = property(get_conf_f, set_conf_f)def connect_f(self):    try:        self.db = cx_Oracle.connect(self.__connect)    except Exception as e:        print(f&#39;Status: Failed to connect database.&#39;)        print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)        print(&#39;------------------&#39; * 2)    finally:        return self.dbdef execute_f(self):    # db = cx_oracle.connect(self.username + &quot;/&quot; + self.password + &quot;@&quot; +    try:        cur = self.db.cursor().execute(self.__sql)    except Exception as e:        print(f&#39;Status: Failed to execute SQL.\nSQL:  {self.conf_f[1]}&#39;)        print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)        print(&#39;------------------&#39; * 2)        self.db.rollback()    else:        self.rs = cur.fetchall()        # print(self.rs)        # print(type(self.rs))    # &lt;class &#39;list&#39;&gt;        if self.__check_style == &#39;JOB&#39;:            data_flag = 0            data_flag_bad = None            i = 0            for r1 in self.rs:                if i == 0:                    data_flag = r1[2]                    if data_flag != 0:                        data_flag_bad = r1[0]                    else:                        continue                elif r1[1] is not None:                    data_flag = data_flag + r1[2]                    if data_flag != 0:                        data_flag_bad = data_flag_bad + &#39;,&#39; + r1[0]                        # 这里也可以直接写成 data_flag_bad = data_flag_bad, r1[0]                    else:                        continue                else:                    data_flag = data_flag + 0                    if data_flag != 0:                        data_flag_bad = data_flag_bad + &#39;,&#39; + r1[0]                    else:                        continue                i += 1            if data_flag != 0:                self.message = &#39;数据库脚本或定时任务异常,请及时核查.报错任务号：&#39; + str(data_flag_bad)                # print(data_flag_bad)            else:                self.message = &#39;数据库脚本正常执行,详细监测日志请查看附件.&#39;            print(&#39;&lt;&#39; + date_f(0)[1] + &#39;&gt; : &#39; + self.message)        else:            self.message = &#39;定时邮件任务完成，详细信息请看附件.&#39;            print(&#39;&lt;&#39; + date_f(0)[1] + &#39;&gt; : &#39; + self.message)        cur.close()    finally:        self.db.close()        # print(type(self.message))        # print(self.rs)        return self.messagedef execute_split_f(self):    # message_data_all = []    # message_data = []    for r1 in self.rs:        # 如果写成 str(r1), list内的行对象list会变成字符串：list [(),()] -&gt; [&quot;()&quot;,&quot;()&quot;]        # 写入时会把每一组对象当成一个个完整的字符串，即一个字母一个数字进一个单元格        # 要写入.csv的文件，按行转换成最基本的list即可，一行一个list对象&lt;即元组tuple&gt;：list [(),()]；或list嵌套list：[[],[]]        # 无需转换为str再拼接为数组        try:            self.message_data.append(r1)  # 数据库拉取出来的一行为一个元组            # print(f&#39;r1:{r1}&#39;)            # r1:(datetime.datetime(2019, 12, 12, 0, 0), &#39;HW_4G_CM&lt;OMC1&gt;&#39;, &#39;00&#39;, 23, 11, 11, 11, 369.36, 357.26,            # 355.16, 0, -12.1, &#39;数量未变&#39;, &#39;大小波动小&#39;, &#39;/LTE/MOBILE/HUAWEI/OMC1/CM/&#39;)            # print(self.message_data)        except Exception as e:            print(e)    # print(self.message_data)    # print(type(self.message_data))    return self.message_data</code></pre><p>class FileWR(OracleExecution):<br>    # 初始化子类属性时，需带上父类属性，这种需全部初始化父类属性的写法，用在：调用含父类属性的父类方法时，不然实例化时，无法顺利调用父类方法<br>    # 调用不含父类属性的父类方法时，可以直接用：super(&lt;子类名&gt;, self).<strong>init</strong>()完成父类初始化<br>    # def <strong>init</strong>(self, local_file_path=None, title=None, connect=None, sql=None, check_style=None):<br>    def <strong>init</strong>(self, local_file_path=None, title=None):<br>        # super(FileWR, self).<strong>init</strong>(connect, sql, check_style)   # 初始化父类属性<br>        # super(FileWR, self).<strong>init</strong>()<br>        super().<strong>init</strong>()  # 简写<br>        # 初始化父类属性，父类属性有默认赋值时，在子类初始化时可以不再指明具体参数<br>        # 后续实例化时，若子类想调用父类属性，则可以直接调用父类属性并赋值；或写成注释行部分的形式去初始化父类属性，在初始化子类时一并加入父类属性<br>        self.local_file_path = local_file_path<br>        self.title = title<br>        # self.message_data = message_date<br>        self.file_name = ‘’  # 可以不定义在初始化内，只作为类的私有属性</p><pre><code>def file_write_f(self, message_date, job_flag, sleep_seconds=0.001):    try:        self.file_name = self.local_file_path + date_f(0)[0] + &#39;_&#39; + job_flag + &#39;.csv&#39;        # print(self.file_name)        with open(self.file_name, &#39;w&#39;, newline=&#39;&#39;, encoding=&#39;GBK&#39;) as file_1:            writer_csv = csv.writer(file_1)            writer_csv.writerow(self.title)            for row in tqdm(message_date, ncols=80):                # print([row])                writer_csv.writerow(row)  # csv提供的写入方法可以按行写入list，无需按照对象一个个写入，效率更高                sleep(sleep_seconds)                # for row in tqdm(iterable=message_date, ncols=80):                #     writer_csv.writerow(row)  # csv提供的写入方法可以按行写入list，无需按照对象一个个写入，效率更高                #     # sleep(0.05)                # file_size = len(message_date)                # for row in range(file_size):                #     writer_csv.writerow(message_date[row])                #     sys.stdout.write(&#39;\r[{0}] Percent:{1}%&#39;.format(&#39;=&#39;*int(row*50/(file_size-1)),                #     str(row*100/(file_size-1))))                #     if row == file_size:                #         sys.stdout.write(&#39;\r[{0}] Percent:{1}%&#39;.format(&#39;=&#39; * int(100), str(100)))                #         print(&#39;\n&#39;)                #     sleep(sleep_seconds)    except Exception as e:        print(e)</code></pre><p>class MailSender(object):<br>    def <strong>init</strong>(self):  # 邮件概览/正文/文件名(含路径)<br>        # self.mail_view = mail_view<br>        # self.mail_text = mail_text<br>        # self.mail_title = mail_title<br>        self.mail_attach = []<br>        # self.file_name = file_name<br>        self.msg = None<br>        # self.attach = None</p><pre><code>def mail_mime_action(self, receivers, message_body):    mail_sender = &#39;shinnosuke1028@qq.com&#39;    mail_password = &#39;ixwzutghdbtxbaie&#39;    mail_server = &#39;smtp.qq.com&#39;    # subject = &#39;Python SMTP 邮件测试...&lt;数据完整性监控(日常JOB/采集)&gt;&#39;    subject = &#39;Py-%s &lt;数据完整性监控(日常JOB/昨日采集)&gt;&#39; % date_f(0)[0]    # MIMEMultipart 形式构造邮件正文    self.msg = MIMEMultipart()  # 开辟一个带邮件的mail接口    self.msg[&#39;From&#39;] = formataddr([&#39;郭皓然测试&#39;, mail_sender])    self.msg[&#39;To&#39;] = formataddr([&#39;,&#39;.join(receivers), &#39;utf-8&#39;])  # 用&#39;,&#39;进行拼接，待拼接内容：join(x)内的x    self.msg[&#39;Subject&#39;] = Header(subject, &#39;utf-8&#39;)    # 正文loading    print(f&#39;Status: Mail body loading...&#39;)    self.msg.attach(MIMEText(message_body, &#39;plain&#39;, &#39;utf-8&#39;))    # self.msg.attach(MIMEText(message + &#39;\n&#39; + title + message_str, &#39;plain&#39;, &#39;utf-8&#39;))    # 邮件装载附件    # 方法1    # for fn in self.file_name:    #     attach_tmp = self.msg_attach(fn)    #     self.mail_attach.append(attach_tmp)    # 方法2    try:        print(f&#39;Status: Mail attachments loading...&#39;)        cur_list_re = []        for fn in os.walk(bas_mail_conf.mail_file_path_class):            print(f&#39;fn[-1]: {fn[-1]}&#39;)  # ./data_output/*            for cur in fn[-1]:                x = re.search(bas_mail_conf.file_pattern, cur)                print(f&#39;cur: {cur}, x: {x}&#39;)                # cur: 20191217_PKG.csv, x: None                # cur: 20191218_GATHER.csv, x: &lt;re.Match object; span=(0, 19), match=&#39;20191218_GATHER.csv&#39;&gt;                if x:                    cur_list_re.append(bas_mail_conf.mail_file_path_class + x.group())                else:                    continue        for rx in cur_list_re:            # print(f&#39;rx: {rx}&#39;)            tx_tmp = self.msg_attach(rx)            self.msg.attach(tx_tmp)        print(f&#39;Status: Mail loaded successfully.&#39;)    except Exception as e:        print(f&#39;Status: Failed to load mail...&#39;)        print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)        print(&#39;------------------&#39; * 2)    try:        server = smtplib.SMTP(mail_server, 25)  # 发件人邮箱中的SMTP服务器，SMTP服务端口是25        server.login(mail_sender, mail_password)  # 括号中对应的是发件人邮箱账号、邮箱密码        server.sendmail(mail_sender, receivers, self.msg.as_string())  # 括号中对应的是发件人邮箱账号、收件人邮箱账号、邮件内容发送        print(&#39;Status: Mail sended successfully.&#39;)        server.quit()  # 关闭连接    except Exception as e:        print(&#39;Status: Failed to send mail...&#39;)        print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)        print(&#39;------------------&#39; * 2)def msg_attach(self, file_name):    &quot;&quot;&quot;    :return:  type(attach): &lt;class &#39;email.mime.text.MIMEText&#39;&gt; 附件封装结果    Ex:        att1 = MIMEText(open(file_name, &#39;rb&#39;).read(), &#39;base64&#39;, &#39;utf-8&#39;)        att1[&quot;Content-Type&quot;] = &#39;application/octet-stream&#39;        att1[&quot;Content-Disposition&quot;] = &#39;attachment; filename=&#39; + file_name    # 这里的filename可任意，写什么名字，邮件中显示什么名字        msg.attach(att1)        att2 = MIMEText(open(file_name2, &#39;rb&#39;).read(), &#39;base64&#39;, &#39;utf-8&#39;)        att2[&quot;Content-Type&quot;] = &#39;application/octet-stream&#39;        att2[&quot;Content-Disposition&quot;] = &#39;attachment; filename=&#39; + file_name2    # 这里的filename可任意，写什么名字，邮件中显示什么名字        msg.attach(att2)    &quot;&quot;&quot;    attach = MIMEText(open(file_name, &#39;rb&#39;).read(), &#39;base64&#39;, &#39;utf-8&#39;)    attach[&quot;Content-Type&quot;] = &#39;application/octet-stream&#39;    attach[&quot;Content-Disposition&quot;] = &#39;attachment; filename=&#39; + file_name  # 这里的filename可任意，写什么名字，邮件中显示什么名字    # print(f&#39;type(attach): {type(attach)}&#39;)    return attach</code></pre><p>class MyThread(threading.Thread):<br>    def <strong>init</strong>(self, func=None, args=()):<br>        super().<strong>init</strong>()<br>        self.func = func<br>        self.args = args<br>        self.result = []</p><pre><code>def run(self):    self.result = self.func(*self.args)def get_result(self):    # noinspection PyBroadException    try:        # print(f&#39;Results return:&#39;)        return self.result    except Exception as e:        print(f&#39;Status: 线程返回结果.&#39;)        print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)        print(&#39;------------------&#39; * 2)        return 1</code></pre><p>######################################################<br>######################################################</p><h1 id="以下是装饰器修饰函数的用法，可省略代码的反复加工"><a href="#以下是装饰器修饰函数的用法，可省略代码的反复加工" class="headerlink" title="以下是装饰器修饰函数的用法，可省略代码的反复加工"></a>以下是装饰器修饰函数的用法，可省略代码的反复加工</h1><p>balance = []</p><p>def lock_f(lock_flag=’N’):<br>    def threading_f(f):<br>        def inner_f(<em>value):<br>            print(‘Status: 1号装饰器测试开始！’)<br>            global balance<br>            if lock_flag == ‘Y’:<br>                # i += 1<br>                lock = threading.RLock()<br>                with lock:<br>                    # r_lock.acquire()<br>                    print(f’Thread {threading.current_thread().getName()} is running. Time: {ctime()}’)<br>                    result = f(</em>value)<br>                    # pprint.pprint(results)<br>                    # print(type(results))    # &lt;class ‘tuple’&gt;<br>                    balance.append(result)<br>                    # r_lock.release()<br>                    print(f’Thread {threading.current_thread().getName()} end. Time: {ctime()}’)<br>                    print(‘1号装饰器测试结束！’)<br>            else:<br>                print(‘Status: 1号装饰器不再调用！’)<br>                print(f’Thread {threading.current_thread().getName()} is running. Time: {ctime()}’)<br>                result = f(*value)<br>                balance.append(result)<br>                print(f’Thread {threading.current_thread().getName()} end. Time: {ctime()}’)<br>            # print(f’balance: {balance}’)    # 线程结果集合 &lt;class: list&gt;<br>            # pprint.pprint(result)   # 单线程结果<br>            return balance</p><pre><code>    return inner_freturn threading_f</code></pre><p>def email_f(email_flag=’N’):<br>    def mail_post_f(f):<br>        def inner_f(i=0, <em>value):<br>            print(‘Status: 2号装饰器测试开始！’)<br>            if email_flag == ‘Y’:<br>                print(f’Thread {threading.current_thread().getName()} is running. Time: {ctime()}’)<br>                results = f(</em>value)</p><pre><code>            # 获取邮件正文body，这里定位到JOB返回的内容            body = f&#39;{results[&quot;JOB&quot;][1]}\n{&quot;,&quot;.join(bas_mail_conf.titleDict[&quot;CONF_JOB&quot;])}&#39;            # 中间对象初始化            body_tmp = None            i_tmp = None            # 拆分数据结果            for rs in results[&quot;JOB&quot;][2]:                # print(f&#39;i: {i}, rs: {rs}&#39;)    # tuple转正文中的字符串                # Ex:                # rs: (21, 0, datetime.datetime(2020, 1, 4, 8, 0), &#39;TRUNC(sysdate+1) + 8/(24)&#39;,..., &#39;PKG...;&#39;)                # rs: (41, 0, datetime.datetime(2020, 1, 3, 17, 0), &#39;TRUNC(sysdate+1) + 17/(24)&#39;,..., &#39;PKG...;&#39;)                # ...                # 转换每一组tuple为字符串并拼接，主要是为了时间的字符显示                for rn in rs:                    if body_tmp is None or i_tmp != i:                        body_tmp = str(rn)                        i_tmp = i                    else:                        body_tmp = str(body_tmp) + &#39;, &#39; + str(rn)                # print(f&#39;body_tmp:\n {body_tmp}&#39;)                # 按行拼接每一组转换后的tuple                body = body + &#39;\n&#39; + body_tmp                i += 1            # 打印body            # print(f&#39;body:\n{body}&#39;)            # 装载/发送            mail = MailSender()            mail.mail_mime_action(bas_mail_conf.receivers, body)            print(f&#39;Thread {threading.current_thread().getName()} end. Time: {ctime()}&#39;)            print(&#39;2号装饰器测试结束！&#39;)        else:            print(&#39;Status: 2号装饰器不再调用！&#39;)            print(f&#39;Thread {threading.current_thread().getName()} is running. Time: {ctime()}&#39;)            results = f(*value)            print(f&#39;Thread {threading.current_thread().getName()} end. Time: {ctime()}&#39;)        # pprint.pprint(results)        return results    return inner_freturn mail_post_f</code></pre><h1 id="程序编译时优先编译内层装饰器-再编译外层装饰器-编译顺序-email-f-gt-lock-f"><a href="#程序编译时优先编译内层装饰器-再编译外层装饰器-编译顺序-email-f-gt-lock-f" class="headerlink" title="程序编译时优先编译内层装饰器/再编译外层装饰器,编译顺序:email_f -&gt; lock_f"></a>程序编译时优先编译内层装饰器/再编译外层装饰器,编译顺序:email_f -&gt; lock_f</h1><h1 id="执行时-类似于Queue-先进后出-执行顺序-lock-f-执行-f-value-之前的内容-gt-email-f-gt-lock-f-f-value"><a href="#执行时-类似于Queue-先进后出-执行顺序-lock-f-执行-f-value-之前的内容-gt-email-f-gt-lock-f-f-value" class="headerlink" title="执行时,类似于Queue(先进后出),执行顺序:lock_f(执行 f(value) 之前的内容) -&gt; email_f -&gt; lock_f(f(value))"></a>执行时,类似于Queue(先进后出),执行顺序:lock_f(执行 f(<em>value) 之前的内容) -&gt; email_f -&gt; lock_f(f(</em>value))</h1><h1 id="email-f-‘Y’-1号装饰器-邮件的发送需要跳出多线程，不然会出现多次发送的现象"><a href="#email-f-‘Y’-1号装饰器-邮件的发送需要跳出多线程，不然会出现多次发送的现象" class="headerlink" title="@email_f(‘Y’)  # 1号装饰器  # 邮件的发送需要跳出多线程，不然会出现多次发送的现象"></a>@email_f(‘Y’)  # 1号装饰器  # 邮件的发送需要跳出多线程，不然会出现多次发送的现象</h1><p>@lock_f(‘Y’)  # 2号装饰器<br>def ora_job(conf_job, file_path, file_title):<br>    “””</p><pre><code>:param conf_job::param file_path::param file_title::return: &lt;class: list&gt;: [(job_flag, file_mail_view_tmp, file_mail_text_tmp),...,()]&quot;&quot;&quot;# with r_lock:# 实例化ora = FileWR(local_file_path=file_path, title=file_title)ora.conf_f = conf_job  # 列表按顺序，进行数据库检索配置job_flag = ora.conf_f[2]ora.connect_f()file_mail_view_tmp = str(ora.execute_f())del ora.message# print(&#39;1:&#39;, file_mail_view_tmp)file_mail_text_tmp = ora.execute_split_f()del ora.message_data# print(type(file_mail_text_tmp))ora.file_write_f(file_mail_text_tmp, job_flag, )return job_flag, file_mail_view_tmp, file_mail_text_tmp</code></pre><h1 id="最后总结send"><a href="#最后总结send" class="headerlink" title="最后总结send"></a>最后总结send</h1><p>@email_f(‘Y’)  # 1号装饰器  # 邮件的发送需要跳出多线程，不然会出现多次发送的现象<br>def main_job():<br>    # print(sys.path)</p><pre><code>####################################################### 准备工作threads = []# 实例化信息&amp;检索语句初始化sqlconf = sql_conf.sqlDict# 文件生成路径/各报表标题初始化filepath, filetitlejob = bas_mail_conf.mail_file_path_class, bas_mail_conf.titleDict############################################################################################################# 采集多线程初始化# r_lock = threading.RLock()for rs in sqlconf.keys():    t = MyThread(ora_job, (sqlconf[rs], filepath, filetitlejob[rs]))    threads.append(t)rt = None# 线程批量启动for rt in threads:    rt.start()dict_final = {}for rt in threads:    rt.join()for rn in range(len(threads)):    # print(f&#39;rt.get_result()[rn]: {rt.get_result()[rn]}&#39;)    # 为什么rt.get_result()一口气返回了所有线程的结果，线程返回的是一个生成器？？？    dict_final[rt.get_result()[rn][0]] = rt.get_result()[rn]  # {&#39;JOB&#39;: (&#39;JOB&#39;,&#39;&#39;,[(),(),()]),... }# pprint.pprint(dict_final)return dict_final</code></pre><p>if <strong>name</strong> == ‘<strong>main</strong>‘:<br>    print(‘Thread’, threading.current_thread().getName(), ‘is Running. Time: %s’ % date_f()[2])</p><pre><code>start_time = time()mail_dict_combine = main_job()# print(f&#39;mail_dict_combine_view:{mail_dict_combine[&quot;JOB&quot;][0]}&#39;)# print(f&#39;mail_dict_combine_view:{mail_dict_combine[&quot;JOB&quot;][1]}&#39;)# print(f&#39;mail_dict_combine_mail_text:{mail_dict_combine[&quot;JOB&quot;][2]}&#39;)# print(f&#39;mail_dict_combine:{mail_dict_combine[&quot;PKG&quot;][1]}&#39;)end_time = time()print(str(end_time - start_time))print(&#39;Thread&#39;, threading.current_thread().getName(), &#39;End. Time: %s&#39; % date_f()[2])# # 下次遍历前初始化字典的写法# mail_dict_combine.append(mail_dict)# mail_dict = {}# print(id(mail_dict_combine))    # 这里打印的内存值虽相同,但??????# # 注意:下面的写法有问题# 若直接mail_dict_combine.append(mail_dict),会出现覆盖情况,数据始终指向mail_dict初始内存,故只能取到最后一组数据# mail_dict_combine.append(mail_dict)# print(id(mail_dict_combine)) # 即多次打印这里的内存值相同# print(mail_dict_combine)############################################################################################################# # 单线程# for rs in sqlConf.keys():#     ora_job(sqlConf[rs], filePath, fileTitleJob[rs])######################################################</code></pre><pre><code>---* main.py```python# -*- coding:utf-8 -*-# @Author: Shin# @Date: 2020/1/3 14:48# @File: main.py# @Usage: Main Drive# CMD Modefrom class_email_daily_3 import *import threadingif __name__ == &#39;__main__&#39;:    print(&#39;Thread&#39;, threading.current_thread().getName(), &#39;is Running. Time: %s&#39; % date_f()[2])    # mail_dict_combine = main_job()    demo()    print(&#39;Thread&#39;, threading.current_thread().getName(), &#39;End. Time: %s&#39; % date_f()[2])    key = input(&#39;Put any key to quit...&#39;)</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>Python3</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20191226_1309_Python3-FTP-list</title>
    <link href="/2019/12/26/20191225-1309-Python3-FTP-list/"/>
    <url>/2019/12/26/20191225-1309-Python3-FTP-list/</url>
    
    <content type="html"><![CDATA[<h2 id="Python3-FTP-List-Gather"><a href="#Python3-FTP-List-Gather" class="headerlink" title="Python3 FTP_List_Gather"></a>Python3 FTP_List_Gather</h2><h3 id="01-服务器数据清单采集（暂时不含可视化部分）"><a href="#01-服务器数据清单采集（暂时不含可视化部分）" class="headerlink" title="01. 服务器数据清单采集（暂时不含可视化部分）"></a>01. 服务器数据清单采集（暂时不含可视化部分）</h3><ul><li>Repo Point<br>  ftplib, os, socket, re<br>  Oracle2File.FileWR（class_email_daily3.py）<br>  func_demo.func_f.date_f</li></ul><pre><code class="bash"># tree ftp_monitor/src -L 2$ tree src -L 3src|-- __init__.py|-- __pycache__|   `-- __init__.cpython-37.pyc|-- conf|   |-- __init__.py|   `-- ftp_conf_bak.py|-- data|-- data_output|   `-- __init__.py|-- ftp_wc.py|-- func_demo|   |-- Oracle2File.py|   |-- __init__.py|   |-- __pycache__|   |-- func_f.py|   `-- os_f.py`-- main.py</code></pre><hr><pre><code class="python"># -*- coding:utf-8 -*-# @Author: Shin# @Date: 2019/12/5 11:48# @File: main.py# @Usage: Main# Self Repofrom src.ftp_wc import *from src.func_demo.os_f import file_createfrom concurrent.futures import ThreadPoolExecutor, as_completeddef ftp_job(flag, local_path, file_flag, file_title ):    &quot;&quot;&quot;    :param flag:        采集标签，标签用来区分清单采集的厂家和内容    :param local_path:  清单输出路径    :param file_flag:   hint to distinguish between different flag_lists    :param file_title:  file title    Ex:        (main_job(&#39;HW_CM&#39;), local_path=file_nlst_path, file_flag=fileDict[&#39;HW_CM&#39;][&#39;flag&#39;], file_title=fileDict[&#39;HW_CM&#39;][&#39;title&#39;])        # 清单标签,和采集清单一一对应        fileDict = {            &#39;LOCAL&#39;:{                &#39;title&#39;: (&#39;flag&#39;, &#39;LOCAL&#39;),                &#39;flag&#39;: &#39;FileList&#39;            },            &#39;HW_CM&#39;:{                &#39;title&#39;: (&#39;flag&#39;, &#39;85_HW_CM&#39;),                &#39;flag&#39;: &#39;85_HW_CM&#39;            },            &#39;HW_CM_NSA&#39;: {                &#39;title&#39;: (&#39;flag&#39;, &#39;85_HW_CM_NSA&#39;),                &#39;flag&#39;: &#39;85_HW_CM&#39;            },            &#39;NSN_CM&#39;: {                &#39;title&#39;: (&#39;flag&#39;, &#39;86_NSN_CM&#39;),# &#39;86_NSN_CM&#39;,                &#39;flag&#39;: &#39;86_NSN_CM&#39;            },        }    :return:    counter: 0 or 1    &quot;&quot;&quot;    res_list = main_job(flag)    # pprint.pprint(res_list)    if len(res_list[&quot;error_list&quot;]):        pass    else:        ftp_nlst_write(res_list[&quot;ftp_findall&quot;], local_path=local_path, file_flag=file_flag, file_title=file_title)    return res_list[&quot;error_list&quot;]if __name__ == &#39;__main__&#39;:    # 测试用文件&amp;文件夹生成，生产环境无需部署以下两步    file_create(d_path, 0, d_name)    file_create(f_path, 1, *file_name_list)    print(&#39;------------------&#39;*2)    all_task = []    # 方法3 ThreadPoolExecutor    # 需要打印时替换方法2    executor = ThreadPoolExecutor(max_workers=5)    for rs in fileDict.keys():        if rs in [&#39;LOCAL&#39;, &#39;HW_CM_NSA&#39;]:            future = executor.submit(ftp_job, rs, file_nlst_path, fileDict[rs][&#39;flag&#39;], fileDict[rs][&#39;title&#39;])            all_task.append(future)            print(f&#39;Future: {future}\n&#39;)    # # 方法2 ThreadPoolExecutor    # executor = ThreadPoolExecutor(max_workers=5)    # all_task = [executor.submit(ftp_job, rs, file_nlst_path, fileDict[rs][&#39;flag&#39;], fileDict[rs][&#39;title&#39;])    #             for rs in fileDict.keys() if rs in [&#39;LOCAL&#39;, &#39;HW_CM_NSA&#39;] ]    A = None    for future in as_completed(all_task):        if not future.result():            print(f&#39;Future: {future}, OK&#39;)        else:            print(f&#39;Future: {future}, Error: {future.result()}&#39;)</code></pre><hr><pre><code class="Python"># -*- coding:utf-8 -*-# @Author: Shin# @Date: 2019/12/5 11:13# @File: ftp_wc.py# @Usage: Generation for Remote FTP Listimport ftplibimport osimport socketimport re# import sys# import csv# Self Repo# from src.func_demo.func_f import date_ffrom src.conf.ftp_conf_bak import *from src.func_demo.Oracle2File import *# from src.ftp_wc import *from src.func_demo.os_f import file_createdef ftp_connect(host, usr, passwd, port=21, timeout=5):    &quot;&quot;&quot;    :param host: remote FTP address    :param usr: username for FTP    :param passwd: password    :param port: port &lt;int&gt;    :param timeout: the timeout to set against the ftp socket(s)    :return: &lt;class &#39;ftplib.FTP&#39;&gt; or 1&lt;num&gt;    Ex: ftp = ftp_connect(host=rs[&#39;host&#39;], port=rs[&#39;port&#39;], usr=rs[&#39;usr&#39;], passwd=rs[&#39;passwd&#39;])    &quot;&quot;&quot;    try:        print(f&#39;Current Connection Info: {host}:{port}/{usr}&#39;)        ftp = ftplib.FTP()        # ftp.encoding = &#39;utf-8&#39;        # print(type(ftp))        ftp.connect(host, port, timeout)    except socket.timeout as e:        print(f&#39;Status: Host&lt;{host}&gt; timed out during connection.&#39;)        print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)        print(&#39;------------------&#39; * 2)        return 1        # raise OSError(&#39;FTP connect timed out!&#39;)    except ConnectionRefusedError as e:        print(&#39;Status: Login failed. Please check whether the remote address is normal.&#39;)        print(&#39;------------------&#39;*2, f&#39;\nError Details:\n{e}&#39;)        print(&#39;------------------&#39;*2)        return 1    else:        ftp.set_debuglevel(0)  # 打开调试级别2，显示详细信息        try:            ftp.login(usr, passwd)            # print(f&#39;***Welcome Infomation: {ftp.welcome}***&#39;)  # 打印出欢迎信息            print(f&#39;Status: FTP User &lt;{usr}&gt; has connected to &lt;{host} {port}&gt;.&#39;)            return ftp        except ftplib.error_perm as e:            print(&#39;Status: Login failed. Please check whether the login information is correct.&#39;)            print(&#39;------------------&#39;*2, f&#39;\nError Details:\n{e}&#39;)            print(&#39;------------------&#39;*2)            return 1        except socket.timeout as e:            print(&#39;Status: Time out during login.&#39;)            print(&#39;------------------&#39;*2, f&#39;\nError Details:\n{str(e).title()}&#39;)            print(&#39;------------------&#39;*2)            return 1def ftp_nlst(ftp, remote_path, re_pattern):    &quot;&quot;&quot;    :param ftp:         &lt;class &#39;ftplib.FTP&#39;&gt;    :param remote_path: FTP file path    :param re_pattern:     RE: Regular expression    Ex: ftp_reply = ftp_nlst(ftp, remote_path=remote_path, re_pattern=f&#39;.*.{y}{m}{d}.*.csv$&#39;)    :return: &lt;class &#39;list&#39;&gt; or Error 1    &quot;&quot;&quot;    # ftp = ftplib.FTP()    print(f&#39;***NLST***&#39;)    # Local Path    print(f&#39;--Local script path: {os.getcwd()}&#39;)    # Remote Path    print(f&#39;--Remote path: {remote_path}&#39;)    try:        n_lst_decode = []        ftp.cwd(remote_path)        # ftp.dir() # &lt;class &#39;NoneType&#39;&gt;        # print(f&#39;ftp_dir: {type(ftp_dir)}&#39;)        cur = ftp.pwd()        print(f&#39;Status: Successfully change dirName into {cur}.&#39;)        try:            n_lst = ftp.nlst()  # &lt;class &#39;list&#39;&gt;            for rs in n_lst:                n_lst_decode.append(rs.encode(&#39;iso-8859-1&#39;).decode(&#39;gbk&#39;))  # 解决Python3中文乱码  latin-1 ---&gt; gbk/gb2312            # print(f&#39;n_lst_decode: {n_lst_decode}&#39;)            # ftp.retrlines(cmd=&#39;LIST&#39;, &lt;function&gt;)            match_result = re_match(list_input=n_lst_decode, re_pattern=re_pattern)            return match_result        except Exception as e:            print(&#39;Status: Failed to obtain the file lists!&#39;)            print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)            print(&#39;------------------&#39; * 2)            return 1    except ftplib.all_errors as e:        print(&#39;Status: Path switching failed!&#39;)        print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)        print(&#39;------------------&#39; * 2)        return 1    finally:        ftp.close()def re_match(list_input, re_pattern):    &quot;&quot;&quot;    :param list_input:  &lt;class: list&gt;    :param re_pattern:  RE    :return:    返回正则匹配结果集 re.compile(re_pattern).findall() &lt;class: list&gt;    &quot;&quot;&quot;    print(f&#39;***RE***&#39;)    try:        # # 1        #     match_result = [rs for rs in list_input if re.match(re_pattern, rs)]        # # 2        #     match_result = []        #     for rs in list_input:        #         if re.match(re_pattern, rs):        #             match_result.append(rs)        #             print(f&#39;Status: File match successfully. Filename: {rs}&#39;)        #         else:        #             # print(f&#39;Match failed. Filename: {rs}&#39;)        #             print(f&#39;Match failed.&#39;)        #     print(match_result)        #     return match_result        # 3        #     match_result = []        #     p = re.compile(re_pattern)        #     for rs in list_input:        #         if p.findall(rs):        #             match_result.append(rs)        #             print(rs)        p = re.compile(re_pattern)        match_result = [rs for rs in list_input if p.findall(rs)]  # 别忘了，findall返回的是 &lt;class &#39;list&#39;&gt;        # print(match_result)        return match_result    except Exception as e:        print(&#39;Status: RE failed!&#39;)        print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)        print(&#39;------------------&#39; * 2)        return 1def main_job(key):    &quot;&quot;&quot;    :param key: 采集标签，标签用来区分清单采集的厂家和内容    :return: &lt;class: dict&gt;  [ftp_findall, error_counter, error_list]    Ex:    &quot;&quot;&quot;    ftp_result = []    ftp_findall = []    error_list = []    # error_counter = 0    # try:    if key in ftp_ip_dict.keys():        print(f&#39;***MAIN***&#39;)        for rn_dict in ftp_ip_dict.keys():            # 测试Limit            if rn_dict == key:  # &#39;NSN_CM&#39;:                # pprint.pprint(ftp_ip_dict[rs])                # print(f&#39;{rn_dict}: {ftp_ip_dict[rn_dict]}&#39;)                for rs in ftp_ip_dict[rn_dict]:                    # print(rs)  # &lt;class &#39;dict&#39;&gt;                    # 这里后续可以引入多线程，同时统计当前采集服务器上的对应的所有IP                    ftp = ftp_connect(host=rs[&#39;host&#39;], port=rs[&#39;port&#39;], usr=rs[&#39;usr&#39;], passwd=rs[&#39;passwd&#39;])                    if ftp == 1:                        # print(f&#39;FinishStatus: User {rs[&quot;usr&quot;]} Exception!&#39;, &#39;\n&#39;*2)                        print(f&#39;FinishStatus: Connect Exception!&#39;, &#39;\n&#39; * 2)                        # error_counter += 1                        error_list.append(rs[&#39;host&#39;])                        continue                    else:                        remote_path = rs[&#39;remotePath&#39;]                        # print(f&#39;已配置的远程路径: {remote_path}&#39;)                        ftp_reply = ftp_nlst(ftp, remote_path=remote_path, re_pattern=rs[&#39;re_pattern&#39;]) # &lt;class &#39;list&#39;&gt;                        if ftp_reply == 1:                            print(f&#39;FinishStatus: NLST Exception!&#39;, &#39;\n&#39; * 2)                            # error_counter += 1                            error_list.append(rs[&#39;host&#39;])                            continue                        else:                            print(f&#39;ftp_current: {ftp_reply}&#39;)                            ftp_result.extend(ftp_reply)                            del ftp_reply                        print(f&#39;ftp_result: {ftp_result}&#39;)                        print(f&#39;FinishStatus: Succeed!\n&#39;)                # Final Result Gather                print(&#39;***FINAL***&#39;)                for rf in ftp_result:                    ftp_findall.append([key, rf])                print(f&#39;ftp_findall: {ftp_findall}&#39;)        # Results filled in dict        data_dict = dict(ftp_findall=ftp_findall, error_list=error_list)        return data_dict    # elif not ftp_findall:    #     return error_counter    else:        print(f&#39;Error FTP Key in ftp_conf...&#39;)def ftp_nlst_write(message_date, local_path, file_flag, file_title):    &quot;&quot;&quot;    :param message_date: list input    :param local_path:  local file path for out-put    :param file_flag:   hint to distinguish between different flag_lists    :param file_title:  file title    :return 0(file output) or 1    &quot;&quot;&quot;    print(f&#39;***NLST_WRITE***&#39;)    # 清单    # local_path = file_nlst_path    # file_title = &#39;LOCAL&#39;    # fileDict[&#39;LOCAL&#39;][&#39;title&#39;]    # file_flag = fileDict[&#39;HW_CM&#39;][&#39;flag&#39;]    try:        file = FileWR(local_file_path=local_path, title=file_title)        flag = file.file_write_f(message_date=message_date, job_flag=file_flag)        return flag    # 方法2:    # try:    #     with open(output_file, &#39;w&#39;, newline=&#39;&#39;, encoding=&#39;UTF-8&#39;) as file_1:    #         writer_csv = csv.writer(file_1)    #         writer_csv.writerow([file_title])    #         for row in n_lst:    #             writer_csv.writerow([row])  # csv提供的写入方法可以按行写入list，无需按照对象一个个写入，效率更高    #         # writer_csv.writerows([n_lst])    #     return 0    # except Exception as e:    #     print(&#39;Status: 文件写入失败!&#39;)    #     print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)    #     print(&#39;------------------&#39; * 2)    #     return 1    except (IOError, OSError, Exception) as e:        print(&#39;Status: File write error!&#39;)        print(&#39;------------------&#39; * 2, f&#39;\nError Details:\n{e}&#39;)        print(&#39;------------------&#39; * 2)        return 1# if __name__ == &#39;__main__&#39;:#     pass</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>Python3</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20191220_1347_ubuntu16.04.2-server-amd64</title>
    <link href="/2019/12/20/20191220-1347-ubuntu16-04-2-server-amd64/"/>
    <url>/2019/12/20/20191220-1347-ubuntu16-04-2-server-amd64/</url>
    
    <content type="html"><![CDATA[<h2 id="Ubuntu-Server-配置"><a href="#Ubuntu-Server-配置" class="headerlink" title="Ubuntu Server 配置"></a>Ubuntu Server 配置</h2><h3 id="01-Ubuntu16-04-2-Server-amd64"><a href="#01-Ubuntu16-04-2-Server-amd64" class="headerlink" title="01. Ubuntu16.04.2-Server-amd64"></a>01. Ubuntu16.04.2-Server-amd64</h3><ul><li>Repo Point<br><a href="http://old-releases.ubuntu.com/releases/16.04.2/ubuntu-16.04.2-server-amd64.iso" target="_blank" rel="noopener">ubuntu-16.04.2-server-amd64.iso</a></li></ul><p><code>Tips</code> <strong>最新的版本已到19.01</strong></p><h3 id="02-环境配置"><a href="#02-环境配置" class="headerlink" title="02. 环境配置"></a>02. 环境配置</h3><h4 id="1-Ubuntu16-04-2-Server-amd64"><a href="#1-Ubuntu16-04-2-Server-amd64" class="headerlink" title="1. Ubuntu16.04.2-Server-amd64"></a>1. Ubuntu16.04.2-Server-amd64</h4><h5 id="服务器BOIS启动"><a href="#服务器BOIS启动" class="headerlink" title="服务器BOIS启动"></a>服务器BOIS启动</h5><pre><code class="bash"># F2、F12或 Del 进BOIS# 部分前置的选项包括语言、键盘语言&amp;布局、用户描述、非root用户账密、时区等</code></pre><h4 id="2-LVM"><a href="#2-LVM" class="headerlink" title="2. LVM"></a>2. LVM</h4><p><code>Tips</code> <a href="https://help.ubuntu.com/lts/serverguide/advanced-installation.html" target="_blank" rel="noopener">https://help.ubuntu.com/lts/serverguide/advanced-installation.html</a></p><h5 id="下面是官方文档步骤，建议使用LVM配置除了-boot外的所有挂载，便于后续LVM扩容"><a href="#下面是官方文档步骤，建议使用LVM配置除了-boot外的所有挂载，便于后续LVM扩容" class="headerlink" title="下面是官方文档步骤，建议使用LVM配置除了/boot外的所有挂载，便于后续LVM扩容."></a>下面是官方文档步骤，建议使用LVM配置除了/boot外的所有挂载，便于后续LVM扩容.</h5><ol><li>Follow the installation steps until you get to the Partition disks step, then: At the “Partition Disks screen choose “Manual”.</li></ol><ul><li>选择分区手动配置.</li></ul><hr><ol start="2"><li>Select the hard disk and on the next screen choose “yes” to “Create a new empty partition table on this device”.</li></ol><ul><li>选择在磁盘上创建新的分区.</li></ul><hr><ol start="3"><li>Next, create standard /boot, swap, and / partitions with whichever filesystem you prefer.</li></ol><ul><li>先手动建立 <code>/boot</code> 分区，剩余所有大小全部配置为 <code>LVM</code> 逻辑卷.</li></ul><hr><ol start="4"><li>For the LVM /srv, create a new Logical partition. Then change “Use as” to “physical volume for LVM” then “Done setting up the partition”.</li></ol><ul><li>LVM内按照自己的需求进行生成和挂载，选择 “physical volume for LVM”.</li></ul><hr><ol start="5"><li>Now select “Configure the Logical Volume Manager” at the top, and choose “Yes” to write the changes to disk.</li></ol><ul><li>磁盘分切完后(/boot &amp; LVM)，进入LVM管理配置，开始配置划分后的LVM磁盘.</li></ul><hr><ol start="6"><li>For the “LVM configuration action” on the next screen, choose “Create volume group”. Enter a name for the VG such as vg01, or something more descriptive. After entering a name, select the partition configured for LVM, and choose “Continue”.</li></ol><ul><li>顺序：<code>PV(Free physical volumes) → VG → LV</code> 此处应该可以从配置项里看到可用的PV，选择创建命名VG(例如：vg01).</li></ul><hr><ol start="7"><li>Back at the “LVM configuration action” screen, select “Create logical volume”. Select the newly created volume group, and enter a name for the new LV, for example srv since that is the intended mount point. Then choose a size, which may be the full partition because it can always be extended later. Choose “Finish” and you should be back at the main “Partition Disks” screen.</li></ol><ul><li>根据自己的需求，将创建好的VG分切至各个LV，并取名(例如：swap、root).</li></ul><hr><ol start="8"><li>Now add a filesystem to the new LVM. Select the partition under “LVM VG vg01, LV srv”, or whatever name you have chosen, the choose Use as. Setup a file system as normal selecting /srv as the mount point. Once done, select “Done setting up the partition”.</li></ol><ul><li>到这里，LVM已按照需求划分为不同分区，现在要做的就是挂载到对应的目录下，按照先前的命名，一一对应挂载.</li></ul><hr><ol start="9"><li>Finally, select “Finish partitioning and write changes to disk”. Then confirm the changes and continue with the rest of the installation.</li></ol><ul><li>Finish.</li></ul><hr><h5 id="具体磁盘配置步骤"><a href="#具体磁盘配置步骤" class="headerlink" title="具体磁盘配置步骤"></a><strong>具体磁盘配置步骤</strong></h5><img src="1.png" srcset="/img/loading.gif" title="01" alt=""><hr><img src="2.png" srcset="/img/loading.gif" title="02" alt=""><hr><img src="3.png" srcset="/img/loading.gif" title="03" alt=""><hr><img src="4.png" srcset="/img/loading.gif" title="单独建立/boot" alt=""><hr><img src="5.png" srcset="/img/loading.gif" title="单独建立/boot" alt=""><hr><img src="6.png" srcset="/img/loading.gif" title="单独建立/boot" alt=""><hr><ul><li>服务器上的安装与虚拟机练习有部分出入，踩了一些坑！！！<br>  boot分区需要配置为 <strong><code>EFI</code></strong>格式，文件分区会被挂载在<code>/boot/efi</code>下，<strong><code>Use as</code></strong>独立的选项，直接选择即可，无需配置<code>Mount point</code>！！！<br>  服务器上安装选择<code>/boot</code>的话会导致后续引导有问题，进入不了ubuntu系统！！！<br>  虚拟机安装可以按照下图示例配置.<img src="7.png" srcset="/img/loading.gif" title="单独建立/boot" alt=""></li></ul><hr><img src="8.png" srcset="/img/loading.gif" title="08" alt=""><hr><img src="9.png" srcset="/img/loading.gif" title="09" alt=""><hr><img src="10.png" srcset="/img/loading.gif" title="10" alt=""><hr><img src="11.png" srcset="/img/loading.gif" title="11" alt=""><hr><img src="12.png" srcset="/img/loading.gif" title="12" alt=""><hr><img src="13.png" srcset="/img/loading.gif" title="13" alt=""><hr><img src="14.png" srcset="/img/loading.gif" title="14" alt=""><hr><img src="15.png" srcset="/img/loading.gif" title="15" alt=""><hr><img src="16.png" srcset="/img/loading.gif" title="16" alt=""><hr><img src="17.png" srcset="/img/loading.gif" title="17" alt=""><hr><img src="18.png" srcset="/img/loading.gif" title="18" alt=""><hr><img src="19.png" srcset="/img/loading.gif" title="19" alt=""><hr><img src="20.png" srcset="/img/loading.gif" title="20" alt=""><hr><img src="21.png" srcset="/img/loading.gif" title="21" alt=""><hr><img src="22.png" srcset="/img/loading.gif" title="配置完成后的挂载" alt=""><hr><img src="23.png" srcset="/img/loading.gif" title="23" alt=""><hr><img src="24.png" srcset="/img/loading.gif" title="24" alt=""><hr><img src="25.png" srcset="/img/loading.gif" title="25" alt=""><hr><img src="26.png" srcset="/img/loading.gif" title="26" alt=""><hr><img src="27.png" srcset="/img/loading.gif" title="27" alt=""><hr><img src="28.png" srcset="/img/loading.gif" title="28" alt=""><hr><img src="29.png" srcset="/img/loading.gif" title="29" alt=""><hr><img src="30.png" srcset="/img/loading.gif" title="不自动更新" alt=""><hr><img src="31.png" srcset="/img/loading.gif" title="31" alt=""><hr><img src="32.png" srcset="/img/loading.gif" title="Install GRUB boot loader" alt=""><hr><img src="33.png" srcset="/img/loading.gif" title="Install GRUB boot loader" alt=""><hr><img src="34.png" srcset="/img/loading.gif" title="Finish!" alt=""><hr><h5 id="安装过程完成后会自动reboot"><a href="#安装过程完成后会自动reboot" class="headerlink" title="安装过程完成后会自动reboot."></a>安装过程完成后会自动reboot.</h5><img src="35.png" srcset="/img/loading.gif" title="Finish!" alt=""><h4 id="3-静态IP"><a href="#3-静态IP" class="headerlink" title="3. 静态IP"></a>3. 静态IP</h4><h5 id="配置root用户"><a href="#配置root用户" class="headerlink" title="配置root用户"></a>配置root用户</h5><pre><code class="bash"># 查看现有网卡ubuntu@ubuntu:~$ ifconfig -a# 若网卡未开则手动开启ubuntu@ubuntu:~$ ifconfig ens33 on# ubuntu@ubuntu:~$ ifconfig ens33 down# 配置静态IPubuntu@ubuntu:~$ sudo vim /etc/network/interfaces[sudo] password for ubuntu: # interfaces# This file describes the network interfaces available on your system# and how to activate them. For more information, see interfaces(5).source /etc/network/interfaces.d/*# The loopback network interfaceauto loiface lo inet loopback# The primary network interfaceauto ens33                 ⬅ 网关自启# iface ens33 inet dhcpiface ens33 inet staticaddress 192.168.73.21      ⬅ 内网静态IPgateway 192.168.73.2       ⬅ 内网网关netmask 255.255.255.0network 192.168.73.0       ⬅ 子网IP####### 配置DNSubuntu@ubuntu:~$ sudo vim /etc/resolv.conf # resolv.conf# Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8)#     DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN# nameserver 192.168.73.2# search localdomainnameserver 202.96.128.86nameserver 114.114.114.114####### 重启网络ubuntu@ubuntu:~$ sudo service networking restart</code></pre><h5 id="4-上述配置正确，此时就可以用客户端进行连接"><a href="#4-上述配置正确，此时就可以用客户端进行连接" class="headerlink" title="4.上述配置正确，此时就可以用客户端进行连接"></a>4.上述配置正确，此时就可以用客户端进行连接</h5><h5 id="5-root用户激活"><a href="#5-root用户激活" class="headerlink" title="5. root用户激活"></a>5. root用户激活</h5><p><code>Tips</code> 这一步可以放在第3步执行</p><pre><code class="bash">ubuntu@ubuntu:~$ sudo passwd rootEnter new UNIX password:        ⬅ root Retype new UNIX password:       ⬅ rootpasswd: password updated successfullyubuntu@ubuntu:~$ su - rootPassword:                       ⬅ rootroot@ubuntu:~# root@ubuntu:~# root@ubuntu:~# </code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>OS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20191219_1545_SPARK</title>
    <link href="/2019/12/19/20191219-1545-Spark/"/>
    <url>/2019/12/19/20191219-1545-Spark/</url>
    
    <content type="html"><![CDATA[<h2 id="Spark-配置"><a href="#Spark-配置" class="headerlink" title="Spark 配置"></a>Spark 配置</h2><h3 id="01-Spark"><a href="#01-Spark" class="headerlink" title="01. Spark"></a>01. Spark</h3><ul><li>Repo Point<br><a href="http://archive.apache.org/dist/spark/spark-2.0.2/spark-2.0.2-bin-hadoop2.6.tgz" target="_blank" rel="noopener">spark-2.0.2-bin-hadoop2.6.tgz</a></li></ul><h3 id="02-环境配置"><a href="#02-环境配置" class="headerlink" title="02. 环境配置"></a>02. 环境配置</h3><h4 id="1-环境包-rz-至相关路径，分流至其它节点"><a href="#1-环境包-rz-至相关路径，分流至其它节点" class="headerlink" title="1. 环境包 rz 至相关路径，分流至其它节点"></a>1. 环境包 rz 至相关路径，分流至其它节点</h4><h5 id="源码分发"><a href="#源码分发" class="headerlink" title="源码分发"></a>源码分发</h5><pre><code class="bash">scp -r /usr/local/src/spark-2.0.2-bin-hadoop2.6 root@slave1:/usr/local/src/scp -r /usr/local/src/spark-2.0.2-bin-hadoop2.6 root@slave2:/usr/local/src/</code></pre><h4 id="2-用户环境变量配置"><a href="#2-用户环境变量配置" class="headerlink" title="2. 用户环境变量配置"></a>2. 用户环境变量配置</h4><pre><code class="bash"># SET JAVA PATHexport JAVA_HOME=/usr/local/src/jdk1.8.0_201export JRE_HOME=${JAVA_HOME}/jreexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/libexport PATH=${JAVA_HOME}/bin:$PATH# SET HADOOP PATHexport HADOOP_HOME=/usr/local/src/hadoop-2.6.1export PATH=$PATH:$HADOOP_HOME/bin# SET HIVE PATHexport HIVE_HOME=/usr/local/src/apache-hive-1.2.2-binexport PATH=$PATH:$HIVE_HOME/bin# SET SCALA PATHexport SCALA_HOME=/usr/local/src/scala-2.11.8export PATH=$PATH:$SCALA_HOME/bin# SET SPARK PATHexport SPARK_HOME=/usr/local/src/spark-2.0.2-bin-hadoop2.6export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin# SET INI PATHexport INI_PATH=/usr/local/src</code></pre><h5 id="环境变量分发"><a href="#环境变量分发" class="headerlink" title="环境变量分发"></a>环境变量分发</h5><pre><code class="bash">scp ~/.bashrc root@slave1:~/scp ~/.bashrc root@slave2:~/</code></pre><h4 id="3-修改-SPARK-源码环境配置文件、添加集群节点："><a href="#3-修改-SPARK-源码环境配置文件、添加集群节点：" class="headerlink" title="3. 修改 SPARK 源码环境配置文件、添加集群节点："></a>3. 修改 SPARK 源码环境配置文件、添加集群节点：</h4><pre><code class="bash">cd /usr/local/src/spark-2.0.2-bin-hadoop2.6/conf/mv spark-env.sh.template spark-env.shvim spark-env.sh# spark-env.shexport SCALA_HOME=/usr/local/src/scala-2.11.8export JAVA_HOME=/usr/local/src/jdk1.8.0_201export HADOOP_HOME=/usr/local/src/hadoop-2.6.1export HADOOP_CONF_DIR=/usr/local/src/hadoop-2.6.1/etc/hadoopSPARK_MASTER_IP=masterSPARK_LOCAL_DIRS=/usr/local/src/spark-2.0.2-bin-hadoop2.6SPARK_DRIVER_MEMORY=1G######mv slaves.template slavesvim slaves# slaves 添加集群节点slave1slave2######</code></pre><h4 id="4-适配Spark-SQL"><a href="#4-适配Spark-SQL" class="headerlink" title="4. 适配Spark-SQL"></a>4. 适配Spark-SQL</h4><h5 id="MySQL相关配置和赋权见相关Blog，此处暂时不进行罗列：MySQL配置"><a href="#MySQL相关配置和赋权见相关Blog，此处暂时不进行罗列：MySQL配置" class="headerlink" title="MySQL相关配置和赋权见相关Blog，此处暂时不进行罗列：MySQL配置"></a>MySQL相关配置和赋权见相关Blog，此处暂时不进行罗列：<a href="">MySQL配置</a></h5><p><code>Tips</code> <strong>SPARK2.0</strong> 中没有 <code>${SPARK_HOME}/lib/spark-assembly-*.jar</code>，<strong>lib</strong>文件中的jar包已经被分成多个小的jar包在放置在 <code>${SPARK_HOME}/jars/</code> 下.</p><ul><li>修改 $HIVE_HOME/bin下的Hive启动脚本 <strong><code>hive</code></strong>：</li></ul><h5 id="修改前："><a href="#修改前：" class="headerlink" title="修改前："></a>修改前：</h5><pre><code class="bash"># add SPARK assembly jar to the classpathif [[ -n &quot;$SPARK_HOME&quot; ]]then  sparkAssemblyPath=`ls ${SPARK_HOME}/lib/spark-assembly-*.jar`  CLASSPATH=&quot;${CLASSPATH}:${sparkAssemblyPath}&quot;fi</code></pre><h5 id="YARN-Hadoop-2-0-内的："><a href="#YARN-Hadoop-2-0-内的：" class="headerlink" title="YARN (Hadoop 2.0) 内的："></a>YARN (Hadoop 2.0) 内的：</h5><pre><code class="bash"># add SPARK assembly jar to the classpathif [[ -n &quot;$SPARK_HOME&quot; ]]then  sparkAssemblyPath=`ls ${SPARK_HOME}/jars/*.jar`  CLASSPATH=&quot;${CLASSPATH}:${sparkAssemblyPath}&quot;fi</code></pre><h4 id="5-更新Hadoop-Yarn下的老版-jline-0-9-94-jar"><a href="#5-更新Hadoop-Yarn下的老版-jline-0-9-94-jar" class="headerlink" title="5. 更新Hadoop Yarn下的老版 jline-0.9.94.jar"></a>5. 更新Hadoop Yarn下的老版 <strong>jline-0.9.94.jar</strong></h4><pre><code class="bash"># Hive内较新版本的jline替换Hadoop内的OLD版本cd $HADOOP_HOME/share/hadoop/yarn/lib/mv jline-0.9.94.jar jline-0.9.94.jar.oldcp $HIVE_HOME/lib/jline-2.12.jar $HADOOP_HOME/share/hadoop/yarn/lib/</code></pre><h4 id="6-Hive集成Spark-SQL"><a href="#6-Hive集成Spark-SQL" class="headerlink" title="6. Hive集成Spark-SQL"></a>6. Hive集成Spark-SQL</h4><p><code>Tips</code> 这部分集成后的功能暂未进行操作</p><h5 id="hive-site-xml-内新增-Metastore-配置"><a href="#hive-site-xml-内新增-Metastore-配置" class="headerlink" title="hive-site.xml 内新增 Metastore 配置"></a>hive-site.xml 内新增 Metastore 配置</h5><pre><code class="bash">vim $HIVE_HOME/conf/hive-site.xml# hive-site.xml 新增如下内容：&lt;property&gt;    &lt;name&gt;hive.metastore.uris&lt;/name&gt;    &lt;value&gt;thrift://hostname:9083&lt;/value&gt;&lt;/property&gt;####### 将Hive的配置文件拷贝给SPARKcp $HIVE_HOME/conf/hive-site.xml $SPARK_HOME/conf/# 将MySQL下的jdbc驱动包拷贝给SPARKcp $HIVE_HOME/lib/mysql-connector-java-5.1.44.jar  $SPARK_HOME/jars/</code></pre><h4 id="7-SPARK集群启动-关闭"><a href="#7-SPARK集群启动-关闭" class="headerlink" title="7. SPARK集群启动/关闭"></a>7. SPARK集群启动/关闭</h4><pre><code class="bash">cd /usr/local/src/spark-2.0.2-bin-hadoop2.6/sbin/./start-all.sh# 关闭集群：cd /usr/local/src/spark-2.0.2-bin-hadoop2.6/sbin/./stop-all.sh</code></pre><h5 id="启动-Hive-metastore"><a href="#启动-Hive-metastore" class="headerlink" title="启动 Hive metastore"></a>启动 Hive metastore</h5><p><code>Metedata</code>元数据.<br><code>Metastore</code>Hive客户端连接Metastore元存储服务，Metastore再去连接MySQL来存取 <strong>元数据</strong>。多个客户端连接Hive时，只需要连接Metastore服务即可；即启动后可以用Java、Python等调用该jdbc</p><pre><code class="bash"># hive --service metastorecd $HIVE_HOME/bin/hive --service metastore  1&gt;/dev/null  2&gt;&amp;1  &amp;# 启动Spark-SQL的shell交互界面cd $SPARK_HOME/bin/spark-shell --master spark://master:7077</code></pre><h5 id="疑问"><a href="#疑问" class="headerlink" title="疑问"></a>疑问</h5><p><code>hiveserver2</code>HS2是一个服务端接口，使远程客户端可以执行对Hive的查询并返回结果，HS2提供了新的CLI：Beeline(基于SQLLine)，可以作为Hive jdbc Client端访问HS2.<br><code>hiveserver2</code> 和 <code>Metastore</code>一样同为Thrift Service，区别是否在于前者访问数据，后者访问元数据？？？</p><ul><li><p>网上有一部分解释：</p><ol><li><p>若有在<code>hive-site.xml</code>中配置<code>hive.metastore.uris</code>，则<code>hiveserver2</code> 启动时会去连接配置好的 Metastore 服务(故需开启Metastore服务，其余jdbc访问也需开启该服务)，这种方式最为常用；</p></li><li><p>配置了<code>hive.metastore.uris</code>情况下未启动<code>Metastore</code>，本地Beeline Client连接会失败；<img src="Spark-error_metastore_beeline.png" srcset="/img/loading.gif" alt="配置MS但未开启时，Beeline报错." title="配置MS但未开启时，Beeline报错."> </p></li><li><p>若未配置，Hive则会在启动HS2时，优先启动一个<code>Metastore</code>服务，然后再启动HS2，便于后续Beeline CLI的连接.</p></li></ol></li></ul><h5 id="回顾下Beeline连接Hive"><a href="#回顾下Beeline连接Hive" class="headerlink" title="回顾下Beeline连接Hive"></a>回顾下Beeline连接Hive</h5><ul><li>这里使用的是<code>Hive Metastore ➡ hiveserver2 ➡ beeline</code>的连接方式.</li></ul><pre><code class="bash"># 配置了hive.metastore.uris后需开启 Metastore# 后缀为重定向，需百度解决（已解决，详见20191219-2254-Hive-1-x）hive --service metastore  1&gt;/dev/null  2&gt;&amp;1  &amp;# beeline方式连接Hive，默认端口为10000hive --service hiveserver2 &amp;#beeline#!connect jdbc:hive2://master:10000 root 123456beeline -u jdbc:hive2://master:10000 -n root -p 123456 --color=true</code></pre><p><code>Tips</code> 可以单独启动 SPARK 集群，但此时无Hadoop下的<code>ResourceManager</code>和<code>NodeManager</code>等进程仅有<code>Master</code>和<code>Worker</code>！！！</p>]]></content>
    
    
    
    <tags>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2019/12/02/20191202_2329_Hello%20World/"/>
    <url>/2019/12/02/20191202_2329_Hello%20World/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Demo</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
